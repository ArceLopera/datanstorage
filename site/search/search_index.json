{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Data n Storage Refresher Everyone can forget about memorizing every query and command. What matters most is knowing where to find the right information. Mastering databases and storage systems isn\u2019t about remembering every syntax detail\u2014it\u2019s about understanding the core principles, exploring best practices, and knowing how to troubleshoot efficiently. With a solid foundation, a habit of continuous learning, and hands-on experimentation, you can refine your skills and confidently manage data in any environment. Basic Topics Advanced Topics Basics This material is a work in progress, so your feedback is welcome. The best way to provide that feedback is to click here and create an issue in this GitHub repository .","title":"Home"},{"location":"#welcome-to-data-n-storage-refresher","text":"Everyone can forget about memorizing every query and command. What matters most is knowing where to find the right information. Mastering databases and storage systems isn\u2019t about remembering every syntax detail\u2014it\u2019s about understanding the core principles, exploring best practices, and knowing how to troubleshoot efficiently. With a solid foundation, a habit of continuous learning, and hands-on experimentation, you can refine your skills and confidently manage data in any environment. Basic Topics Advanced Topics Basics This material is a work in progress, so your feedback is welcome. The best way to provide that feedback is to click here and create an issue in this GitHub repository .","title":"Welcome to Data n Storage Refresher"},{"location":"db_basic/","text":"How to choose DB Integration The most important thing to consider while choosing the right database is what system you need to integrate together? Make sure that your database management system can be integrated with other tools and services within your project. Different technologies have different connectors for different other technologies. For example, if you have a big analytics job that\u2019s currently running an Apache spark then probably you want to limit yourself to external databases that can connect easily to apache spark. Scaling Requirement It\u2019s important to know the scaling requirement before installing your production database. How much data are you really talking about? Is it really going to grow unbounded over time? if so then you need some sort of database technology that is not limited to the data that you can store on one PC. You need to look at something like Cassandra or MongoDB or HBase where you can actually distribute the storage of your data across an entire cluster and scale horizontally instead of vertically. While choosing a database you also need to think about the transaction rate or throughput which means how many requests you intend to get per second. Databases with high throughput can support many simultaneous users. If we are talking about thousands then again a single database service is not going to work out. This is especially important when you are working on some big websites where we have a lot of web servers that are serving a lot of people at the same time. You will have to choose a database that is distributed and allows you to spread out a load of those transactions more evenly. In those situations, NoSQL databases are a good choice instead of RDBMS. Support Consideration Think about the supports you might need for your database. Do you have the in-house expertise to spin up this new technology and actually configure it properly? It\u2019s going to be harder than you think especially if you\u2019re using this in the real world or any sort of situation where you have personally identifiable information in the mix from your end-users. In that case, you need to make sure you\u2019re thinking about the security of your system. The truth is most of the NoSQL database we\u2019ve talked about if you configure them with their default settings there will be no security at all. CAP Consideration CAP stands for Consistency, Availability, and Partition tolerance. The theorem states that you cannot achieve all the properties at the best level in a single database, as there are natural trade offs between the items. You can only pick two out of three at a time and that totally depends on your prioritize based on your requirements. For example, if your system needs to be available and partition tolerant, then you must be willing to accept some latency in your consistency requirements. Traditional relational databases are a natural fit for the CA side whereas Non-relational database engines mostly satisfy AP and CP requirements. Consistency means that any read request will return the most recent write. Data consistency is usually \u201cstrong\u201d for SQL databases and for NoSQL database consistency may be anything from \u201ceventual\u201d to \u201cstrong\u201d. Availability means that a non-responding node must respond in a reasonable amount of time. Not every application needs to run 24/7 with 99.999% availability but most likely you will prefer a database with higher availability. Partition tolerance means the system will continue to operate despite network or node failures. Schemas or Data Model Relational databases store data in a fixed and predefined structure. It means when you start development you will have to define your data schema in terms of tables and columns. You have to change the schema every time the requirements change. This will lead to creating new columns, defining new relations, reflecting the changes in your application, discussing with your database administrators, etc. NoSQL database provides much more flexibility when it comes to handling data. There is no requirement to specify the schema to start working with the application. Also, the NoSQL database doesn\u2019t put a restriction on the types of data you can store together. It allows you to add more new types as your needs change. In the application building process, most of the developers prefer high coding velocity and great agility. NoSQL databases have proven to be a much better choice in that regard especially for agile development which requires fast implementation. Types of DB You have a variety of options available in relational (MySQL, PostgreSQL, Oracle DB, etc) and non-relational (MongoDB, Apache HBase, Cassandra, etc) database but you need to understand none of them fits on all kinds of projects requirement. Each one of them has some strengths and weaknesses. Databases through two lenses: access characteristics and the pattern of the data being stored. Relational With large spans of usage, relational databases are still the dominant database type today. A relational database is self-describing because it enables developers to define the database's schema as well as relations and constraints between rows and tables in the database. Developers rely on the functionality of the relational database and not the application code to enforce the schema and preserve the referential integrity of the data within the database. Typical use cases for a relational database include web and mobile applications, enterprise applications, and online gaming. Various flavors or versions of Amazon RDS and Amazon Aurora are used by startups for high-performance and scalable applications on AWS. Both RDS and Aurora are fully managed, scalable systems. MySQL Relational (+Document since 5.7.8) SQL with JOINS JSON type support (since 5.7.8) Open source (with proprietary, closed-sourced modules) When to use MySQL When you already widely use it in your organization When you want both relational tables (when you know the schema upfront) and JSON collections (Schemaless) Relational / normalized \u2014 when you need to optimize on writes instead of reads, to have strong read consistency MySQL Advantages Maturity & Reliability \u2014 MySQL is highly used, battle tested and mature Fast read performance Improved JSON/Document support (MySQL 8) Cross-DC write consistency (when ProxySQL is used) MySQL Disadvantages Scalability \u2014 Does not scale horizontally. Limited by amount of disk space Consistency and Replication Issues (when not using ProxySQL) Other DBs in this category PostgreSQL, MariaDB, SQL Server, Oracle, Db2, SQLite Key-value As your system grows, large amounts of data are often in the form of key-value data, where a single row maps to a primary key. Key-value databases are highly partitionable and allow horizontal scaling at levels that other types of databases cannot achieve. Use cases such as gaming, ad tech, and IoT lend themselves particularly well to the key-value data model where the access patterns require low-latency Gets/Puts for known key values. Amazon DynamoDB is a managed key-value and document database that delivers single-digit millisecond performance at any scale. Key-value DBs store data in pairs, each containing a unique ID and a data value. These DBs provide a flexible storage structure since values can store any amount of unstructured data. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Session management, user preferences, and product recommendations. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Amazon DynamoDB, Azure Cosmos DB. Amazon DynamoDB Other DBs in this category Cassandra, HBase, Redis (Key-value Store) When to use DynamoDB When you need a simple key value store without complex querying patterns When you need to store expirable data Low-medium throughput apps as writes are expensive and consistent reads are twice the cost of eventually consistent reads. DynamoDB Advantages Fast performance in any scale (as long as enough capacity is provisioned) No storage limit Schemaless \u2014 it\u2019s possible to define a schema for each item, rather than for the whole table +Multi-master replication (update data in multiple regions) Supports TTL per item Built-in CDC events (DynamoDB streams) DynamoDB Disadvantages Size limit \u2014 item can only reach 400KB in size Limited querying options (limited number of indices) Throttling on burst throughput (and hot keys in certain situations) Amazon Simple Storage Service (S3) Other DBs in this category Google Cloud Storage, Azure Blob Storage When to use S3 When you need to store large binary objects/files (up to 5TB each) When the amount of data you need to store is large (>10TB), continues to grow daily, and may need to be retrieved (can\u2019t be deleted) S3 Advantages Supports very high throughput Infinite scalability \u2014 No limit on amount of storage S3 Disadvantages No Query support, only key-based retrieval Latency is 100\u2013200 ms for small objects. Caching can alleviate this Document Document databases are intuitive for developers to use, because the data in the application tier is typically represented as a JSON document. Developers can persist data using the same document model format that they use in their application code and use the flexible schema model of Amazon DocumentDB to achieve developer efficiency. \ud835\uddd7\ud835\uddfc\ud835\uddf0\ud835\ude02\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00 Document databases are structured similarly to key-value databases except that keys and values are stored in documents written in a markup language like JSON, XML, or YAML. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: User profiles, product catalogs, and content management. Examples: MongoDB, Amazon DocumentDB MongoDB When to use MongoDB When data schema is predicted to keep changing and evolving When working with dynamic JSON content When keeping data denormalized is not a problem. i.e. to have eventual consistency MongoDB Advantages Flexibility \u2014 with Schemaless documents, the number of fields, content and size of the document can differ from one document to another in the same collection. Easy to scale with sharding MongoDB Disadvantages High Memory Usage \u2014 a lot of denormalized data is kept in memory Document size limit \u2014 16MB Non optimal replication solution (data cannot be re-replicated after recovery from failure). Consistency issues on traffic switch to another data center (No automatic remaster) Graph databases A graph database's purpose is to make it easy to build and run applications that work with highly connected data sets. Typical use cases for a graph database include social networking, recommendation engines, fraud detection, and knowledge graphs. Amazon Neptune is a fully managed graph database service. Neptune supports both the Property Graph model and the Resource Description Framework (RDF), giving you the choice of two graph APIs: TinkerPop and RDF/SPARQL. Startups use Amazon Neptune to build knowledge graphs, make in-game offer recommendations, and detect fraud. \ud835\uddda\ud835\uddff\ud835\uddee\ud835\uddfd\ud835\uddf5 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00 Graph databases map the relationships between data using nodes and edges. Nodes are the individual data values, and edges are the relationships between those values. Use cases : Social graphs, recommendation engines, and fraud detection. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: New4j, Amazon Neptune, Azure Gremlin. In-memory databases Financial services, ecommerce, web, and mobile applications have use cases such as leaderboards, session stores, and real-time analytics that require microsecond response times and can have large spikes in traffic coming at any time. We built Amazon ElastiCache, offering Memcached and Redis, to serve low latency, high throughput workloads that cannot be served with disk-based data stores. Amazon DynamoDB Accelerator (DAX) is another example of a purpose-built data store. DAX was built to make DynamoDB reads an order of magnitude faster, from milliseconds to microseconds, even at millions of requests per second. \ud835\udddc\ud835\uddfb-\ud835\uddfa\ud835\uddf2\ud835\uddfa\ud835\uddfc\ud835\uddff\ud835\ude06 \ud835\uddde\ud835\uddf2\ud835\ude06-\ud835\udde9\ud835\uddee\ud835\uddf9\ud835\ude02\ud835\uddf2 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00 The data is primarily stored in memory, unlike disk-based databases. By eliminating disk access, these databases enable minimal response times. Because all data is stored in main memory, in-memory databases risk losing data upon a process or server failure. In-memory databases can persist data on disks by storing each operation in a log or by taking snapshots. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Redis, Memcached, Amazon ElastiCache Search Databases Many applications output logs to help developers troubleshoot issues. Amazon Elasticsearch Service, or Amazon ES, is purpose-built for providing near real-time visualizations and analytics of machine-generated data by indexing, aggregating, and searching semi-structured logs and metrics. Amazon ES is also a powerful, high-performance search engine for full-text search use cases. Startups store billions of documents for a variety of mission-critical use cases, ranging from operational monitoring and troubleshooting to distributed application stack tracing and pricing optimization. Amazon ElasticSearch Other DBs in this category Apache Solr, Splunk, Amazon CloudSearch When to use Elasticsearch When you need to perform fuzzy search or have results with ranking When you have another data store as source of truth (populate Elasticsearch as a materialized view) Elasticsearch Advantages Easy to horizontally scale with index sharding Rich search API Query for analytical data using aggregations Elasticsearch Disadvantages Indexes are created with a predefined number of shards. More shards requires migration to a new index. Usually done with ReIndex API Performance issues when indices hit very large scale (> 1TB with hundreds of nodes and shards) Wide Column Databases Wide column databases are based on tables but without a strict column format. Rows do not need a value in every column and segments of rows and columns containing different data formats can be combined. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Telemetry, analytics data, messaging, and time-series data. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Apache Cassandra, Azure Table Storage, HBase Time Series Databases These DBs store data in time-ordered streams. Data is not sorted by value or ID but by the time of collection, ingestion, or other timestamps included in the metadata. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Industrial telemetry, DevOps, and Internet of things (IoT) applications. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Graphite, Prometheus, Amazon Timestream Ledger Databases Ledger databases are based on logs that record events related to data values. These DBs store data changes that are used to verify the integrity of data. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Banking systems, registrations, supply chains, and systems of record. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Amazon Quantum Ledger Database (QLDB)","title":"General"},{"location":"db_basic/#how-to-choose-db","text":"","title":"How to choose DB"},{"location":"db_basic/#integration","text":"The most important thing to consider while choosing the right database is what system you need to integrate together? Make sure that your database management system can be integrated with other tools and services within your project. Different technologies have different connectors for different other technologies. For example, if you have a big analytics job that\u2019s currently running an Apache spark then probably you want to limit yourself to external databases that can connect easily to apache spark.","title":"Integration"},{"location":"db_basic/#scaling-requirement","text":"It\u2019s important to know the scaling requirement before installing your production database. How much data are you really talking about? Is it really going to grow unbounded over time? if so then you need some sort of database technology that is not limited to the data that you can store on one PC. You need to look at something like Cassandra or MongoDB or HBase where you can actually distribute the storage of your data across an entire cluster and scale horizontally instead of vertically. While choosing a database you also need to think about the transaction rate or throughput which means how many requests you intend to get per second. Databases with high throughput can support many simultaneous users. If we are talking about thousands then again a single database service is not going to work out. This is especially important when you are working on some big websites where we have a lot of web servers that are serving a lot of people at the same time. You will have to choose a database that is distributed and allows you to spread out a load of those transactions more evenly. In those situations, NoSQL databases are a good choice instead of RDBMS.","title":"Scaling Requirement"},{"location":"db_basic/#support-consideration","text":"Think about the supports you might need for your database. Do you have the in-house expertise to spin up this new technology and actually configure it properly? It\u2019s going to be harder than you think especially if you\u2019re using this in the real world or any sort of situation where you have personally identifiable information in the mix from your end-users. In that case, you need to make sure you\u2019re thinking about the security of your system. The truth is most of the NoSQL database we\u2019ve talked about if you configure them with their default settings there will be no security at all.","title":"Support Consideration"},{"location":"db_basic/#cap-consideration","text":"CAP stands for Consistency, Availability, and Partition tolerance. The theorem states that you cannot achieve all the properties at the best level in a single database, as there are natural trade offs between the items. You can only pick two out of three at a time and that totally depends on your prioritize based on your requirements. For example, if your system needs to be available and partition tolerant, then you must be willing to accept some latency in your consistency requirements. Traditional relational databases are a natural fit for the CA side whereas Non-relational database engines mostly satisfy AP and CP requirements. Consistency means that any read request will return the most recent write. Data consistency is usually \u201cstrong\u201d for SQL databases and for NoSQL database consistency may be anything from \u201ceventual\u201d to \u201cstrong\u201d. Availability means that a non-responding node must respond in a reasonable amount of time. Not every application needs to run 24/7 with 99.999% availability but most likely you will prefer a database with higher availability. Partition tolerance means the system will continue to operate despite network or node failures.","title":"CAP Consideration"},{"location":"db_basic/#schemas-or-data-model","text":"Relational databases store data in a fixed and predefined structure. It means when you start development you will have to define your data schema in terms of tables and columns. You have to change the schema every time the requirements change. This will lead to creating new columns, defining new relations, reflecting the changes in your application, discussing with your database administrators, etc. NoSQL database provides much more flexibility when it comes to handling data. There is no requirement to specify the schema to start working with the application. Also, the NoSQL database doesn\u2019t put a restriction on the types of data you can store together. It allows you to add more new types as your needs change. In the application building process, most of the developers prefer high coding velocity and great agility. NoSQL databases have proven to be a much better choice in that regard especially for agile development which requires fast implementation.","title":"Schemas or Data Model"},{"location":"db_basic/#types-of-db","text":"You have a variety of options available in relational (MySQL, PostgreSQL, Oracle DB, etc) and non-relational (MongoDB, Apache HBase, Cassandra, etc) database but you need to understand none of them fits on all kinds of projects requirement. Each one of them has some strengths and weaknesses. Databases through two lenses: access characteristics and the pattern of the data being stored.","title":"Types of DB"},{"location":"db_basic/#relational","text":"With large spans of usage, relational databases are still the dominant database type today. A relational database is self-describing because it enables developers to define the database's schema as well as relations and constraints between rows and tables in the database. Developers rely on the functionality of the relational database and not the application code to enforce the schema and preserve the referential integrity of the data within the database. Typical use cases for a relational database include web and mobile applications, enterprise applications, and online gaming. Various flavors or versions of Amazon RDS and Amazon Aurora are used by startups for high-performance and scalable applications on AWS. Both RDS and Aurora are fully managed, scalable systems.","title":"Relational"},{"location":"db_basic/#mysql","text":"Relational (+Document since 5.7.8) SQL with JOINS JSON type support (since 5.7.8) Open source (with proprietary, closed-sourced modules) When to use MySQL When you already widely use it in your organization When you want both relational tables (when you know the schema upfront) and JSON collections (Schemaless) Relational / normalized \u2014 when you need to optimize on writes instead of reads, to have strong read consistency MySQL Advantages Maturity & Reliability \u2014 MySQL is highly used, battle tested and mature Fast read performance Improved JSON/Document support (MySQL 8) Cross-DC write consistency (when ProxySQL is used) MySQL Disadvantages Scalability \u2014 Does not scale horizontally. Limited by amount of disk space Consistency and Replication Issues (when not using ProxySQL) Other DBs in this category PostgreSQL, MariaDB, SQL Server, Oracle, Db2, SQLite","title":"MySQL"},{"location":"db_basic/#key-value","text":"As your system grows, large amounts of data are often in the form of key-value data, where a single row maps to a primary key. Key-value databases are highly partitionable and allow horizontal scaling at levels that other types of databases cannot achieve. Use cases such as gaming, ad tech, and IoT lend themselves particularly well to the key-value data model where the access patterns require low-latency Gets/Puts for known key values. Amazon DynamoDB is a managed key-value and document database that delivers single-digit millisecond performance at any scale. Key-value DBs store data in pairs, each containing a unique ID and a data value. These DBs provide a flexible storage structure since values can store any amount of unstructured data. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Session management, user preferences, and product recommendations. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Amazon DynamoDB, Azure Cosmos DB.","title":"Key-value"},{"location":"db_basic/#amazon-dynamodb","text":"Other DBs in this category Cassandra, HBase, Redis (Key-value Store) When to use DynamoDB When you need a simple key value store without complex querying patterns When you need to store expirable data Low-medium throughput apps as writes are expensive and consistent reads are twice the cost of eventually consistent reads. DynamoDB Advantages Fast performance in any scale (as long as enough capacity is provisioned) No storage limit Schemaless \u2014 it\u2019s possible to define a schema for each item, rather than for the whole table +Multi-master replication (update data in multiple regions) Supports TTL per item Built-in CDC events (DynamoDB streams) DynamoDB Disadvantages Size limit \u2014 item can only reach 400KB in size Limited querying options (limited number of indices) Throttling on burst throughput (and hot keys in certain situations)","title":"Amazon DynamoDB"},{"location":"db_basic/#amazon-simple-storage-service-s3","text":"Other DBs in this category Google Cloud Storage, Azure Blob Storage When to use S3 When you need to store large binary objects/files (up to 5TB each) When the amount of data you need to store is large (>10TB), continues to grow daily, and may need to be retrieved (can\u2019t be deleted) S3 Advantages Supports very high throughput Infinite scalability \u2014 No limit on amount of storage S3 Disadvantages No Query support, only key-based retrieval Latency is 100\u2013200 ms for small objects. Caching can alleviate this","title":"Amazon Simple Storage Service (S3)"},{"location":"db_basic/#document","text":"Document databases are intuitive for developers to use, because the data in the application tier is typically represented as a JSON document. Developers can persist data using the same document model format that they use in their application code and use the flexible schema model of Amazon DocumentDB to achieve developer efficiency. \ud835\uddd7\ud835\uddfc\ud835\uddf0\ud835\ude02\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00 Document databases are structured similarly to key-value databases except that keys and values are stored in documents written in a markup language like JSON, XML, or YAML. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: User profiles, product catalogs, and content management. Examples: MongoDB, Amazon DocumentDB","title":"Document"},{"location":"db_basic/#mongodb","text":"When to use MongoDB When data schema is predicted to keep changing and evolving When working with dynamic JSON content When keeping data denormalized is not a problem. i.e. to have eventual consistency MongoDB Advantages Flexibility \u2014 with Schemaless documents, the number of fields, content and size of the document can differ from one document to another in the same collection. Easy to scale with sharding MongoDB Disadvantages High Memory Usage \u2014 a lot of denormalized data is kept in memory Document size limit \u2014 16MB Non optimal replication solution (data cannot be re-replicated after recovery from failure). Consistency issues on traffic switch to another data center (No automatic remaster)","title":"MongoDB"},{"location":"db_basic/#graph-databases","text":"A graph database's purpose is to make it easy to build and run applications that work with highly connected data sets. Typical use cases for a graph database include social networking, recommendation engines, fraud detection, and knowledge graphs. Amazon Neptune is a fully managed graph database service. Neptune supports both the Property Graph model and the Resource Description Framework (RDF), giving you the choice of two graph APIs: TinkerPop and RDF/SPARQL. Startups use Amazon Neptune to build knowledge graphs, make in-game offer recommendations, and detect fraud. \ud835\uddda\ud835\uddff\ud835\uddee\ud835\uddfd\ud835\uddf5 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00 Graph databases map the relationships between data using nodes and edges. Nodes are the individual data values, and edges are the relationships between those values. Use cases : Social graphs, recommendation engines, and fraud detection. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: New4j, Amazon Neptune, Azure Gremlin.","title":"Graph databases"},{"location":"db_basic/#in-memory-databases","text":"Financial services, ecommerce, web, and mobile applications have use cases such as leaderboards, session stores, and real-time analytics that require microsecond response times and can have large spikes in traffic coming at any time. We built Amazon ElastiCache, offering Memcached and Redis, to serve low latency, high throughput workloads that cannot be served with disk-based data stores. Amazon DynamoDB Accelerator (DAX) is another example of a purpose-built data store. DAX was built to make DynamoDB reads an order of magnitude faster, from milliseconds to microseconds, even at millions of requests per second. \ud835\udddc\ud835\uddfb-\ud835\uddfa\ud835\uddf2\ud835\uddfa\ud835\uddfc\ud835\uddff\ud835\ude06 \ud835\uddde\ud835\uddf2\ud835\ude06-\ud835\udde9\ud835\uddee\ud835\uddf9\ud835\ude02\ud835\uddf2 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00 The data is primarily stored in memory, unlike disk-based databases. By eliminating disk access, these databases enable minimal response times. Because all data is stored in main memory, in-memory databases risk losing data upon a process or server failure. In-memory databases can persist data on disks by storing each operation in a log or by taking snapshots. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Redis, Memcached, Amazon ElastiCache","title":"In-memory databases"},{"location":"db_basic/#search-databases","text":"Many applications output logs to help developers troubleshoot issues. Amazon Elasticsearch Service, or Amazon ES, is purpose-built for providing near real-time visualizations and analytics of machine-generated data by indexing, aggregating, and searching semi-structured logs and metrics. Amazon ES is also a powerful, high-performance search engine for full-text search use cases. Startups store billions of documents for a variety of mission-critical use cases, ranging from operational monitoring and troubleshooting to distributed application stack tracing and pricing optimization.","title":"Search Databases"},{"location":"db_basic/#amazon-elasticsearch","text":"Other DBs in this category Apache Solr, Splunk, Amazon CloudSearch When to use Elasticsearch When you need to perform fuzzy search or have results with ranking When you have another data store as source of truth (populate Elasticsearch as a materialized view) Elasticsearch Advantages Easy to horizontally scale with index sharding Rich search API Query for analytical data using aggregations Elasticsearch Disadvantages Indexes are created with a predefined number of shards. More shards requires migration to a new index. Usually done with ReIndex API Performance issues when indices hit very large scale (> 1TB with hundreds of nodes and shards)","title":"Amazon ElasticSearch"},{"location":"db_basic/#wide-column-databases","text":"Wide column databases are based on tables but without a strict column format. Rows do not need a value in every column and segments of rows and columns containing different data formats can be combined. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Telemetry, analytics data, messaging, and time-series data. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Apache Cassandra, Azure Table Storage, HBase","title":"Wide Column Databases"},{"location":"db_basic/#time-series-databases","text":"These DBs store data in time-ordered streams. Data is not sorted by value or ID but by the time of collection, ingestion, or other timestamps included in the metadata. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Industrial telemetry, DevOps, and Internet of things (IoT) applications. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Graphite, Prometheus, Amazon Timestream","title":"Time Series Databases"},{"location":"db_basic/#ledger-databases","text":"Ledger databases are based on logs that record events related to data values. These DBs store data changes that are used to verify the integrity of data. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Banking systems, registrations, supply chains, and systems of record. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Amazon Quantum Ledger Database (QLDB)","title":"Ledger Databases"},{"location":"db_postgresql/","text":"Reading PostgreSQL database from TensorFlow IO View on TensorFlow.org Run in Google Colab View source on GitHub Download notebook Overview This tutorial shows how to create tf.data.Dataset from a PostgreSQL database server, so that the created Dataset could be passed to tf.keras for training or inference purposes. A SQL database is an important source of data for data scientist. As one of the most popular open source SQL database, PostgreSQL is widely used in enterprises for storing critial and transactional data across the board. Creating Dataset from a PostgreSQL database server directly and pass the Dataset to tf.keras for training or inference, could greatly simplify the data pipeline and help data scientist to focus on building machine learning models. Setup and usage Install required tensorflow-io packages, and restart runtime try: %tensorflow_version 2.x except Exception: pass !pip install tensorflow-io Install and setup PostgreSQL (optional) Warning: This notebook is designed to be run in a Google Colab only . It installs packages on the system and requires sudo access. If you want to run it in a local Jupyter notebook, please proceed with caution. In order to demo the usage on Google Colab you will install PostgreSQL server. The password and an empty database is also needed. If you are not running this notebook on Google Colab, or you prefer to use an existing database, please skip the following setup and proceed to the next section. Install postgresql server !sudo apt-get -y -qq update !sudo apt-get -y -qq install postgresql !sudo service postgresql start Setup a password postgres for username postgres !sudo -u postgres psql -U postgres -c \"ALTER USER postgres PASSWORD 'postgres';\" Setup a database with name tfio_demo to be used !sudo -u postgres psql -U postgres -c 'DROP DATABASE IF EXISTS tfio_demo;' !sudo -u postgres psql -U postgres -c 'CREATE DATABASE tfio_demo;' Setup necessary environmental variables The following environmental variables are based on the PostgreSQL setup in the last section. If you have a different setup or you are using an existing database, they should be changed accordingly: %env TFIO_DEMO_DATABASE_NAME=tfio_demo %env TFIO_DEMO_DATABASE_HOST=localhost %env TFIO_DEMO_DATABASE_PORT=5432 %env TFIO_DEMO_DATABASE_USER=postgres %env TFIO_DEMO_DATABASE_PASS=postgres Prepare data in PostgreSQL server For demo purposes this tutorial will create a database and populate the database with some data. The data used in this tutorial is from Air Quality Data Set , available from UCI Machine Learning Repository . Below is a sneak preview of a subset of the Air Quality Data Set: Date Time CO(GT) PT08.S1(CO) NMHC(GT) C6H6(GT) PT08.S2(NMHC) NOx(GT) PT08.S3(NOx) NO2(GT) PT08.S4(NO2) PT08.S5(O3) T RH AH 10/03/2004 18.00.00 2,6 1360 150 11,9 1046 166 1056 113 1692 1268 13,6 48,9 0,7578 10/03/2004 19.00.00 2 1292 112 9,4 955 103 1174 92 1559 972 13,3 47,7 0,7255 10/03/2004 20.00.00 2,2 1402 88 9,0 939 131 1140 114 1555 1074 11,9 54,0 0,7502 10/03/2004 21.00.00 2,2 1376 80 9,2 948 172 1092 122 1584 1203 11,0 60,0 0,7867 10/03/2004 22.00.00 1,6 1272 51 6,5 836 131 1205 116 1490 1110 11,2 59,6 0,7888 More information about Air Quality Data Set and UCI Machine Learning Repository are availabel in References section. To help simplify the data preparation, a sql version of the Air Quality Data Set has been prepared and is available as AirQualityUCI.sql . The statement to create the table is: CREATE TABLE AirQualityUCI ( Date DATE, Time TIME, CO REAL, PT08S1 INT, NMHC REAL, C6H6 REAL, PT08S2 INT, NOx REAL, PT08S3 INT, NO2 REAL, PT08S4 INT, PT08S5 INT, T REAL, RH REAL, AH REAL ); The complete commands to create the table in database and populate the data are: !curl -s -OL https://github.com/tensorflow/io/raw/master/docs/tutorials/postgresql/AirQualityUCI.sql !PGPASSWORD=$TFIO_DEMO_DATABASE_PASS psql -q -h $TFIO_DEMO_DATABASE_HOST -p $TFIO_DEMO_DATABASE_PORT -U $TFIO_DEMO_DATABASE_USER -d $TFIO_DEMO_DATABASE_NAME -f AirQualityUCI.sql Create Dataset from PostgreSQL server and use it in TensorFlow Create a Dataset from PostgreSQL server is as easy as calling tfio.experimental.IODataset.from_sql with query and endpoint arguments. The query is the SQL query for select columns in tables and the endpoint argument is the address and database name: import os import tensorflow_io as tfio endpoint=\"postgresql://{}:{}@{}?port={}&dbname={}\".format( os.environ['TFIO_DEMO_DATABASE_USER'], os.environ['TFIO_DEMO_DATABASE_PASS'], os.environ['TFIO_DEMO_DATABASE_HOST'], os.environ['TFIO_DEMO_DATABASE_PORT'], os.environ['TFIO_DEMO_DATABASE_NAME'], ) dataset = tfio.experimental.IODataset.from_sql( query=\"SELECT co, pt08s1 FROM AirQualityUCI;\", endpoint=endpoint) print(dataset.element_spec) As you could see from the output of dataset.element_spec above, the element of the created Dataset is a python dict object with column names of the database table as keys. It is quite convenient to apply further operations. For example, you could select both nox and no2 field of the Dataset , and calculate the difference: dataset = tfio.experimental.IODataset.from_sql( query=\"SELECT nox, no2 FROM AirQualityUCI;\", endpoint=endpoint) dataset = dataset.map(lambda e: (e['nox'] - e['no2'])) check only the first 20 record dataset = dataset.take(20) print(\"NOx - NO2:\") for difference in dataset: print(difference.numpy()) The created Dataset is ready to be passed to tf.keras directly for either training or inference purposes now. References Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science. S. De Vito, E. Massera, M. Piga, L. Martinotto, G. Di Francia, On field calibration of an electronic nose for benzene estimation in an urban pollution monitoring scenario, Sensors and Actuators B: Chemical, Volume 129, Issue 2, 22 February 2008, Pages 750-757, ISSN 0925-4005","title":"PostgreSQL"},{"location":"db_postgresql/#reading-postgresql-database-from-tensorflow-io","text":"View on TensorFlow.org Run in Google Colab View source on GitHub Download notebook","title":"Reading PostgreSQL database from TensorFlow IO"},{"location":"db_postgresql/#overview","text":"This tutorial shows how to create tf.data.Dataset from a PostgreSQL database server, so that the created Dataset could be passed to tf.keras for training or inference purposes. A SQL database is an important source of data for data scientist. As one of the most popular open source SQL database, PostgreSQL is widely used in enterprises for storing critial and transactional data across the board. Creating Dataset from a PostgreSQL database server directly and pass the Dataset to tf.keras for training or inference, could greatly simplify the data pipeline and help data scientist to focus on building machine learning models.","title":"Overview"},{"location":"db_postgresql/#setup-and-usage","text":"","title":"Setup and usage"},{"location":"db_postgresql/#install-required-tensorflow-io-packages-and-restart-runtime","text":"try: %tensorflow_version 2.x except Exception: pass !pip install tensorflow-io","title":"Install required tensorflow-io packages, and restart runtime"},{"location":"db_postgresql/#install-and-setup-postgresql-optional","text":"Warning: This notebook is designed to be run in a Google Colab only . It installs packages on the system and requires sudo access. If you want to run it in a local Jupyter notebook, please proceed with caution. In order to demo the usage on Google Colab you will install PostgreSQL server. The password and an empty database is also needed. If you are not running this notebook on Google Colab, or you prefer to use an existing database, please skip the following setup and proceed to the next section.","title":"Install and setup PostgreSQL (optional)"},{"location":"db_postgresql/#install-postgresql-server","text":"!sudo apt-get -y -qq update !sudo apt-get -y -qq install postgresql !sudo service postgresql start","title":"Install postgresql server"},{"location":"db_postgresql/#setup-a-password-postgres-for-username-postgres","text":"!sudo -u postgres psql -U postgres -c \"ALTER USER postgres PASSWORD 'postgres';\"","title":"Setup a password postgres for username postgres"},{"location":"db_postgresql/#setup-a-database-with-name-tfio_demo-to-be-used","text":"!sudo -u postgres psql -U postgres -c 'DROP DATABASE IF EXISTS tfio_demo;' !sudo -u postgres psql -U postgres -c 'CREATE DATABASE tfio_demo;'","title":"Setup a database with name tfio_demo to be used"},{"location":"db_postgresql/#setup-necessary-environmental-variables","text":"The following environmental variables are based on the PostgreSQL setup in the last section. If you have a different setup or you are using an existing database, they should be changed accordingly: %env TFIO_DEMO_DATABASE_NAME=tfio_demo %env TFIO_DEMO_DATABASE_HOST=localhost %env TFIO_DEMO_DATABASE_PORT=5432 %env TFIO_DEMO_DATABASE_USER=postgres %env TFIO_DEMO_DATABASE_PASS=postgres","title":"Setup necessary environmental variables"},{"location":"db_postgresql/#prepare-data-in-postgresql-server","text":"For demo purposes this tutorial will create a database and populate the database with some data. The data used in this tutorial is from Air Quality Data Set , available from UCI Machine Learning Repository . Below is a sneak preview of a subset of the Air Quality Data Set: Date Time CO(GT) PT08.S1(CO) NMHC(GT) C6H6(GT) PT08.S2(NMHC) NOx(GT) PT08.S3(NOx) NO2(GT) PT08.S4(NO2) PT08.S5(O3) T RH AH 10/03/2004 18.00.00 2,6 1360 150 11,9 1046 166 1056 113 1692 1268 13,6 48,9 0,7578 10/03/2004 19.00.00 2 1292 112 9,4 955 103 1174 92 1559 972 13,3 47,7 0,7255 10/03/2004 20.00.00 2,2 1402 88 9,0 939 131 1140 114 1555 1074 11,9 54,0 0,7502 10/03/2004 21.00.00 2,2 1376 80 9,2 948 172 1092 122 1584 1203 11,0 60,0 0,7867 10/03/2004 22.00.00 1,6 1272 51 6,5 836 131 1205 116 1490 1110 11,2 59,6 0,7888 More information about Air Quality Data Set and UCI Machine Learning Repository are availabel in References section. To help simplify the data preparation, a sql version of the Air Quality Data Set has been prepared and is available as AirQualityUCI.sql . The statement to create the table is: CREATE TABLE AirQualityUCI ( Date DATE, Time TIME, CO REAL, PT08S1 INT, NMHC REAL, C6H6 REAL, PT08S2 INT, NOx REAL, PT08S3 INT, NO2 REAL, PT08S4 INT, PT08S5 INT, T REAL, RH REAL, AH REAL ); The complete commands to create the table in database and populate the data are: !curl -s -OL https://github.com/tensorflow/io/raw/master/docs/tutorials/postgresql/AirQualityUCI.sql !PGPASSWORD=$TFIO_DEMO_DATABASE_PASS psql -q -h $TFIO_DEMO_DATABASE_HOST -p $TFIO_DEMO_DATABASE_PORT -U $TFIO_DEMO_DATABASE_USER -d $TFIO_DEMO_DATABASE_NAME -f AirQualityUCI.sql","title":"Prepare data in PostgreSQL server"},{"location":"db_postgresql/#create-dataset-from-postgresql-server-and-use-it-in-tensorflow","text":"Create a Dataset from PostgreSQL server is as easy as calling tfio.experimental.IODataset.from_sql with query and endpoint arguments. The query is the SQL query for select columns in tables and the endpoint argument is the address and database name: import os import tensorflow_io as tfio endpoint=\"postgresql://{}:{}@{}?port={}&dbname={}\".format( os.environ['TFIO_DEMO_DATABASE_USER'], os.environ['TFIO_DEMO_DATABASE_PASS'], os.environ['TFIO_DEMO_DATABASE_HOST'], os.environ['TFIO_DEMO_DATABASE_PORT'], os.environ['TFIO_DEMO_DATABASE_NAME'], ) dataset = tfio.experimental.IODataset.from_sql( query=\"SELECT co, pt08s1 FROM AirQualityUCI;\", endpoint=endpoint) print(dataset.element_spec) As you could see from the output of dataset.element_spec above, the element of the created Dataset is a python dict object with column names of the database table as keys. It is quite convenient to apply further operations. For example, you could select both nox and no2 field of the Dataset , and calculate the difference: dataset = tfio.experimental.IODataset.from_sql( query=\"SELECT nox, no2 FROM AirQualityUCI;\", endpoint=endpoint) dataset = dataset.map(lambda e: (e['nox'] - e['no2']))","title":"Create Dataset from PostgreSQL server and use it in TensorFlow"},{"location":"db_postgresql/#check-only-the-first-20-record","text":"dataset = dataset.take(20) print(\"NOx - NO2:\") for difference in dataset: print(difference.numpy()) The created Dataset is ready to be passed to tf.keras directly for either training or inference purposes now.","title":"check only the first 20 record"},{"location":"db_postgresql/#references","text":"Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science. S. De Vito, E. Massera, M. Piga, L. Martinotto, G. Di Francia, On field calibration of an electronic nose for benzene estimation in an urban pollution monitoring scenario, Sensors and Actuators B: Chemical, Volume 129, Issue 2, 22 February 2008, Pages 750-757, ISSN 0925-4005","title":"References"},{"location":"db_sql/","text":"sqlite SQLite is an open-source library that provides a lightweight disk-based database. sqlite it doesn\u2019t require a separate server process allows accessing the database using a nonstandard variant of the SQL query language. provides a SQL interface compliant with the python's DB-API 2.0 specification described by PEP 249 . applications can use SQLite for internal data storage. It\u2019s also possible to prototype an application using SQLite and then port the code to a larger database such as PostgreSQL or Oracle. To use the module, you must first create a Connection object that represents the database. A connection object's key methods are: cursor() returns a cursor object that can execute queries and retrieve results commit() submits the current transaction to the DB. If you don\u2019t call this method, anything you did since the last call to commit() is not visible from other database connections. If you wonder why you don\u2019t see the data you\u2019ve written to the database, please check you didn\u2019t forget to call this method. rollback() rolls back any changes to the database since the last call to commit(). close() closes the database connection. Note that this does not automatically call commit(). If you just close your database connection without calling commit() first, your changes will be lost! in our example the data will be stored in a local file called example.db : import sqlite3 conn = sqlite3 . connect ( 'example.db' ) You can also supply the special name :memory: to create a database in RAM. Once you have a Connection, you can create a Cursor object and call its execute() method to perform SQL commands: c = conn . cursor () # Create table called stocks c . execute ( ''' CREATE TABLE stocks ( date text, trans text, symbol text, qty real, price real ) ''' ) # Save (commit) the changes conn . commit () # We can also close the connection if we are done with it. # Just be sure any changes have been committed or they will be lost. conn . close () since we don't want to forget closing the connection, here's a nice utility function that opens a connection and returns a handy cursor object from contextlib import contextmanager @contextmanager def sqlite3_connect ( database , * args , ** kwargs ): conn = sqlite3 . connect ( database , * args , ** kwargs ) try : cursor = conn . cursor () yield ( conn , cursor ) finally : conn . close () and here's how sqlite3_connect() can be used: with sqlite3_connect ( 'example.db' ) as [ conn , c ]: # Insert a row of data c . execute ( \"INSERT INTO stocks VALUES ('2008-01-05','BUY','AAPL',120,37.14)\" ) # Save (commit) the changes conn . commit () # closing is done for us Usually your SQL operations will need to use values from Python variables like people's names, and fields from forms. DO NOT assemble your SQL using Python\u2019s string operations because doing so is insecure; it makes your program vulnerable to an SQL injection attack Instead, use the API\u2019s parameter substitution - Put ? as a placeholder wherever you want to use a value, and then provide a tuple of values as the second argument to the cursor\u2019s execute() method. with sqlite3_connect ( 'example.db' ) as [ conn , c ]: # DO NOT use str.format or f-strings or any other way to embed your variables into strings # INSTEAD, use the sanitized substitution capability of the execute() function t = ( 'RHAT' ,) c . execute ( 'SELECT * FROM stocks WHERE symbol=?' , t ) print ( c . fetchone ()) # Larger example that inserts many records at a time purchases = [( '2006-03-28' , 'BUY' , 'IBM' , 1000 , 45.00 ), ( '2006-04-05' , 'BUY' , 'MSFT' , 1000 , 72.00 ), ( '2006-04-06' , 'SELL' , 'IBM' , 500 , 53.00 ), ] c . executemany ( 'INSERT INTO stocks VALUES (?,?,?,?,?)' , purchases ) conn . commit () To retrieve data after executing a SELECT statement, you can treat the cursor as an iterator: with sqlite3_connect ( 'example.db' ) as [ conn , c ]: for row in c . execute ( 'SELECT * FROM stocks ORDER BY price' ): print ( row ) you can also call the cursor\u2019s fetchone() method to retrieve a single matching row, or call fetchall() to get a list of the matching rows. Table metadata We can access all the tables, their fields and their types using a simple query with sqlite3_connect ( 'example.db' ) as [ conn , c ]: rs = c . execute ( \"\"\" SELECT name, sql FROM sqlite_master WHERE type='table' ORDER BY name; \"\"\" ) for name , sql , * args in rs : print ( name ) print ( sql ) SQLite and Python types SQLite natively supports the following types: NULL , INTEGER , REAL , TEXT , BLOB . The following Python types can thus be sent to SQLite without any problem: Python type SQLite type None NULL int INTEGER float REAL str TEXT bytes BLOB SQLite supports only a limited set of types natively. To use other Python types with SQLite, you must adapt them to one of the sqlite3 module\u2019s supported types for SQLite: one of NoneType , int , float , str , bytes . Custom types there are two ways to read/write objects from a database: 1. encoding the object into a text/blob column using some format (often JSON) 2. converting objects into tables rows (respecting foreign keys as potential links to other objects) with a DB-API based SQL interface like sqlite3, both of these methods can contain a lot of boiler plate code and so we will leave them out of thid tutorial pyodbc pip install pyodbc pyodbc is a python-ODBC bridge library that also supports the DB-API 2.0 specification. and can connect to a vast number of databases, including MS SQL Server, MySQL, PostgreSQL, Oracel, Google Big Data, SQLite, among others. supporting the DB-API 2.0 means that code that uses pyodbc can look almost identical to code using SQLite Open Database Connectivity (ODBC) is the standard that allows using identical (or at least very similar) SQL statements for querying different Databases (DBMS). The designers of ODBC aimed to make it independent of database systems and operating systems. ODBC accomplishes DBMS independence by using an ODBC driver as a translation layer between the application and the DBMS. The driver often has to be installed on the client operating system Example connections to connect to a DB, your often need to know the server/IP it is running on, the name of the datanase, and username/password to access the databse import pyodbc server = \"your_server\" db = \"your_db\" user = \"your_user\" password = \"your_password\" MS SQL Server connection_str = \\ 'DRIVER={ODBC Driver 17 for SQL Server};' + \\ f 'SERVER= { server } ;' \\ f 'DATABASE= { db } ;' \\ f 'UID= { user } ;' \\ f 'PWD= { password } ' print ( connection_str ) # # Connect to MS SQL Server # conn = pyodbc.connect(connection_str) MySQL connection_str = \\ \"DRIVER={MySQL ODBC 3.51 Driver};\" \\ f \"SERVER= { server } ;\" \\ f \"DATABASE= { db } ;\" \\ f \"UID= { user } ;\" \\ f \"PASSWORD= { password } ;\" print ( connection_str ) # # Connect to MySQL # conn = pyodbc.connect(connection_str) SQLite We don't to connect to SQLite via ODBC, because python can use these databases directly. however, if we want to show this as a demo, we need to install the SQLite ODBC driver For Windows, you can get the SQLite ODBC driver here . Download \"sqliteodbc.exe\" if you are using 32-bit Python, or \"sqliteodbc_w64.exe\" if you are using 64-bit Python. db = \"example.db\" connection_str = \\ \"Driver=SQLite3 ODBC Driver;\" \\ f \"Database= { db } \" print ( connection_str ) conn = pyodbc . connect ( connection_str ) c = conn . cursor () for row in c . execute ( 'SELECT * FROM stocks ORDER BY price' ): print ( row )","title":"SQL"},{"location":"db_sql/#sqlite","text":"SQLite is an open-source library that provides a lightweight disk-based database. sqlite it doesn\u2019t require a separate server process allows accessing the database using a nonstandard variant of the SQL query language. provides a SQL interface compliant with the python's DB-API 2.0 specification described by PEP 249 . applications can use SQLite for internal data storage. It\u2019s also possible to prototype an application using SQLite and then port the code to a larger database such as PostgreSQL or Oracle. To use the module, you must first create a Connection object that represents the database. A connection object's key methods are: cursor() returns a cursor object that can execute queries and retrieve results commit() submits the current transaction to the DB. If you don\u2019t call this method, anything you did since the last call to commit() is not visible from other database connections. If you wonder why you don\u2019t see the data you\u2019ve written to the database, please check you didn\u2019t forget to call this method. rollback() rolls back any changes to the database since the last call to commit(). close() closes the database connection. Note that this does not automatically call commit(). If you just close your database connection without calling commit() first, your changes will be lost! in our example the data will be stored in a local file called example.db : import sqlite3 conn = sqlite3 . connect ( 'example.db' ) You can also supply the special name :memory: to create a database in RAM. Once you have a Connection, you can create a Cursor object and call its execute() method to perform SQL commands: c = conn . cursor () # Create table called stocks c . execute ( ''' CREATE TABLE stocks ( date text, trans text, symbol text, qty real, price real ) ''' ) # Save (commit) the changes conn . commit () # We can also close the connection if we are done with it. # Just be sure any changes have been committed or they will be lost. conn . close () since we don't want to forget closing the connection, here's a nice utility function that opens a connection and returns a handy cursor object from contextlib import contextmanager @contextmanager def sqlite3_connect ( database , * args , ** kwargs ): conn = sqlite3 . connect ( database , * args , ** kwargs ) try : cursor = conn . cursor () yield ( conn , cursor ) finally : conn . close () and here's how sqlite3_connect() can be used: with sqlite3_connect ( 'example.db' ) as [ conn , c ]: # Insert a row of data c . execute ( \"INSERT INTO stocks VALUES ('2008-01-05','BUY','AAPL',120,37.14)\" ) # Save (commit) the changes conn . commit () # closing is done for us Usually your SQL operations will need to use values from Python variables like people's names, and fields from forms. DO NOT assemble your SQL using Python\u2019s string operations because doing so is insecure; it makes your program vulnerable to an SQL injection attack Instead, use the API\u2019s parameter substitution - Put ? as a placeholder wherever you want to use a value, and then provide a tuple of values as the second argument to the cursor\u2019s execute() method. with sqlite3_connect ( 'example.db' ) as [ conn , c ]: # DO NOT use str.format or f-strings or any other way to embed your variables into strings # INSTEAD, use the sanitized substitution capability of the execute() function t = ( 'RHAT' ,) c . execute ( 'SELECT * FROM stocks WHERE symbol=?' , t ) print ( c . fetchone ()) # Larger example that inserts many records at a time purchases = [( '2006-03-28' , 'BUY' , 'IBM' , 1000 , 45.00 ), ( '2006-04-05' , 'BUY' , 'MSFT' , 1000 , 72.00 ), ( '2006-04-06' , 'SELL' , 'IBM' , 500 , 53.00 ), ] c . executemany ( 'INSERT INTO stocks VALUES (?,?,?,?,?)' , purchases ) conn . commit () To retrieve data after executing a SELECT statement, you can treat the cursor as an iterator: with sqlite3_connect ( 'example.db' ) as [ conn , c ]: for row in c . execute ( 'SELECT * FROM stocks ORDER BY price' ): print ( row ) you can also call the cursor\u2019s fetchone() method to retrieve a single matching row, or call fetchall() to get a list of the matching rows.","title":"sqlite"},{"location":"db_sql/#table-metadata","text":"We can access all the tables, their fields and their types using a simple query with sqlite3_connect ( 'example.db' ) as [ conn , c ]: rs = c . execute ( \"\"\" SELECT name, sql FROM sqlite_master WHERE type='table' ORDER BY name; \"\"\" ) for name , sql , * args in rs : print ( name ) print ( sql )","title":"Table metadata"},{"location":"db_sql/#sqlite-and-python-types","text":"SQLite natively supports the following types: NULL , INTEGER , REAL , TEXT , BLOB . The following Python types can thus be sent to SQLite without any problem: Python type SQLite type None NULL int INTEGER float REAL str TEXT bytes BLOB SQLite supports only a limited set of types natively. To use other Python types with SQLite, you must adapt them to one of the sqlite3 module\u2019s supported types for SQLite: one of NoneType , int , float , str , bytes .","title":"SQLite and Python types"},{"location":"db_sql/#custom-types","text":"there are two ways to read/write objects from a database: 1. encoding the object into a text/blob column using some format (often JSON) 2. converting objects into tables rows (respecting foreign keys as potential links to other objects) with a DB-API based SQL interface like sqlite3, both of these methods can contain a lot of boiler plate code and so we will leave them out of thid tutorial","title":"Custom types"},{"location":"db_sql/#pyodbc","text":"pip install pyodbc pyodbc is a python-ODBC bridge library that also supports the DB-API 2.0 specification. and can connect to a vast number of databases, including MS SQL Server, MySQL, PostgreSQL, Oracel, Google Big Data, SQLite, among others. supporting the DB-API 2.0 means that code that uses pyodbc can look almost identical to code using SQLite Open Database Connectivity (ODBC) is the standard that allows using identical (or at least very similar) SQL statements for querying different Databases (DBMS). The designers of ODBC aimed to make it independent of database systems and operating systems. ODBC accomplishes DBMS independence by using an ODBC driver as a translation layer between the application and the DBMS. The driver often has to be installed on the client operating system","title":"pyodbc"},{"location":"db_sql/#example-connections","text":"to connect to a DB, your often need to know the server/IP it is running on, the name of the datanase, and username/password to access the databse import pyodbc server = \"your_server\" db = \"your_db\" user = \"your_user\" password = \"your_password\"","title":"Example connections"},{"location":"db_sql/#ms-sql-server","text":"connection_str = \\ 'DRIVER={ODBC Driver 17 for SQL Server};' + \\ f 'SERVER= { server } ;' \\ f 'DATABASE= { db } ;' \\ f 'UID= { user } ;' \\ f 'PWD= { password } ' print ( connection_str ) # # Connect to MS SQL Server # conn = pyodbc.connect(connection_str)","title":"MS SQL Server"},{"location":"db_sql/#mysql","text":"connection_str = \\ \"DRIVER={MySQL ODBC 3.51 Driver};\" \\ f \"SERVER= { server } ;\" \\ f \"DATABASE= { db } ;\" \\ f \"UID= { user } ;\" \\ f \"PASSWORD= { password } ;\" print ( connection_str ) # # Connect to MySQL # conn = pyodbc.connect(connection_str)","title":"MySQL"},{"location":"db_sql/#sqlite_1","text":"We don't to connect to SQLite via ODBC, because python can use these databases directly. however, if we want to show this as a demo, we need to install the SQLite ODBC driver For Windows, you can get the SQLite ODBC driver here . Download \"sqliteodbc.exe\" if you are using 32-bit Python, or \"sqliteodbc_w64.exe\" if you are using 64-bit Python. db = \"example.db\" connection_str = \\ \"Driver=SQLite3 ODBC Driver;\" \\ f \"Database= { db } \" print ( connection_str ) conn = pyodbc . connect ( connection_str ) c = conn . cursor () for row in c . execute ( 'SELECT * FROM stocks ORDER BY price' ): print ( row )","title":"SQLite"}]}