{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Data n Storage Refresher Everyone can forget about memorizing every query and command. What matters most is knowing where to find the right information. Mastering databases and storage systems isn\u2019t about remembering every syntax detail\u2014it\u2019s about understanding the core principles, exploring best practices, and knowing how to troubleshoot efficiently. With a solid foundation, a habit of continuous learning, and hands-on experimentation, you can refine your skills and confidently manage data in any environment. Basic Topics Advanced Topics Basics This material is a work in progress, so your feedback is welcome. The best way to provide that feedback is to click here and create an issue in this GitHub repository .","title":"Home"},{"location":"#welcome-to-data-n-storage-refresher","text":"Everyone can forget about memorizing every query and command. What matters most is knowing where to find the right information. Mastering databases and storage systems isn\u2019t about remembering every syntax detail\u2014it\u2019s about understanding the core principles, exploring best practices, and knowing how to troubleshoot efficiently. With a solid foundation, a habit of continuous learning, and hands-on experimentation, you can refine your skills and confidently manage data in any environment. Basic Topics Advanced Topics Basics This material is a work in progress, so your feedback is welcome. The best way to provide that feedback is to click here and create an issue in this GitHub repository .","title":"Welcome to Data n Storage Refresher"},{"location":"ai_frameworks/","text":"The most commonly used AI frameworks fall into several categories, including deep learning, machine learning, and natural language processing (NLP). Deep Learning Frameworks TensorFlow (Google) \u2013 A highly flexible and scalable framework widely used for deep learning, neural networks, and AI applications. PyTorch (Meta) \u2013 A popular deep learning framework known for its dynamic computation graph and ease of use, especially for research. JAX (Google) \u2013 A newer framework optimized for high-performance computing and automatic differentiation, often used in reinforcement learning and deep learning. MXNet (Apache) \u2013 Used by Amazon for deep learning tasks, but less popular than TensorFlow and PyTorch. ONNX (Open Neural Network Exchange) \u2013 A format for model interoperability across different AI frameworks. Machine Learning Frameworks Scikit-learn \u2013 A widely used framework for classical machine learning algorithms, such as regression, clustering, and classification. XGBoost \u2013 A powerful gradient boosting library for structured/tabular data. LightGBM \u2013 A fast and efficient gradient boosting framework optimized for performance. CatBoost \u2013 Another gradient boosting library, designed to work well with categorical features. NLP & Large Language Model (LLM) Frameworks Hugging Face Transformers \u2013 A leading NLP framework for pre-trained transformer models like BERT, GPT, and LLaMA. spaCy \u2013 A fast and efficient NLP library for tokenization, named entity recognition, and more. NLTK \u2013 A traditional NLP toolkit useful for research and academic work. Reinforcement Learning (RL) Frameworks Stable-Baselines3 \u2013 A PyTorch-based library for reinforcement learning. RLlib (Ray) \u2013 A scalable RL framework designed for production use. AutoML & AI Deployment AutoML frameworks \u2013 Include Auto-sklearn, TPOT, and H2O.ai for automated machine learning. MLflow \u2013 A tool for managing ML experiments and deployment. TensorFlow Serving & TorchServe \u2013 Used for deploying deep learning models in production. Computer Vision (CV) Frameworks OpenCV \u2013 The most widely used open-source computer vision library, supports image/video processing, object detection, and more. Detectron2 (Meta) \u2013 A powerful PyTorch-based library for object detection and segmentation. MMDetection (OpenMMLab) \u2013 A modular framework for object detection and instance segmentation. YOLO (You Only Look Once) \u2013 A family of real-time object detection models (YOLOv8 is the latest). Mediapipe (Google) \u2013 Optimized for real-time hand tracking, face detection, and pose estimation. Albumentations \u2013 A fast and efficient image augmentation library, often used in deep learning pipelines. Kornia \u2013 A PyTorch-based differentiable vision library for image transformations. SuperGradients (Deci) \u2013 A CV training library with pre-trained models for segmentation and detection. Ultralytics \u2013 Provides an easy-to-use API for training and deploying YOLO models. AI Cloud Services \u2601\ufe0f\ud83d\udca1 \ud83d\udd39 Major AI Cloud Platforms 1\ufe0f\u20e3 Google Cloud AI (Vertex AI & TensorFlow Cloud) Vertex AI \u2013 Unified ML platform for training, deploying, and managing models. AutoML \u2013 No-code/low-code ML model training for vision, NLP, and structured data. TensorFlow Cloud \u2013 Seamlessly runs TensorFlow models on Google Cloud infrastructure. TPUs (Tensor Processing Units) \u2013 Specialized hardware for deep learning acceleration. \u2705 Best for: Scalable deep learning, AutoML, and enterprise AI. 2\ufe0f\u20e3 AWS AI & ML Services SageMaker \u2013 Fully managed service for building, training, and deploying ML models. Rekognition \u2013 Image and video analysis (face detection, object recognition, etc.). Lex \u2013 Conversational AI for chatbots (based on Alexa). Comprehend \u2013 NLP for sentiment analysis, entity recognition, etc. Transcribe & Polly \u2013 Speech-to-text and text-to-speech services. Inferentia \u2013 Custom chips for AI inference acceleration. \u2705 Best for: Scalable ML training, NLP, and real-time vision applications. 3\ufe0f\u20e3 Microsoft Azure AI Azure Machine Learning \u2013 End-to-end ML lifecycle management. Cognitive Services \u2013 Pre-trained AI models for vision, speech, NLP, and decision-making. Azure OpenAI Service \u2013 Provides access to OpenAI\u2019s GPT models. Computer Vision API \u2013 Object detection, OCR, and facial recognition. Speech Services \u2013 Text-to-speech, speech recognition, and translation. \u2705 Best for: Enterprises using Microsoft ecosystems (Windows, Office, etc.). \ud83d\udd39 Specialized Cloud AI Services 4\ufe0f\u20e3 OpenAI API (ChatGPT, DALL\u00b7E, Whisper) GPT-4 & GPT-3.5 \u2013 Text generation and conversational AI. DALL\u00b7E \u2013 AI-powered image generation. Whisper \u2013 AI-powered speech-to-text. \u2705 Best for: NLP, chatbots, and AI-generated content. 5\ufe0f\u20e3 Hugging Face Hub Inference API \u2013 Deploys pre-trained AI models instantly. Spaces \u2013 Hosts AI apps using Gradio and Streamlit. AutoTrain \u2013 Automates model fine-tuning for NLP, vision, and tabular data. \u2705 Best for: Rapid AI prototyping and model sharing. 6\ufe0f\u20e3 NVIDIA AI Cloud (NIM & DGX Cloud) DGX Cloud \u2013 On-demand GPU clusters for AI model training. NVIDIA AI Enterprise \u2013 Pre-trained models for speech, CV, and recommendation systems. RAPIDS \u2013 GPU-accelerated ML for big data processing. \u2705 Best for: High-performance AI training (e.g., deep learning, reinforcement learning). 7\ufe0f\u20e3 IBM Watson AI Watson Studio \u2013 AI/ML model training with AutoML and cloud deployment. Watson Assistant \u2013 AI-powered chatbots. Watson NLP \u2013 Pre-trained NLP models. \u2705 Best for: Enterprise AI applications and NLP-driven automation. TensorFlow vs PyTorch https://opencv.org/blog/pytorch-vs-tensorflow/","title":"Frameworks"},{"location":"ai_frameworks/#deep-learning-frameworks","text":"TensorFlow (Google) \u2013 A highly flexible and scalable framework widely used for deep learning, neural networks, and AI applications. PyTorch (Meta) \u2013 A popular deep learning framework known for its dynamic computation graph and ease of use, especially for research. JAX (Google) \u2013 A newer framework optimized for high-performance computing and automatic differentiation, often used in reinforcement learning and deep learning. MXNet (Apache) \u2013 Used by Amazon for deep learning tasks, but less popular than TensorFlow and PyTorch. ONNX (Open Neural Network Exchange) \u2013 A format for model interoperability across different AI frameworks.","title":"Deep Learning Frameworks"},{"location":"ai_frameworks/#machine-learning-frameworks","text":"Scikit-learn \u2013 A widely used framework for classical machine learning algorithms, such as regression, clustering, and classification. XGBoost \u2013 A powerful gradient boosting library for structured/tabular data. LightGBM \u2013 A fast and efficient gradient boosting framework optimized for performance. CatBoost \u2013 Another gradient boosting library, designed to work well with categorical features.","title":"Machine Learning Frameworks"},{"location":"ai_frameworks/#nlp-large-language-model-llm-frameworks","text":"Hugging Face Transformers \u2013 A leading NLP framework for pre-trained transformer models like BERT, GPT, and LLaMA. spaCy \u2013 A fast and efficient NLP library for tokenization, named entity recognition, and more. NLTK \u2013 A traditional NLP toolkit useful for research and academic work.","title":"NLP &amp; Large Language Model (LLM) Frameworks"},{"location":"ai_frameworks/#reinforcement-learning-rl-frameworks","text":"Stable-Baselines3 \u2013 A PyTorch-based library for reinforcement learning. RLlib (Ray) \u2013 A scalable RL framework designed for production use.","title":"Reinforcement Learning (RL) Frameworks"},{"location":"ai_frameworks/#automl-ai-deployment","text":"AutoML frameworks \u2013 Include Auto-sklearn, TPOT, and H2O.ai for automated machine learning. MLflow \u2013 A tool for managing ML experiments and deployment. TensorFlow Serving & TorchServe \u2013 Used for deploying deep learning models in production.","title":"AutoML &amp; AI Deployment"},{"location":"ai_frameworks/#computer-vision-cv-frameworks","text":"OpenCV \u2013 The most widely used open-source computer vision library, supports image/video processing, object detection, and more. Detectron2 (Meta) \u2013 A powerful PyTorch-based library for object detection and segmentation. MMDetection (OpenMMLab) \u2013 A modular framework for object detection and instance segmentation. YOLO (You Only Look Once) \u2013 A family of real-time object detection models (YOLOv8 is the latest). Mediapipe (Google) \u2013 Optimized for real-time hand tracking, face detection, and pose estimation. Albumentations \u2013 A fast and efficient image augmentation library, often used in deep learning pipelines. Kornia \u2013 A PyTorch-based differentiable vision library for image transformations. SuperGradients (Deci) \u2013 A CV training library with pre-trained models for segmentation and detection. Ultralytics \u2013 Provides an easy-to-use API for training and deploying YOLO models.","title":"Computer Vision (CV) Frameworks"},{"location":"ai_frameworks/#ai-cloud-services","text":"","title":"AI Cloud Services \u2601\ufe0f\ud83d\udca1"},{"location":"ai_frameworks/#major-ai-cloud-platforms","text":"","title":"\ud83d\udd39 Major AI Cloud Platforms"},{"location":"ai_frameworks/#1-google-cloud-ai-vertex-ai-tensorflow-cloud","text":"Vertex AI \u2013 Unified ML platform for training, deploying, and managing models. AutoML \u2013 No-code/low-code ML model training for vision, NLP, and structured data. TensorFlow Cloud \u2013 Seamlessly runs TensorFlow models on Google Cloud infrastructure. TPUs (Tensor Processing Units) \u2013 Specialized hardware for deep learning acceleration. \u2705 Best for: Scalable deep learning, AutoML, and enterprise AI.","title":"1\ufe0f\u20e3 Google Cloud AI (Vertex AI &amp; TensorFlow Cloud)"},{"location":"ai_frameworks/#2-aws-ai-ml-services","text":"SageMaker \u2013 Fully managed service for building, training, and deploying ML models. Rekognition \u2013 Image and video analysis (face detection, object recognition, etc.). Lex \u2013 Conversational AI for chatbots (based on Alexa). Comprehend \u2013 NLP for sentiment analysis, entity recognition, etc. Transcribe & Polly \u2013 Speech-to-text and text-to-speech services. Inferentia \u2013 Custom chips for AI inference acceleration. \u2705 Best for: Scalable ML training, NLP, and real-time vision applications.","title":"2\ufe0f\u20e3 AWS AI &amp; ML Services"},{"location":"ai_frameworks/#3-microsoft-azure-ai","text":"Azure Machine Learning \u2013 End-to-end ML lifecycle management. Cognitive Services \u2013 Pre-trained AI models for vision, speech, NLP, and decision-making. Azure OpenAI Service \u2013 Provides access to OpenAI\u2019s GPT models. Computer Vision API \u2013 Object detection, OCR, and facial recognition. Speech Services \u2013 Text-to-speech, speech recognition, and translation. \u2705 Best for: Enterprises using Microsoft ecosystems (Windows, Office, etc.).","title":"3\ufe0f\u20e3 Microsoft Azure AI"},{"location":"ai_frameworks/#specialized-cloud-ai-services","text":"","title":"\ud83d\udd39 Specialized Cloud AI Services"},{"location":"ai_frameworks/#4-openai-api-chatgpt-dalle-whisper","text":"GPT-4 & GPT-3.5 \u2013 Text generation and conversational AI. DALL\u00b7E \u2013 AI-powered image generation. Whisper \u2013 AI-powered speech-to-text. \u2705 Best for: NLP, chatbots, and AI-generated content.","title":"4\ufe0f\u20e3 OpenAI API (ChatGPT, DALL\u00b7E, Whisper)"},{"location":"ai_frameworks/#5-hugging-face-hub","text":"Inference API \u2013 Deploys pre-trained AI models instantly. Spaces \u2013 Hosts AI apps using Gradio and Streamlit. AutoTrain \u2013 Automates model fine-tuning for NLP, vision, and tabular data. \u2705 Best for: Rapid AI prototyping and model sharing.","title":"5\ufe0f\u20e3 Hugging Face Hub"},{"location":"ai_frameworks/#6-nvidia-ai-cloud-nim-dgx-cloud","text":"DGX Cloud \u2013 On-demand GPU clusters for AI model training. NVIDIA AI Enterprise \u2013 Pre-trained models for speech, CV, and recommendation systems. RAPIDS \u2013 GPU-accelerated ML for big data processing. \u2705 Best for: High-performance AI training (e.g., deep learning, reinforcement learning).","title":"6\ufe0f\u20e3 NVIDIA AI Cloud (NIM &amp; DGX Cloud)"},{"location":"ai_frameworks/#7-ibm-watson-ai","text":"Watson Studio \u2013 AI/ML model training with AutoML and cloud deployment. Watson Assistant \u2013 AI-powered chatbots. Watson NLP \u2013 Pre-trained NLP models. \u2705 Best for: Enterprise AI applications and NLP-driven automation. TensorFlow vs PyTorch https://opencv.org/blog/pytorch-vs-tensorflow/","title":"7\ufe0f\u20e3 IBM Watson AI"},{"location":"db/db_basic/","text":"How to choose DB Integration The most important thing to consider while choosing the right database is what system you need to integrate together? Make sure that your database management system can be integrated with other tools and services within your project. Different technologies have different connectors for different other technologies. For example, if you have a big analytics job that\u2019s currently running an Apache spark then probably you want to limit yourself to external databases that can connect easily to apache spark. Scaling Requirement It\u2019s important to know the scaling requirement before installing your production database. How much data are you really talking about? Is it really going to grow unbounded over time? if so then you need some sort of database technology that is not limited to the data that you can store on one PC. You need to look at something like Cassandra or MongoDB or HBase where you can actually distribute the storage of your data across an entire cluster and scale horizontally instead of vertically. While choosing a database you also need to think about the transaction rate or throughput which means how many requests you intend to get per second. Databases with high throughput can support many simultaneous users. If we are talking about thousands then again a single database service is not going to work out. This is especially important when you are working on some big websites where we have a lot of web servers that are serving a lot of people at the same time. You will have to choose a database that is distributed and allows you to spread out a load of those transactions more evenly. In those situations, NoSQL databases are a good choice instead of RDBMS. Support Consideration Think about the supports you might need for your database. Do you have the in-house expertise to spin up this new technology and actually configure it properly? It\u2019s going to be harder than you think especially if you\u2019re using this in the real world or any sort of situation where you have personally identifiable information in the mix from your end-users. In that case, you need to make sure you\u2019re thinking about the security of your system. The truth is most of the NoSQL database we\u2019ve talked about if you configure them with their default settings there will be no security at all. CAP Consideration CAP stands for Consistency, Availability, and Partition tolerance. The theorem states that you cannot achieve all the properties at the best level in a single database, as there are natural trade offs between the items. You can only pick two out of three at a time and that totally depends on your prioritize based on your requirements. For example, if your system needs to be available and partition tolerant, then you must be willing to accept some latency in your consistency requirements. Traditional relational databases are a natural fit for the CA side whereas Non-relational database engines mostly satisfy AP and CP requirements. Consistency means that any read request will return the most recent write. Data consistency is usually \u201cstrong\u201d for SQL databases and for NoSQL database consistency may be anything from \u201ceventual\u201d to \u201cstrong\u201d. Availability means that a non-responding node must respond in a reasonable amount of time. Not every application needs to run 24/7 with 99.999% availability but most likely you will prefer a database with higher availability. Partition tolerance means the system will continue to operate despite network or node failures. Schemas or Data Model Relational databases store data in a fixed and predefined structure. It means when you start development you will have to define your data schema in terms of tables and columns. You have to change the schema every time the requirements change. This will lead to creating new columns, defining new relations, reflecting the changes in your application, discussing with your database administrators, etc. NoSQL database provides much more flexibility when it comes to handling data. There is no requirement to specify the schema to start working with the application. Also, the NoSQL database doesn\u2019t put a restriction on the types of data you can store together. It allows you to add more new types as your needs change. In the application building process, most of the developers prefer high coding velocity and great agility. NoSQL databases have proven to be a much better choice in that regard especially for agile development which requires fast implementation. Types of DB You have a variety of options available in relational (MySQL, PostgreSQL, Oracle DB, etc) and non-relational (MongoDB, Apache HBase, Cassandra, etc) database but you need to understand none of them fits on all kinds of projects requirement. Each one of them has some strengths and weaknesses. Databases through two lenses: access characteristics and the pattern of the data being stored. Relational With large spans of usage, relational databases are still the dominant database type today. A relational database is self-describing because it enables developers to define the database's schema as well as relations and constraints between rows and tables in the database. Developers rely on the functionality of the relational database and not the application code to enforce the schema and preserve the referential integrity of the data within the database. Typical use cases for a relational database include web and mobile applications, enterprise applications, and online gaming. Various flavors or versions of Amazon RDS and Amazon Aurora are used by startups for high-performance and scalable applications on AWS. Both RDS and Aurora are fully managed, scalable systems. MySQL Relational (+Document since 5.7.8) SQL with JOINS JSON type support (since 5.7.8) Open source (with proprietary, closed-sourced modules) When to use MySQL When you already widely use it in your organization When you want both relational tables (when you know the schema upfront) and JSON collections (Schemaless) Relational / normalized \u2014 when you need to optimize on writes instead of reads, to have strong read consistency MySQL Advantages Maturity & Reliability \u2014 MySQL is highly used, battle tested and mature Fast read performance Improved JSON/Document support (MySQL 8) Cross-DC write consistency (when ProxySQL is used) MySQL Disadvantages Scalability \u2014 Does not scale horizontally. Limited by amount of disk space Consistency and Replication Issues (when not using ProxySQL) Other DBs in this category PostgreSQL, MariaDB, SQL Server, Oracle, Db2, SQLite Key-value As your system grows, large amounts of data are often in the form of key-value data, where a single row maps to a primary key. Key-value databases are highly partitionable and allow horizontal scaling at levels that other types of databases cannot achieve. Use cases such as gaming, ad tech, and IoT lend themselves particularly well to the key-value data model where the access patterns require low-latency Gets/Puts for known key values. Amazon DynamoDB is a managed key-value and document database that delivers single-digit millisecond performance at any scale. Key-value DBs store data in pairs, each containing a unique ID and a data value. These DBs provide a flexible storage structure since values can store any amount of unstructured data. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Session management, user preferences, and product recommendations. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Amazon DynamoDB, Azure Cosmos DB. Amazon DynamoDB Other DBs in this category Cassandra, HBase, Redis (Key-value Store) When to use DynamoDB When you need a simple key value store without complex querying patterns When you need to store expirable data Low-medium throughput apps as writes are expensive and consistent reads are twice the cost of eventually consistent reads. DynamoDB Advantages Fast performance in any scale (as long as enough capacity is provisioned) No storage limit Schemaless \u2014 it\u2019s possible to define a schema for each item, rather than for the whole table +Multi-master replication (update data in multiple regions) Supports TTL per item Built-in CDC events (DynamoDB streams) DynamoDB Disadvantages Size limit \u2014 item can only reach 400KB in size Limited querying options (limited number of indices) Throttling on burst throughput (and hot keys in certain situations) Amazon Simple Storage Service (S3) Other DBs in this category Google Cloud Storage, Azure Blob Storage When to use S3 When you need to store large binary objects/files (up to 5TB each) When the amount of data you need to store is large (>10TB), continues to grow daily, and may need to be retrieved (can\u2019t be deleted) S3 Advantages Supports very high throughput Infinite scalability \u2014 No limit on amount of storage S3 Disadvantages No Query support, only key-based retrieval Latency is 100\u2013200 ms for small objects. Caching can alleviate this Document Document databases are intuitive for developers to use, because the data in the application tier is typically represented as a JSON document. Developers can persist data using the same document model format that they use in their application code and use the flexible schema model of Amazon DocumentDB to achieve developer efficiency. \ud835\uddd7\ud835\uddfc\ud835\uddf0\ud835\ude02\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00 Document databases are structured similarly to key-value databases except that keys and values are stored in documents written in a markup language like JSON, XML, or YAML. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: User profiles, product catalogs, and content management. Examples: MongoDB, Amazon DocumentDB MongoDB When to use MongoDB When data schema is predicted to keep changing and evolving When working with dynamic JSON content When keeping data denormalized is not a problem. i.e. to have eventual consistency MongoDB Advantages Flexibility \u2014 with Schemaless documents, the number of fields, content and size of the document can differ from one document to another in the same collection. Easy to scale with sharding MongoDB Disadvantages High Memory Usage \u2014 a lot of denormalized data is kept in memory Document size limit \u2014 16MB Non optimal replication solution (data cannot be re-replicated after recovery from failure). Consistency issues on traffic switch to another data center (No automatic remaster) Graph databases A graph database's purpose is to make it easy to build and run applications that work with highly connected data sets. Typical use cases for a graph database include social networking, recommendation engines, fraud detection, and knowledge graphs. Amazon Neptune is a fully managed graph database service. Neptune supports both the Property Graph model and the Resource Description Framework (RDF), giving you the choice of two graph APIs: TinkerPop and RDF/SPARQL. Startups use Amazon Neptune to build knowledge graphs, make in-game offer recommendations, and detect fraud. \ud835\uddda\ud835\uddff\ud835\uddee\ud835\uddfd\ud835\uddf5 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00 Graph databases map the relationships between data using nodes and edges. Nodes are the individual data values, and edges are the relationships between those values. Use cases : Social graphs, recommendation engines, and fraud detection. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: New4j, Amazon Neptune, Azure Gremlin. In-memory databases Financial services, ecommerce, web, and mobile applications have use cases such as leaderboards, session stores, and real-time analytics that require microsecond response times and can have large spikes in traffic coming at any time. We built Amazon ElastiCache, offering Memcached and Redis, to serve low latency, high throughput workloads that cannot be served with disk-based data stores. Amazon DynamoDB Accelerator (DAX) is another example of a purpose-built data store. DAX was built to make DynamoDB reads an order of magnitude faster, from milliseconds to microseconds, even at millions of requests per second. \ud835\udddc\ud835\uddfb-\ud835\uddfa\ud835\uddf2\ud835\uddfa\ud835\uddfc\ud835\uddff\ud835\ude06 \ud835\uddde\ud835\uddf2\ud835\ude06-\ud835\udde9\ud835\uddee\ud835\uddf9\ud835\ude02\ud835\uddf2 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00 The data is primarily stored in memory, unlike disk-based databases. By eliminating disk access, these databases enable minimal response times. Because all data is stored in main memory, in-memory databases risk losing data upon a process or server failure. In-memory databases can persist data on disks by storing each operation in a log or by taking snapshots. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Redis, Memcached, Amazon ElastiCache Search Databases Many applications output logs to help developers troubleshoot issues. Amazon Elasticsearch Service, or Amazon ES, is purpose-built for providing near real-time visualizations and analytics of machine-generated data by indexing, aggregating, and searching semi-structured logs and metrics. Amazon ES is also a powerful, high-performance search engine for full-text search use cases. Startups store billions of documents for a variety of mission-critical use cases, ranging from operational monitoring and troubleshooting to distributed application stack tracing and pricing optimization. Amazon ElasticSearch Other DBs in this category Apache Solr, Splunk, Amazon CloudSearch When to use Elasticsearch When you need to perform fuzzy search or have results with ranking When you have another data store as source of truth (populate Elasticsearch as a materialized view) Elasticsearch Advantages Easy to horizontally scale with index sharding Rich search API Query for analytical data using aggregations Elasticsearch Disadvantages Indexes are created with a predefined number of shards. More shards requires migration to a new index. Usually done with ReIndex API Performance issues when indices hit very large scale (> 1TB with hundreds of nodes and shards) Wide Column Databases Wide column databases are based on tables but without a strict column format. Rows do not need a value in every column and segments of rows and columns containing different data formats can be combined. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Telemetry, analytics data, messaging, and time-series data. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Apache Cassandra, Azure Table Storage, HBase Time Series Databases These DBs store data in time-ordered streams. Data is not sorted by value or ID but by the time of collection, ingestion, or other timestamps included in the metadata. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Industrial telemetry, DevOps, and Internet of things (IoT) applications. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Graphite, Prometheus, Amazon Timestream Ledger Databases Ledger databases are based on logs that record events related to data values. These DBs store data changes that are used to verify the integrity of data. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Banking systems, registrations, supply chains, and systems of record. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Amazon Quantum Ledger Database (QLDB)","title":"General"},{"location":"db/db_basic/#how-to-choose-db","text":"","title":"How to choose DB"},{"location":"db/db_basic/#integration","text":"The most important thing to consider while choosing the right database is what system you need to integrate together? Make sure that your database management system can be integrated with other tools and services within your project. Different technologies have different connectors for different other technologies. For example, if you have a big analytics job that\u2019s currently running an Apache spark then probably you want to limit yourself to external databases that can connect easily to apache spark.","title":"Integration"},{"location":"db/db_basic/#scaling-requirement","text":"It\u2019s important to know the scaling requirement before installing your production database. How much data are you really talking about? Is it really going to grow unbounded over time? if so then you need some sort of database technology that is not limited to the data that you can store on one PC. You need to look at something like Cassandra or MongoDB or HBase where you can actually distribute the storage of your data across an entire cluster and scale horizontally instead of vertically. While choosing a database you also need to think about the transaction rate or throughput which means how many requests you intend to get per second. Databases with high throughput can support many simultaneous users. If we are talking about thousands then again a single database service is not going to work out. This is especially important when you are working on some big websites where we have a lot of web servers that are serving a lot of people at the same time. You will have to choose a database that is distributed and allows you to spread out a load of those transactions more evenly. In those situations, NoSQL databases are a good choice instead of RDBMS.","title":"Scaling Requirement"},{"location":"db/db_basic/#support-consideration","text":"Think about the supports you might need for your database. Do you have the in-house expertise to spin up this new technology and actually configure it properly? It\u2019s going to be harder than you think especially if you\u2019re using this in the real world or any sort of situation where you have personally identifiable information in the mix from your end-users. In that case, you need to make sure you\u2019re thinking about the security of your system. The truth is most of the NoSQL database we\u2019ve talked about if you configure them with their default settings there will be no security at all.","title":"Support Consideration"},{"location":"db/db_basic/#cap-consideration","text":"CAP stands for Consistency, Availability, and Partition tolerance. The theorem states that you cannot achieve all the properties at the best level in a single database, as there are natural trade offs between the items. You can only pick two out of three at a time and that totally depends on your prioritize based on your requirements. For example, if your system needs to be available and partition tolerant, then you must be willing to accept some latency in your consistency requirements. Traditional relational databases are a natural fit for the CA side whereas Non-relational database engines mostly satisfy AP and CP requirements. Consistency means that any read request will return the most recent write. Data consistency is usually \u201cstrong\u201d for SQL databases and for NoSQL database consistency may be anything from \u201ceventual\u201d to \u201cstrong\u201d. Availability means that a non-responding node must respond in a reasonable amount of time. Not every application needs to run 24/7 with 99.999% availability but most likely you will prefer a database with higher availability. Partition tolerance means the system will continue to operate despite network or node failures.","title":"CAP Consideration"},{"location":"db/db_basic/#schemas-or-data-model","text":"Relational databases store data in a fixed and predefined structure. It means when you start development you will have to define your data schema in terms of tables and columns. You have to change the schema every time the requirements change. This will lead to creating new columns, defining new relations, reflecting the changes in your application, discussing with your database administrators, etc. NoSQL database provides much more flexibility when it comes to handling data. There is no requirement to specify the schema to start working with the application. Also, the NoSQL database doesn\u2019t put a restriction on the types of data you can store together. It allows you to add more new types as your needs change. In the application building process, most of the developers prefer high coding velocity and great agility. NoSQL databases have proven to be a much better choice in that regard especially for agile development which requires fast implementation.","title":"Schemas or Data Model"},{"location":"db/db_basic/#types-of-db","text":"You have a variety of options available in relational (MySQL, PostgreSQL, Oracle DB, etc) and non-relational (MongoDB, Apache HBase, Cassandra, etc) database but you need to understand none of them fits on all kinds of projects requirement. Each one of them has some strengths and weaknesses. Databases through two lenses: access characteristics and the pattern of the data being stored.","title":"Types of DB"},{"location":"db/db_basic/#relational","text":"With large spans of usage, relational databases are still the dominant database type today. A relational database is self-describing because it enables developers to define the database's schema as well as relations and constraints between rows and tables in the database. Developers rely on the functionality of the relational database and not the application code to enforce the schema and preserve the referential integrity of the data within the database. Typical use cases for a relational database include web and mobile applications, enterprise applications, and online gaming. Various flavors or versions of Amazon RDS and Amazon Aurora are used by startups for high-performance and scalable applications on AWS. Both RDS and Aurora are fully managed, scalable systems.","title":"Relational"},{"location":"db/db_basic/#mysql","text":"Relational (+Document since 5.7.8) SQL with JOINS JSON type support (since 5.7.8) Open source (with proprietary, closed-sourced modules) When to use MySQL When you already widely use it in your organization When you want both relational tables (when you know the schema upfront) and JSON collections (Schemaless) Relational / normalized \u2014 when you need to optimize on writes instead of reads, to have strong read consistency MySQL Advantages Maturity & Reliability \u2014 MySQL is highly used, battle tested and mature Fast read performance Improved JSON/Document support (MySQL 8) Cross-DC write consistency (when ProxySQL is used) MySQL Disadvantages Scalability \u2014 Does not scale horizontally. Limited by amount of disk space Consistency and Replication Issues (when not using ProxySQL) Other DBs in this category PostgreSQL, MariaDB, SQL Server, Oracle, Db2, SQLite","title":"MySQL"},{"location":"db/db_basic/#key-value","text":"As your system grows, large amounts of data are often in the form of key-value data, where a single row maps to a primary key. Key-value databases are highly partitionable and allow horizontal scaling at levels that other types of databases cannot achieve. Use cases such as gaming, ad tech, and IoT lend themselves particularly well to the key-value data model where the access patterns require low-latency Gets/Puts for known key values. Amazon DynamoDB is a managed key-value and document database that delivers single-digit millisecond performance at any scale. Key-value DBs store data in pairs, each containing a unique ID and a data value. These DBs provide a flexible storage structure since values can store any amount of unstructured data. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Session management, user preferences, and product recommendations. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Amazon DynamoDB, Azure Cosmos DB.","title":"Key-value"},{"location":"db/db_basic/#amazon-dynamodb","text":"Other DBs in this category Cassandra, HBase, Redis (Key-value Store) When to use DynamoDB When you need a simple key value store without complex querying patterns When you need to store expirable data Low-medium throughput apps as writes are expensive and consistent reads are twice the cost of eventually consistent reads. DynamoDB Advantages Fast performance in any scale (as long as enough capacity is provisioned) No storage limit Schemaless \u2014 it\u2019s possible to define a schema for each item, rather than for the whole table +Multi-master replication (update data in multiple regions) Supports TTL per item Built-in CDC events (DynamoDB streams) DynamoDB Disadvantages Size limit \u2014 item can only reach 400KB in size Limited querying options (limited number of indices) Throttling on burst throughput (and hot keys in certain situations)","title":"Amazon DynamoDB"},{"location":"db/db_basic/#amazon-simple-storage-service-s3","text":"Other DBs in this category Google Cloud Storage, Azure Blob Storage When to use S3 When you need to store large binary objects/files (up to 5TB each) When the amount of data you need to store is large (>10TB), continues to grow daily, and may need to be retrieved (can\u2019t be deleted) S3 Advantages Supports very high throughput Infinite scalability \u2014 No limit on amount of storage S3 Disadvantages No Query support, only key-based retrieval Latency is 100\u2013200 ms for small objects. Caching can alleviate this","title":"Amazon Simple Storage Service (S3)"},{"location":"db/db_basic/#document","text":"Document databases are intuitive for developers to use, because the data in the application tier is typically represented as a JSON document. Developers can persist data using the same document model format that they use in their application code and use the flexible schema model of Amazon DocumentDB to achieve developer efficiency. \ud835\uddd7\ud835\uddfc\ud835\uddf0\ud835\ude02\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00 Document databases are structured similarly to key-value databases except that keys and values are stored in documents written in a markup language like JSON, XML, or YAML. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: User profiles, product catalogs, and content management. Examples: MongoDB, Amazon DocumentDB","title":"Document"},{"location":"db/db_basic/#mongodb","text":"When to use MongoDB When data schema is predicted to keep changing and evolving When working with dynamic JSON content When keeping data denormalized is not a problem. i.e. to have eventual consistency MongoDB Advantages Flexibility \u2014 with Schemaless documents, the number of fields, content and size of the document can differ from one document to another in the same collection. Easy to scale with sharding MongoDB Disadvantages High Memory Usage \u2014 a lot of denormalized data is kept in memory Document size limit \u2014 16MB Non optimal replication solution (data cannot be re-replicated after recovery from failure). Consistency issues on traffic switch to another data center (No automatic remaster)","title":"MongoDB"},{"location":"db/db_basic/#graph-databases","text":"A graph database's purpose is to make it easy to build and run applications that work with highly connected data sets. Typical use cases for a graph database include social networking, recommendation engines, fraud detection, and knowledge graphs. Amazon Neptune is a fully managed graph database service. Neptune supports both the Property Graph model and the Resource Description Framework (RDF), giving you the choice of two graph APIs: TinkerPop and RDF/SPARQL. Startups use Amazon Neptune to build knowledge graphs, make in-game offer recommendations, and detect fraud. \ud835\uddda\ud835\uddff\ud835\uddee\ud835\uddfd\ud835\uddf5 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00 Graph databases map the relationships between data using nodes and edges. Nodes are the individual data values, and edges are the relationships between those values. Use cases : Social graphs, recommendation engines, and fraud detection. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: New4j, Amazon Neptune, Azure Gremlin.","title":"Graph databases"},{"location":"db/db_basic/#in-memory-databases","text":"Financial services, ecommerce, web, and mobile applications have use cases such as leaderboards, session stores, and real-time analytics that require microsecond response times and can have large spikes in traffic coming at any time. We built Amazon ElastiCache, offering Memcached and Redis, to serve low latency, high throughput workloads that cannot be served with disk-based data stores. Amazon DynamoDB Accelerator (DAX) is another example of a purpose-built data store. DAX was built to make DynamoDB reads an order of magnitude faster, from milliseconds to microseconds, even at millions of requests per second. \ud835\udddc\ud835\uddfb-\ud835\uddfa\ud835\uddf2\ud835\uddfa\ud835\uddfc\ud835\uddff\ud835\ude06 \ud835\uddde\ud835\uddf2\ud835\ude06-\ud835\udde9\ud835\uddee\ud835\uddf9\ud835\ude02\ud835\uddf2 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00 The data is primarily stored in memory, unlike disk-based databases. By eliminating disk access, these databases enable minimal response times. Because all data is stored in main memory, in-memory databases risk losing data upon a process or server failure. In-memory databases can persist data on disks by storing each operation in a log or by taking snapshots. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Redis, Memcached, Amazon ElastiCache","title":"In-memory databases"},{"location":"db/db_basic/#search-databases","text":"Many applications output logs to help developers troubleshoot issues. Amazon Elasticsearch Service, or Amazon ES, is purpose-built for providing near real-time visualizations and analytics of machine-generated data by indexing, aggregating, and searching semi-structured logs and metrics. Amazon ES is also a powerful, high-performance search engine for full-text search use cases. Startups store billions of documents for a variety of mission-critical use cases, ranging from operational monitoring and troubleshooting to distributed application stack tracing and pricing optimization.","title":"Search Databases"},{"location":"db/db_basic/#amazon-elasticsearch","text":"Other DBs in this category Apache Solr, Splunk, Amazon CloudSearch When to use Elasticsearch When you need to perform fuzzy search or have results with ranking When you have another data store as source of truth (populate Elasticsearch as a materialized view) Elasticsearch Advantages Easy to horizontally scale with index sharding Rich search API Query for analytical data using aggregations Elasticsearch Disadvantages Indexes are created with a predefined number of shards. More shards requires migration to a new index. Usually done with ReIndex API Performance issues when indices hit very large scale (> 1TB with hundreds of nodes and shards)","title":"Amazon ElasticSearch"},{"location":"db/db_basic/#wide-column-databases","text":"Wide column databases are based on tables but without a strict column format. Rows do not need a value in every column and segments of rows and columns containing different data formats can be combined. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Telemetry, analytics data, messaging, and time-series data. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Apache Cassandra, Azure Table Storage, HBase","title":"Wide Column Databases"},{"location":"db/db_basic/#time-series-databases","text":"These DBs store data in time-ordered streams. Data is not sorted by value or ID but by the time of collection, ingestion, or other timestamps included in the metadata. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Industrial telemetry, DevOps, and Internet of things (IoT) applications. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Graphite, Prometheus, Amazon Timestream","title":"Time Series Databases"},{"location":"db/db_basic/#ledger-databases","text":"Ledger databases are based on logs that record events related to data values. These DBs store data changes that are used to verify the integrity of data. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Banking systems, registrations, supply chains, and systems of record. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Amazon Quantum Ledger Database (QLDB)","title":"Ledger Databases"},{"location":"db/nosql/db_redis/","text":"import sys IN_COLAB = 'google.colab' in sys . modules if IN_COLAB : ! pip install redis - server ! / usr / local / lib / python */ dist - packages / redis_server / bin / redis - server -- daemonize yes else : ! redis - server -- daemonize yes try : import redis except ImportError : ! pip install redis import redis r = redis . Redis () The set method adds a key-value pair to the database. In the following example, the key and value are both strings. r . set ( 'key' , 'value' ) The get method looks up a key and returns the corresponding value. r . get ( 'key' ) The result is not actually a string; it is a bytearray. For many purposes, a bytearray behaves like a string so for now we will treat it like a string and deal with differences as they arise. The values can be integers or floating-point numbers. r . set ( 'x' , 5 ) And Redis provides some functions that understand numbers, like incr. r . incr ( 'x' ) But if you get a numeric value, the result is a bytearray. value = r . get ( 'x' ) value If you want to do math with it, you have to convert it back to a number. int ( value ) If you want to set more than one value at a time, you can pass a dictionary to mset. d = dict ( x = 5 , y = 'string' , z = 1.23 ) r . mset ( d ) r . get ( 'y' ) r . get ( 'z' ) If you try to store any other type in a Redis database, you get an error. from redis import DataError t = [ 1 , 2 , 3 ] try : r . set ( 't' , t ) except DataError as e : print ( e ) We could use the repr function to create a string representation of a list, but that representation is Python-specific. It would be better to make a database that can work with any language. To do that, we can use JSON to create a string representation. The json module provides a function dumps, that creates a language-independent representation of most Python objects. import json t = [ 1 , 2 , 3 ] s = json . dumps ( t ) s When we read one of these strings back, we can use loads to convert it back to a Python object. t = json . loads ( s ) t Lists The rpush method adds new elements to the end of a list (the r indicates the right-hand side of the list). r . rpush ( 't' , 1 , 2 , 3 ) You don\u2019t have to do anything special to create a list; if it doesn\u2019t exist, Redis creates it. llen returns the length of the list. r . llen ( 't' ) lrange gets elements from a list. With the indices 0 and -1, it gets all of the elements. r . lrange ( 't' , 0 , - 1 ) The result is a Python list, but the elements are bytestrings. rpop removes elements from the end of the list. r . rpop ( 't' ) Note: Redis lists behave like linked lists, so you can add and remove elements from either end in constant time. r . lpush ( 't' , - 3 , - 2 , - 1 ) r . lpop ( 't' ) Hash A Redis hash is similar to a Python dictionary, but just to make things confusing the nomenclature is a little different. What we would call a \u201ckey\u201d in a Python dictionary is called a \u201cfield\u201d in a Redis hash. The hset method sets a field-value pair in a hash: r . hset ( 'h' , 'field' , 'value' ) The hget method looks up a field and returns the corresponding value. r . hget ( 'h' , 'field' ) hset can also take a Python dictionary as a parameter: d = dict ( a = 1 , b = 2 , c = 3 ) r . hset ( 'h' , mapping = d ) To iterate the elements of a hash, we can use hscan_iter: for field , value in r . hscan_iter ( 'h' ): print ( field , value ) The results are bytestrings for both the fields and values. Deleting Before we go on, let\u2019s clean up the database by deleting all of the key-value pairs. for key in r . keys (): r . delete ( key ) Anagrams!! We\u2019ll start by solving this problem again using Python data structures; then we\u2019ll translate it into Redis. The following cell downloads a file that contains the list of words. from os.path import basename , exists def download ( url ): filename = basename ( url ) if not exists ( filename ): from urllib.request import urlretrieve local , _ = urlretrieve ( url , filename ) print ( 'Downloaded ' + local ) download ( 'https://github.com/AllenDowney/DSIRP/raw/main/american-english' ) And here\u2019s a generator function that reads the words in the file and yields them one at a time. def iterate_words ( filename ): \"\"\"Read lines from a file and split them into words.\"\"\" for line in open ( filename ): for word in line . split (): yield word . strip () The \u201csignature\u201d of a word is a string that contains the letter of the word in sorted order. So if two words are anagrams, they have the same signature. def signature ( word ): return '' . join ( sorted ( word )) The following loop makes a dictionary of anagram lists. anagram_dict = {} for word in iterate_words ( 'american-english' ): key = signature ( word ) anagram_dict . setdefault ( key , []) . append ( word ) The following loop prints all anagram lists with 6 or more words for v in anagram_dict . values (): if len ( v ) >= 6 : print ( len ( v ), v ) Now, to do the same thing in Redis, we have two options: We can store the anagram lists using Redis lists, using the signatures as keys. We can store the whole data structure in a Redis hash. A problem with the first option is that the keys in a Redis database are like global variables. If we create a large number of keys, we are likely to run into name conflicts. We can mitigate this problem by giving each key a prefix that identifies its purpose. The following loop implements the first option, using \u201cAnagram\u201d as a prefix for the keys. for word in iterate_words ( 'american-english' ): key = f 'Anagram: { signature ( word ) } ' r . rpush ( key , word ) An advantage of this option is that it makes good use of Redis lists. A drawback is that makes many small database transactions, so it is relatively slow. We can use keys to get a list of all keys with a given prefix. keys = r . keys ( 'Anagram*' ) len ( keys ) Before we go on, we can delete the keys from the database like this. r . delete ( * keys ) The second option is to compute the dictionary of anagram lists locally and then store it as a Redis hash. The following function uses dumps to convert lists to strings that can be stored as values in a Redis hash. hash_key = 'AnagramHash' for field , t in anagram_dict . items (): value = json . dumps ( t ) r . hset ( hash_key , field , value ) We can do the same thing faster if we convert all of the lists to JSON locally and store all of the field-value pairs with one hset command. First, I\u2019ll delete the hash we just created. r . delete ( hash_key ) Shut down If you are running this notebook on your own computer, you can use the following command to shut down the Redis server. If you are running on Colab, it\u2019s not really necessary: the Redis server will get shut down when the Colab runtime shuts down (and everything stored in it will disappear). ! killall redis - server","title":"Redis\ud83d\udd25"},{"location":"db/nosql/db_redis/#lists","text":"The rpush method adds new elements to the end of a list (the r indicates the right-hand side of the list). r . rpush ( 't' , 1 , 2 , 3 ) You don\u2019t have to do anything special to create a list; if it doesn\u2019t exist, Redis creates it. llen returns the length of the list. r . llen ( 't' ) lrange gets elements from a list. With the indices 0 and -1, it gets all of the elements. r . lrange ( 't' , 0 , - 1 ) The result is a Python list, but the elements are bytestrings. rpop removes elements from the end of the list. r . rpop ( 't' ) Note: Redis lists behave like linked lists, so you can add and remove elements from either end in constant time. r . lpush ( 't' , - 3 , - 2 , - 1 ) r . lpop ( 't' )","title":"Lists"},{"location":"db/nosql/db_redis/#hash","text":"A Redis hash is similar to a Python dictionary, but just to make things confusing the nomenclature is a little different. What we would call a \u201ckey\u201d in a Python dictionary is called a \u201cfield\u201d in a Redis hash. The hset method sets a field-value pair in a hash: r . hset ( 'h' , 'field' , 'value' ) The hget method looks up a field and returns the corresponding value. r . hget ( 'h' , 'field' ) hset can also take a Python dictionary as a parameter: d = dict ( a = 1 , b = 2 , c = 3 ) r . hset ( 'h' , mapping = d ) To iterate the elements of a hash, we can use hscan_iter: for field , value in r . hscan_iter ( 'h' ): print ( field , value ) The results are bytestrings for both the fields and values.","title":"Hash"},{"location":"db/nosql/db_redis/#deleting","text":"Before we go on, let\u2019s clean up the database by deleting all of the key-value pairs. for key in r . keys (): r . delete ( key )","title":"Deleting"},{"location":"db/nosql/db_redis/#anagrams","text":"We\u2019ll start by solving this problem again using Python data structures; then we\u2019ll translate it into Redis. The following cell downloads a file that contains the list of words. from os.path import basename , exists def download ( url ): filename = basename ( url ) if not exists ( filename ): from urllib.request import urlretrieve local , _ = urlretrieve ( url , filename ) print ( 'Downloaded ' + local ) download ( 'https://github.com/AllenDowney/DSIRP/raw/main/american-english' ) And here\u2019s a generator function that reads the words in the file and yields them one at a time. def iterate_words ( filename ): \"\"\"Read lines from a file and split them into words.\"\"\" for line in open ( filename ): for word in line . split (): yield word . strip () The \u201csignature\u201d of a word is a string that contains the letter of the word in sorted order. So if two words are anagrams, they have the same signature. def signature ( word ): return '' . join ( sorted ( word )) The following loop makes a dictionary of anagram lists. anagram_dict = {} for word in iterate_words ( 'american-english' ): key = signature ( word ) anagram_dict . setdefault ( key , []) . append ( word ) The following loop prints all anagram lists with 6 or more words for v in anagram_dict . values (): if len ( v ) >= 6 : print ( len ( v ), v ) Now, to do the same thing in Redis, we have two options: We can store the anagram lists using Redis lists, using the signatures as keys. We can store the whole data structure in a Redis hash. A problem with the first option is that the keys in a Redis database are like global variables. If we create a large number of keys, we are likely to run into name conflicts. We can mitigate this problem by giving each key a prefix that identifies its purpose. The following loop implements the first option, using \u201cAnagram\u201d as a prefix for the keys. for word in iterate_words ( 'american-english' ): key = f 'Anagram: { signature ( word ) } ' r . rpush ( key , word ) An advantage of this option is that it makes good use of Redis lists. A drawback is that makes many small database transactions, so it is relatively slow. We can use keys to get a list of all keys with a given prefix. keys = r . keys ( 'Anagram*' ) len ( keys ) Before we go on, we can delete the keys from the database like this. r . delete ( * keys ) The second option is to compute the dictionary of anagram lists locally and then store it as a Redis hash. The following function uses dumps to convert lists to strings that can be stored as values in a Redis hash. hash_key = 'AnagramHash' for field , t in anagram_dict . items (): value = json . dumps ( t ) r . hset ( hash_key , field , value ) We can do the same thing faster if we convert all of the lists to JSON locally and store all of the field-value pairs with one hset command. First, I\u2019ll delete the hash we just created. r . delete ( hash_key )","title":"Anagrams!!"},{"location":"db/nosql/db_redis/#shut-down","text":"If you are running this notebook on your own computer, you can use the following command to shut down the Redis server. If you are running on Colab, it\u2019s not really necessary: the Redis server will get shut down when the Colab runtime shuts down (and everything stored in it will disappear). ! killall redis - server","title":"Shut down"},{"location":"db/sql/db_mysql/","text":"","title":"MySQL\ud83d\udc2c"},{"location":"db/sql/db_odbc/","text":"Connecting SQL to Programming Languages What is ODBC? (Open Database Connectivity) \ud83d\udee0\ufe0f ODBC (Open Database Connectivity) is a standard interface for connecting applications to databases, regardless of the database system. \ud83d\udca1 Use Case: ODBC allows applications written in Python, PHP, JavaScript, C#, and more to connect to MySQL, PostgreSQL, SQL Server, and other databases. Install ODBC Driver for MySQL (Example for Linux): sudo apt install unixODBC unixODBC-dev sudo apt install odbc-mariadb Configure ODBC in odbc.ini (Linux/Mac) or ODBC Data Source Administrator (Windows). SQL Libraries for Different Programming Languages JavaScript (Node.js) SQL Libraries \ud83d\udfe2 Library Database Support Features mysql2 MySQL Lightweight MySQL client pg (node-postgres) PostgreSQL Native PostgreSQL driver sqlite3 SQLite SQLite support in Node.js knex.js MySQL, PostgreSQL, SQLite SQL query builder for JavaScript Sequelize MySQL, PostgreSQL, SQLite ORM for Node.js \ud83d\udca1 Example: Connecting Node.js to MySQL with mysql2 const mysql = require ( 'mysql2' ); const connection = mysql . createConnection ({ host : 'localhost' , user : 'root' , password : 'mypassword' , database : 'mydb' }); connection . query ( 'SELECT * FROM users' , ( err , results ) => { if ( err ) throw err ; console . log ( results ); }); connection . end (); PHP SQL Libraries \ud83d\udc18 Library Database Support Features MySQLi (Improved MySQL Extension) MySQL Native MySQL support in PHP PDO (PHP Data Objects) MySQL, PostgreSQL, SQLite Supports multiple databases pg_connect PostgreSQL Native PostgreSQL driver \ud83d\udca1 Example: Connecting PHP to MySQL using PDO <?php $dsn = 'mysql:host=localhost;dbname=mydb;charset=utf8mb4' ; $username = 'root' ; $password = 'mypassword' ; try { $pdo = new PDO ( $dsn , $username , $password ); $stmt = $pdo -> query ( \"SELECT * FROM users\" ); while ( $row = $stmt -> fetch ( PDO :: FETCH_ASSOC )) { print_r ( $row ); } } catch ( PDOException $e ) { echo \"Connection failed: \" . $e -> getMessage (); } ?> Python SQL Libraries \ud83d\udc0d Library Database Support Features sqlite3 SQLite Built into Python, easy for local databases mysql-connector-python MySQL Official MySQL connector for Python psycopg2 PostgreSQL Most popular PostgreSQL driver SQLAlchemy MySQL, PostgreSQL, SQLite ORM (Object-Relational Mapping) for Python pyodbc All (via ODBC) Universal ODBC connector \ud83d\udca1 Example: Connecting Python to MySQL with mysql-connector-python import mysql.connector conn = mysql . connector . connect ( host = \"localhost\" , user = \"root\" , password = \"mypassword\" , database = \"mydb\" ) cursor = conn . cursor () cursor . execute ( \"SELECT * FROM users;\" ) for row in cursor . fetchall (): print ( row ) conn . close () pyodbc pip install pyodbc pyodbc is a python-ODBC bridge library that also supports the DB-API 2.0 specification. and can connect to a vast number of databases, including MS SQL Server, MySQL, PostgreSQL, Oracle, Google Big Data, SQLite, among others. supporting the DB-API 2.0 means that code that uses pyodbc can look almost identical to code using SQLite Open Database Connectivity (ODBC) is the standard that allows using identical (or at least very similar) SQL statements for querying different Databases (DBMS). The designers of ODBC aimed to make it independent of database systems and operating systems. ODBC accomplishes DBMS independence by using an ODBC driver as a translation layer between the application and the DBMS. The driver often has to be installed on the client operating system Example connections to connect to a DB, your often need to know the server/IP it is running on, the name of the datanase, and username/password to access the databse import pyodbc server = \"your_server\" db = \"your_db\" user = \"your_user\" password = \"your_password\" MS SQL Server connection_str = \\ 'DRIVER={ODBC Driver 17 for SQL Server};' + \\ f 'SERVER= { server } ;' \\ f 'DATABASE= { db } ;' \\ f 'UID= { user } ;' \\ f 'PWD= { password } ' print ( connection_str ) # # Connect to MS SQL Server # conn = pyodbc.connect(connection_str) MySQL connection_str = \\ \"DRIVER={MySQL ODBC 3.51 Driver};\" \\ f \"SERVER= { server } ;\" \\ f \"DATABASE= { db } ;\" \\ f \"UID= { user } ;\" \\ f \"PASSWORD= { password } ;\" print ( connection_str ) # # Connect to MySQL # conn = pyodbc.connect(connection_str) SQLite We don't to connect to SQLite via ODBC, because python can use these databases directly. however, if we want to show this as a demo, we need to install the SQLite ODBC driver For Windows, you can get the SQLite ODBC driver here . Download \"sqliteodbc.exe\" if you are using 32-bit Python, or \"sqliteodbc_w64.exe\" if you are using 64-bit Python. db = \"example.db\" connection_str = \\ \"Driver=SQLite3 ODBC Driver;\" \\ f \"Database= { db } \" print ( connection_str ) conn = pyodbc . connect ( connection_str ) c = conn . cursor () for row in c . execute ( 'SELECT * FROM stocks ORDER BY price' ): print ( row )","title":"ODBC"},{"location":"db/sql/db_odbc/#connecting-sql-to-programming-languages","text":"","title":"Connecting SQL to Programming Languages"},{"location":"db/sql/db_odbc/#what-is-odbc-open-database-connectivity","text":"ODBC (Open Database Connectivity) is a standard interface for connecting applications to databases, regardless of the database system. \ud83d\udca1 Use Case: ODBC allows applications written in Python, PHP, JavaScript, C#, and more to connect to MySQL, PostgreSQL, SQL Server, and other databases. Install ODBC Driver for MySQL (Example for Linux): sudo apt install unixODBC unixODBC-dev sudo apt install odbc-mariadb Configure ODBC in odbc.ini (Linux/Mac) or ODBC Data Source Administrator (Windows).","title":"What is ODBC? (Open Database Connectivity) \ud83d\udee0\ufe0f"},{"location":"db/sql/db_odbc/#sql-libraries-for-different-programming-languages","text":"","title":"SQL Libraries for Different Programming Languages"},{"location":"db/sql/db_odbc/#javascript-nodejs-sql-libraries","text":"Library Database Support Features mysql2 MySQL Lightweight MySQL client pg (node-postgres) PostgreSQL Native PostgreSQL driver sqlite3 SQLite SQLite support in Node.js knex.js MySQL, PostgreSQL, SQLite SQL query builder for JavaScript Sequelize MySQL, PostgreSQL, SQLite ORM for Node.js \ud83d\udca1 Example: Connecting Node.js to MySQL with mysql2 const mysql = require ( 'mysql2' ); const connection = mysql . createConnection ({ host : 'localhost' , user : 'root' , password : 'mypassword' , database : 'mydb' }); connection . query ( 'SELECT * FROM users' , ( err , results ) => { if ( err ) throw err ; console . log ( results ); }); connection . end ();","title":"JavaScript (Node.js) SQL Libraries \ud83d\udfe2"},{"location":"db/sql/db_odbc/#php-sql-libraries","text":"Library Database Support Features MySQLi (Improved MySQL Extension) MySQL Native MySQL support in PHP PDO (PHP Data Objects) MySQL, PostgreSQL, SQLite Supports multiple databases pg_connect PostgreSQL Native PostgreSQL driver \ud83d\udca1 Example: Connecting PHP to MySQL using PDO <?php $dsn = 'mysql:host=localhost;dbname=mydb;charset=utf8mb4' ; $username = 'root' ; $password = 'mypassword' ; try { $pdo = new PDO ( $dsn , $username , $password ); $stmt = $pdo -> query ( \"SELECT * FROM users\" ); while ( $row = $stmt -> fetch ( PDO :: FETCH_ASSOC )) { print_r ( $row ); } } catch ( PDOException $e ) { echo \"Connection failed: \" . $e -> getMessage (); } ?>","title":"PHP SQL Libraries \ud83d\udc18"},{"location":"db/sql/db_odbc/#python-sql-libraries","text":"Library Database Support Features sqlite3 SQLite Built into Python, easy for local databases mysql-connector-python MySQL Official MySQL connector for Python psycopg2 PostgreSQL Most popular PostgreSQL driver SQLAlchemy MySQL, PostgreSQL, SQLite ORM (Object-Relational Mapping) for Python pyodbc All (via ODBC) Universal ODBC connector \ud83d\udca1 Example: Connecting Python to MySQL with mysql-connector-python import mysql.connector conn = mysql . connector . connect ( host = \"localhost\" , user = \"root\" , password = \"mypassword\" , database = \"mydb\" ) cursor = conn . cursor () cursor . execute ( \"SELECT * FROM users;\" ) for row in cursor . fetchall (): print ( row ) conn . close ()","title":"Python SQL Libraries \ud83d\udc0d"},{"location":"db/sql/db_odbc/#pyodbc","text":"pip install pyodbc pyodbc is a python-ODBC bridge library that also supports the DB-API 2.0 specification. and can connect to a vast number of databases, including MS SQL Server, MySQL, PostgreSQL, Oracle, Google Big Data, SQLite, among others. supporting the DB-API 2.0 means that code that uses pyodbc can look almost identical to code using SQLite Open Database Connectivity (ODBC) is the standard that allows using identical (or at least very similar) SQL statements for querying different Databases (DBMS). The designers of ODBC aimed to make it independent of database systems and operating systems. ODBC accomplishes DBMS independence by using an ODBC driver as a translation layer between the application and the DBMS. The driver often has to be installed on the client operating system","title":"pyodbc"},{"location":"db/sql/db_postgresql/","text":"TODO","title":"PostgreSQL\ud83d\udc18"},{"location":"db/sql/db_sql/","text":"Topic Key Concepts Introduction to SQL What is SQL , SQL vs NoSQL , Installation (SQLite, MySQL, PostgreSQL) Basic SQL Commands SELECT , INSERT , UPDATE , DELETE Working with Tables CREATE TABLE , ALTER TABLE , DROP TABLE , Data Types ( INT , VARCHAR , etc.) Filtering Data WHERE , ORDER BY , LIMIT , DISTINCT SQL Joins & Relationships INNER JOIN , LEFT JOIN , RIGHT JOIN , FULL OUTER JOIN , SELF JOIN , CROSS JOIN Grouping & Aggregation GROUP BY , HAVING , Aggregate Functions ( COUNT() , SUM() , AVG() , MIN() , MAX() ) Subqueries & CTEs Subqueries ( SELECT inside SELECT ), CTEs ( WITH clause), Recursive CTEs Indexing & Performance Optimization CREATE INDEX , Execution Plans ( EXPLAIN ), Query Optimization Transactions & ACID Principles BEGIN , COMMIT , ROLLBACK , ACID Properties (Atomicity, Consistency, Isolation, Durability) Advanced SQL Features CASE Statements, Window Functions ( RANK() , DENSE_RANK() , ROW_NUMBER() ), Pivoting & JSON Handling Stored Procedures & Triggers Creating & Executing Stored Procedures, Triggers, Cursors & Loops NoSQL Features in SQL Working with JSON & XML, Full-Text Search ( tsvector , MATCH AGAINST ) Security & Best Practices SQL Injection Prevention, Role-Based Access ( GRANT , REVOKE ), Data Encryption ( MD5() , SHA256() ) Introduction to SQL What is SQL? SQL ( Structured Query Language ) is used to manage relational databases , allowing you to: \u2714 Store, retrieve, and manipulate data \u2714 Define database structures (tables, indexes, etc.) \u2714 Control access and security SQL is used in Relational Database Management Systems (RDBMS) such as: - MySQL \ud83d\udc2c - PostgreSQL \ud83d\udc18 - SQLite \ud83d\udee2\ufe0f - Microsoft SQL Server \ud83c\udfe2 - Oracle Database \ud83d\udd25 Comparing SQL vs. NoSQL Feature SQL (Relational DBs) NoSQL (Non-Relational DBs) Structure Tables (rows & columns) Documents, Key-Value, Graphs Schema Fixed, predefined Flexible, dynamic Scalability Vertical (scale-up) Horizontal (scale-out) Use Case Structured data (banking, e-commerce) Unstructured data (social media, real-time apps) Popular NoSQL databases: MongoDB \ud83c\udf43, Redis \ud83d\udd25, Cassandra \ud83d\udcbe","title":"General"},{"location":"db/sql/db_sql/#introduction-to-sql","text":"","title":"Introduction to SQL"},{"location":"db/sql/db_sql/#what-is-sql","text":"SQL ( Structured Query Language ) is used to manage relational databases , allowing you to: \u2714 Store, retrieve, and manipulate data \u2714 Define database structures (tables, indexes, etc.) \u2714 Control access and security SQL is used in Relational Database Management Systems (RDBMS) such as: - MySQL \ud83d\udc2c - PostgreSQL \ud83d\udc18 - SQLite \ud83d\udee2\ufe0f - Microsoft SQL Server \ud83c\udfe2 - Oracle Database \ud83d\udd25","title":"What is SQL?"},{"location":"db/sql/db_sql/#comparing-sql-vs-nosql","text":"Feature SQL (Relational DBs) NoSQL (Non-Relational DBs) Structure Tables (rows & columns) Documents, Key-Value, Graphs Schema Fixed, predefined Flexible, dynamic Scalability Vertical (scale-up) Horizontal (scale-out) Use Case Structured data (banking, e-commerce) Unstructured data (social media, real-time apps) Popular NoSQL databases: MongoDB \ud83c\udf43, Redis \ud83d\udd25, Cassandra \ud83d\udcbe","title":"Comparing SQL vs. NoSQL"},{"location":"db/sql/db_sqlbasic/","text":"Basic SQL Commands SELECT \u2013 Retrieving data INSERT \u2013 Adding records UPDATE \u2013 Modifying data DELETE \u2013 Removing records Working with Tables CREATE TABLE \u2013 Defining structure ALTER TABLE \u2013 Modifying schema DROP TABLE \u2013 Deleting a table Data Types ( INT , VARCHAR , DATE , etc.) Filtering Data WHERE \u2013 Conditional filtering ORDER BY \u2013 Sorting results LIMIT \u2013 Restricting output DISTINCT \u2013 Removing duplicates SQL Joins & Relationships INNER JOIN \u2013 Matching records LEFT JOIN / RIGHT JOIN \u2013 Including unmatched records FULL OUTER JOIN \u2013 Combining everything SELF JOIN \u2013 Joining a table to itself CROSS JOIN \u2013 Cartesian product Grouping & Aggregation GROUP BY \u2013 Grouping data HAVING \u2013 Filtering grouped results Aggregate Functions ( COUNT() , SUM() , AVG() , MIN() , MAX() ) Subqueries & CTEs (Common Table Expressions) Subqueries ( SELECT inside SELECT ) CTEs ( WITH clause for readability) Recursive CTEs Indexing & Performance Optimization What is an Index? Creating & Using Indexes ( CREATE INDEX ) Understanding Execution Plans ( EXPLAIN ) Transactions & ACID Principles BEGIN , COMMIT , ROLLBACK \u2013 Managing transactions ACID Properties (Atomicity, Consistency, Isolation, Durability) Advanced SQL Features CASE Statements \u2013 Conditional logic Window Functions ( RANK() , DENSE_RANK() , ROW_NUMBER() ) Pivoting & Unpivoting Data JSON Handling ( JSONB , -> , ->> , #> in PostgreSQL) Stored Procedures & Triggers Creating & Executing Stored Procedures Using Triggers for Automation Cursors & Loops NoSQL Features in SQL Databases Working with JSON & XML Full-Text Search ( tsvector , MATCH AGAINST ) Security & Best Practices SQL Injection Prevention Role-Based Access Control ( GRANT , REVOKE ) Data Encryption & Hashing ( MD5() , SHA256() )","title":"Basic SQL Commands"},{"location":"db/sql/db_sqlbasic/#basic-sql-commands","text":"SELECT \u2013 Retrieving data INSERT \u2013 Adding records UPDATE \u2013 Modifying data DELETE \u2013 Removing records","title":"Basic SQL Commands"},{"location":"db/sql/db_sqlbasic/#working-with-tables","text":"CREATE TABLE \u2013 Defining structure ALTER TABLE \u2013 Modifying schema DROP TABLE \u2013 Deleting a table Data Types ( INT , VARCHAR , DATE , etc.)","title":"Working with Tables"},{"location":"db/sql/db_sqlbasic/#filtering-data","text":"WHERE \u2013 Conditional filtering ORDER BY \u2013 Sorting results LIMIT \u2013 Restricting output DISTINCT \u2013 Removing duplicates","title":"Filtering Data"},{"location":"db/sql/db_sqlbasic/#sql-joins-relationships","text":"INNER JOIN \u2013 Matching records LEFT JOIN / RIGHT JOIN \u2013 Including unmatched records FULL OUTER JOIN \u2013 Combining everything SELF JOIN \u2013 Joining a table to itself CROSS JOIN \u2013 Cartesian product","title":"SQL Joins &amp; Relationships"},{"location":"db/sql/db_sqlbasic/#grouping-aggregation","text":"GROUP BY \u2013 Grouping data HAVING \u2013 Filtering grouped results Aggregate Functions ( COUNT() , SUM() , AVG() , MIN() , MAX() )","title":"Grouping &amp; Aggregation"},{"location":"db/sql/db_sqlbasic/#subqueries-ctes-common-table-expressions","text":"Subqueries ( SELECT inside SELECT ) CTEs ( WITH clause for readability) Recursive CTEs","title":"Subqueries &amp; CTEs (Common Table Expressions)"},{"location":"db/sql/db_sqlbasic/#indexing-performance-optimization","text":"What is an Index? Creating & Using Indexes ( CREATE INDEX ) Understanding Execution Plans ( EXPLAIN )","title":"Indexing &amp; Performance Optimization"},{"location":"db/sql/db_sqlbasic/#transactions-acid-principles","text":"BEGIN , COMMIT , ROLLBACK \u2013 Managing transactions ACID Properties (Atomicity, Consistency, Isolation, Durability)","title":"Transactions &amp; ACID Principles"},{"location":"db/sql/db_sqlbasic/#advanced-sql-features","text":"CASE Statements \u2013 Conditional logic Window Functions ( RANK() , DENSE_RANK() , ROW_NUMBER() ) Pivoting & Unpivoting Data JSON Handling ( JSONB , -> , ->> , #> in PostgreSQL)","title":"Advanced SQL Features"},{"location":"db/sql/db_sqlbasic/#stored-procedures-triggers","text":"Creating & Executing Stored Procedures Using Triggers for Automation Cursors & Loops","title":"Stored Procedures &amp; Triggers"},{"location":"db/sql/db_sqlbasic/#nosql-features-in-sql-databases","text":"Working with JSON & XML Full-Text Search ( tsvector , MATCH AGAINST )","title":"NoSQL Features in SQL Databases"},{"location":"db/sql/db_sqlbasic/#security-best-practices","text":"SQL Injection Prevention Role-Based Access Control ( GRANT , REVOKE ) Data Encryption & Hashing ( MD5() , SHA256() )","title":"Security &amp; Best Practices"},{"location":"db/sql/db_sqlinstall/","text":"Installing & Setting Up SQL Installing SQLite (Lightweight, No Server Needed) \ud83d\udca1 SQLite is great for local development and small applications. - Install SQLite: sudo apt install sqlite3 # Linux brew install sqlite3 # macOS - Open SQLite CLI: sqlite3 my_database.db - Verify installation: SELECT sqlite_version (); Installing MySQL (Popular for Web Apps) Install MySQL Server: sudo apt install mysql-server # Linux brew install mysql # macOS Start MySQL and login: mysql -u root -p Installing PostgreSQL (Advanced & Scalable) Install PostgreSQL: sudo apt install postgresql # Linux brew install postgresql # macOS Start PostgreSQL and login: psql -U postgres","title":"General"},{"location":"db/sql/db_sqlinstall/#installing-setting-up-sql","text":"","title":"Installing &amp; Setting Up SQL"},{"location":"db/sql/db_sqlinstall/#installing-sqlite-lightweight-no-server-needed","text":"\ud83d\udca1 SQLite is great for local development and small applications. - Install SQLite: sudo apt install sqlite3 # Linux brew install sqlite3 # macOS - Open SQLite CLI: sqlite3 my_database.db - Verify installation: SELECT sqlite_version ();","title":"Installing SQLite (Lightweight, No Server Needed)"},{"location":"db/sql/db_sqlinstall/#installing-mysql-popular-for-web-apps","text":"Install MySQL Server: sudo apt install mysql-server # Linux brew install mysql # macOS Start MySQL and login: mysql -u root -p","title":"Installing MySQL (Popular for Web Apps)"},{"location":"db/sql/db_sqlinstall/#installing-postgresql-advanced-scalable","text":"Install PostgreSQL: sudo apt install postgresql # Linux brew install postgresql # macOS Start PostgreSQL and login: psql -U postgres","title":"Installing PostgreSQL (Advanced &amp; Scalable)"},{"location":"db/sql/db_sqlite/","text":"SQLite is an open-source library that provides a lightweight disk-based database. sqlite it doesn\u2019t require a separate server process allows accessing the database using a nonstandard variant of the SQL query language. provides a SQL interface compliant with the python's DB-API 2.0 specification described by PEP 249 . applications can use SQLite for internal data storage. It\u2019s also possible to prototype an application using SQLite and then port the code to a larger database such as PostgreSQL or Oracle. To use the module, you must first create a Connection object that represents the database. A connection object's key methods are: cursor() returns a cursor object that can execute queries and retrieve results commit() submits the current transaction to the DB. If you don\u2019t call this method, anything you did since the last call to commit() is not visible from other database connections. If you wonder why you don\u2019t see the data you\u2019ve written to the database, please check you didn\u2019t forget to call this method. rollback() rolls back any changes to the database since the last call to commit(). close() closes the database connection. Note that this does not automatically call commit(). If you just close your database connection without calling commit() first, your changes will be lost! in our example the data will be stored in a local file called example.db : import sqlite3 conn = sqlite3 . connect ( 'example.db' ) You can also supply the special name :memory: to create a database in RAM. Once you have a Connection, you can create a Cursor object and call its execute() method to perform SQL commands: c = conn . cursor () # Create table called stocks c . execute ( ''' CREATE TABLE stocks ( date text, trans text, symbol text, qty real, price real ) ''' ) # Save (commit) the changes conn . commit () # We can also close the connection if we are done with it. # Just be sure any changes have been committed or they will be lost. conn . close () since we don't want to forget closing the connection, here's a nice utility function that opens a connection and returns a handy cursor object from contextlib import contextmanager @contextmanager def sqlite3_connect ( database , * args , ** kwargs ): conn = sqlite3 . connect ( database , * args , ** kwargs ) try : cursor = conn . cursor () yield ( conn , cursor ) finally : conn . close () and here's how sqlite3_connect() can be used: with sqlite3_connect ( 'example.db' ) as [ conn , c ]: # Insert a row of data c . execute ( \"INSERT INTO stocks VALUES ('2008-01-05','BUY','AAPL',120,37.14)\" ) # Save (commit) the changes conn . commit () # closing is done for us Usually your SQL operations will need to use values from Python variables like people's names, and fields from forms. DO NOT assemble your SQL using Python\u2019s string operations because doing so is insecure; it makes your program vulnerable to an SQL injection attack Instead, use the API\u2019s parameter substitution - Put ? as a placeholder wherever you want to use a value, and then provide a tuple of values as the second argument to the cursor\u2019s execute() method. with sqlite3_connect ( 'example.db' ) as [ conn , c ]: # DO NOT use str.format or f-strings or any other way to embed your variables into strings # INSTEAD, use the sanitized substitution capability of the execute() function t = ( 'RHAT' ,) c . execute ( 'SELECT * FROM stocks WHERE symbol=?' , t ) print ( c . fetchone ()) # Larger example that inserts many records at a time purchases = [( '2006-03-28' , 'BUY' , 'IBM' , 1000 , 45.00 ), ( '2006-04-05' , 'BUY' , 'MSFT' , 1000 , 72.00 ), ( '2006-04-06' , 'SELL' , 'IBM' , 500 , 53.00 ), ] c . executemany ( 'INSERT INTO stocks VALUES (?,?,?,?,?)' , purchases ) conn . commit () To retrieve data after executing a SELECT statement, you can treat the cursor as an iterator: with sqlite3_connect ( 'example.db' ) as [ conn , c ]: for row in c . execute ( 'SELECT * FROM stocks ORDER BY price' ): print ( row ) you can also call the cursor\u2019s fetchone() method to retrieve a single matching row, or call fetchall() to get a list of the matching rows. Table metadata We can access all the tables, their fields and their types using a simple query with sqlite3_connect ( 'example.db' ) as [ conn , c ]: rs = c . execute ( \"\"\" SELECT name, sql FROM sqlite_master WHERE type='table' ORDER BY name; \"\"\" ) for name , sql , * args in rs : print ( name ) print ( sql ) SQLite and Python types SQLite natively supports the following types: NULL , INTEGER , REAL , TEXT , BLOB . The following Python types can thus be sent to SQLite without any problem: Python type SQLite type None NULL int INTEGER float REAL str TEXT bytes BLOB SQLite supports only a limited set of types natively. To use other Python types with SQLite, you must adapt them to one of the sqlite3 module\u2019s supported types for SQLite: one of NoneType , int , float , str , bytes . Custom types there are two ways to read/write objects from a database: 1. encoding the object into a text/blob column using some format (often JSON) 2. converting objects into tables rows (respecting foreign keys as potential links to other objects) with a DB-API based SQL interface like sqlite3, both of these methods can contain a lot of boiler plate code and so we will leave them out of thid tutorial","title":"SQLite \ud83d\udee2\ufe0f"},{"location":"db/sql/db_sqlite/#table-metadata","text":"We can access all the tables, their fields and their types using a simple query with sqlite3_connect ( 'example.db' ) as [ conn , c ]: rs = c . execute ( \"\"\" SELECT name, sql FROM sqlite_master WHERE type='table' ORDER BY name; \"\"\" ) for name , sql , * args in rs : print ( name ) print ( sql )","title":"Table metadata"},{"location":"db/sql/db_sqlite/#sqlite-and-python-types","text":"SQLite natively supports the following types: NULL , INTEGER , REAL , TEXT , BLOB . The following Python types can thus be sent to SQLite without any problem: Python type SQLite type None NULL int INTEGER float REAL str TEXT bytes BLOB SQLite supports only a limited set of types natively. To use other Python types with SQLite, you must adapt them to one of the sqlite3 module\u2019s supported types for SQLite: one of NoneType , int , float , str , bytes .","title":"SQLite and Python types"},{"location":"db/sql/db_sqlite/#custom-types","text":"there are two ways to read/write objects from a database: 1. encoding the object into a text/blob column using some format (often JSON) 2. converting objects into tables rows (respecting foreign keys as potential links to other objects) with a DB-API based SQL interface like sqlite3, both of these methods can contain a lot of boiler plate code and so we will leave them out of thid tutorial","title":"Custom types"}]}