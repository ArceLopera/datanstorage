{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Data n Storage Refresher Everyone can forget about memorizing every query and command. What matters most is knowing where to find the right information. Mastering databases and storage systems isn\u2019t about remembering every syntax detail\u2014it\u2019s about understanding the core principles, exploring best practices, and knowing how to troubleshoot efficiently. With a solid foundation, a habit of continuous learning, and hands-on experimentation, you can refine your skills and confidently manage data in any environment. Basic Topics Advanced Topics Basics Subqueries SQL Index JOIN ACID Various More This material is a work in progress, so your feedback is welcome. The best way to provide that feedback is to click here and create an issue in this GitHub repository .","title":"Home"},{"location":"#welcome-to-data-n-storage-refresher","text":"Everyone can forget about memorizing every query and command. What matters most is knowing where to find the right information. Mastering databases and storage systems isn\u2019t about remembering every syntax detail\u2014it\u2019s about understanding the core principles, exploring best practices, and knowing how to troubleshoot efficiently. With a solid foundation, a habit of continuous learning, and hands-on experimentation, you can refine your skills and confidently manage data in any environment. Basic Topics Advanced Topics Basics Subqueries SQL Index JOIN ACID Various More This material is a work in progress, so your feedback is welcome. The best way to provide that feedback is to click here and create an issue in this GitHub repository .","title":"Welcome to Data n Storage Refresher"},{"location":"ai_frameworks/","text":"The most commonly used AI frameworks fall into several categories, including deep learning, machine learning, and natural language processing (NLP). Deep Learning Frameworks TensorFlow (Google) \u2013 A highly flexible and scalable framework widely used for deep learning, neural networks, and AI applications. PyTorch (Meta) \u2013 A popular deep learning framework known for its dynamic computation graph and ease of use, especially for research. JAX (Google) \u2013 A newer framework optimized for high-performance computing and automatic differentiation, often used in reinforcement learning and deep learning. MXNet (Apache) \u2013 Used by Amazon for deep learning tasks, but less popular than TensorFlow and PyTorch. ONNX (Open Neural Network Exchange) \u2013 A format for model interoperability across different AI frameworks. Machine Learning Frameworks Scikit-learn \u2013 A widely used framework for classical machine learning algorithms, such as regression, clustering, and classification. XGBoost \u2013 A powerful gradient boosting library for structured/tabular data. LightGBM \u2013 A fast and efficient gradient boosting framework optimized for performance. CatBoost \u2013 Another gradient boosting library, designed to work well with categorical features. NLP & Large Language Model (LLM) Frameworks Hugging Face Transformers \u2013 A leading NLP framework for pre-trained transformer models like BERT, GPT, and LLaMA. spaCy \u2013 A fast and efficient NLP library for tokenization, named entity recognition, and more. NLTK \u2013 A traditional NLP toolkit useful for research and academic work. Reinforcement Learning (RL) Frameworks Stable-Baselines3 \u2013 A PyTorch-based library for reinforcement learning. RLlib (Ray) \u2013 A scalable RL framework designed for production use. AutoML & AI Deployment AutoML frameworks \u2013 Include Auto-sklearn, TPOT, and H2O.ai for automated machine learning. MLflow \u2013 A tool for managing ML experiments and deployment. TensorFlow Serving & TorchServe \u2013 Used for deploying deep learning models in production. Computer Vision (CV) Frameworks OpenCV \u2013 The most widely used open-source computer vision library, supports image/video processing, object detection, and more. Detectron2 (Meta) \u2013 A powerful PyTorch-based library for object detection and segmentation. MMDetection (OpenMMLab) \u2013 A modular framework for object detection and instance segmentation. YOLO (You Only Look Once) \u2013 A family of real-time object detection models (YOLOv8 is the latest). Mediapipe (Google) \u2013 Optimized for real-time hand tracking, face detection, and pose estimation. Albumentations \u2013 A fast and efficient image augmentation library, often used in deep learning pipelines. Kornia \u2013 A PyTorch-based differentiable vision library for image transformations. SuperGradients (Deci) \u2013 A CV training library with pre-trained models for segmentation and detection. Ultralytics \u2013 Provides an easy-to-use API for training and deploying YOLO models. AI Cloud Services \u2601\ufe0f\ud83d\udca1 \ud83d\udd39 Major AI Cloud Platforms 1\ufe0f\u20e3 Google Cloud AI (Vertex AI & TensorFlow Cloud) Vertex AI \u2013 Unified ML platform for training, deploying, and managing models. AutoML \u2013 No-code/low-code ML model training for vision, NLP, and structured data. TensorFlow Cloud \u2013 Seamlessly runs TensorFlow models on Google Cloud infrastructure. TPUs (Tensor Processing Units) \u2013 Specialized hardware for deep learning acceleration. \u2705 Best for: Scalable deep learning, AutoML, and enterprise AI. 2\ufe0f\u20e3 AWS AI & ML Services SageMaker \u2013 Fully managed service for building, training, and deploying ML models. Rekognition \u2013 Image and video analysis (face detection, object recognition, etc.). Lex \u2013 Conversational AI for chatbots (based on Alexa). Comprehend \u2013 NLP for sentiment analysis, entity recognition, etc. Transcribe & Polly \u2013 Speech-to-text and text-to-speech services. Inferentia \u2013 Custom chips for AI inference acceleration. \u2705 Best for: Scalable ML training, NLP, and real-time vision applications. 3\ufe0f\u20e3 Microsoft Azure AI Azure Machine Learning \u2013 End-to-end ML lifecycle management. Cognitive Services \u2013 Pre-trained AI models for vision, speech, NLP, and decision-making. Azure OpenAI Service \u2013 Provides access to OpenAI\u2019s GPT models. Computer Vision API \u2013 Object detection, OCR, and facial recognition. Speech Services \u2013 Text-to-speech, speech recognition, and translation. \u2705 Best for: Enterprises using Microsoft ecosystems (Windows, Office, etc.). \ud83d\udd39 Specialized Cloud AI Services 4\ufe0f\u20e3 OpenAI API (ChatGPT, DALL\u00b7E, Whisper) GPT-4 & GPT-3.5 \u2013 Text generation and conversational AI. DALL\u00b7E \u2013 AI-powered image generation. Whisper \u2013 AI-powered speech-to-text. \u2705 Best for: NLP, chatbots, and AI-generated content. 5\ufe0f\u20e3 Hugging Face Hub Inference API \u2013 Deploys pre-trained AI models instantly. Spaces \u2013 Hosts AI apps using Gradio and Streamlit. AutoTrain \u2013 Automates model fine-tuning for NLP, vision, and tabular data. \u2705 Best for: Rapid AI prototyping and model sharing. 6\ufe0f\u20e3 NVIDIA AI Cloud (NIM & DGX Cloud) DGX Cloud \u2013 On-demand GPU clusters for AI model training. NVIDIA AI Enterprise \u2013 Pre-trained models for speech, CV, and recommendation systems. RAPIDS \u2013 GPU-accelerated ML for big data processing. \u2705 Best for: High-performance AI training (e.g., deep learning, reinforcement learning). 7\ufe0f\u20e3 IBM Watson AI Watson Studio \u2013 AI/ML model training with AutoML and cloud deployment. Watson Assistant \u2013 AI-powered chatbots. Watson NLP \u2013 Pre-trained NLP models. \u2705 Best for: Enterprise AI applications and NLP-driven automation. TensorFlow vs PyTorch https://opencv.org/blog/pytorch-vs-tensorflow/","title":"Frameworks"},{"location":"ai_frameworks/#deep-learning-frameworks","text":"TensorFlow (Google) \u2013 A highly flexible and scalable framework widely used for deep learning, neural networks, and AI applications. PyTorch (Meta) \u2013 A popular deep learning framework known for its dynamic computation graph and ease of use, especially for research. JAX (Google) \u2013 A newer framework optimized for high-performance computing and automatic differentiation, often used in reinforcement learning and deep learning. MXNet (Apache) \u2013 Used by Amazon for deep learning tasks, but less popular than TensorFlow and PyTorch. ONNX (Open Neural Network Exchange) \u2013 A format for model interoperability across different AI frameworks.","title":"Deep Learning Frameworks"},{"location":"ai_frameworks/#machine-learning-frameworks","text":"Scikit-learn \u2013 A widely used framework for classical machine learning algorithms, such as regression, clustering, and classification. XGBoost \u2013 A powerful gradient boosting library for structured/tabular data. LightGBM \u2013 A fast and efficient gradient boosting framework optimized for performance. CatBoost \u2013 Another gradient boosting library, designed to work well with categorical features.","title":"Machine Learning Frameworks"},{"location":"ai_frameworks/#nlp-large-language-model-llm-frameworks","text":"Hugging Face Transformers \u2013 A leading NLP framework for pre-trained transformer models like BERT, GPT, and LLaMA. spaCy \u2013 A fast and efficient NLP library for tokenization, named entity recognition, and more. NLTK \u2013 A traditional NLP toolkit useful for research and academic work.","title":"NLP &amp; Large Language Model (LLM) Frameworks"},{"location":"ai_frameworks/#reinforcement-learning-rl-frameworks","text":"Stable-Baselines3 \u2013 A PyTorch-based library for reinforcement learning. RLlib (Ray) \u2013 A scalable RL framework designed for production use.","title":"Reinforcement Learning (RL) Frameworks"},{"location":"ai_frameworks/#automl-ai-deployment","text":"AutoML frameworks \u2013 Include Auto-sklearn, TPOT, and H2O.ai for automated machine learning. MLflow \u2013 A tool for managing ML experiments and deployment. TensorFlow Serving & TorchServe \u2013 Used for deploying deep learning models in production.","title":"AutoML &amp; AI Deployment"},{"location":"ai_frameworks/#computer-vision-cv-frameworks","text":"OpenCV \u2013 The most widely used open-source computer vision library, supports image/video processing, object detection, and more. Detectron2 (Meta) \u2013 A powerful PyTorch-based library for object detection and segmentation. MMDetection (OpenMMLab) \u2013 A modular framework for object detection and instance segmentation. YOLO (You Only Look Once) \u2013 A family of real-time object detection models (YOLOv8 is the latest). Mediapipe (Google) \u2013 Optimized for real-time hand tracking, face detection, and pose estimation. Albumentations \u2013 A fast and efficient image augmentation library, often used in deep learning pipelines. Kornia \u2013 A PyTorch-based differentiable vision library for image transformations. SuperGradients (Deci) \u2013 A CV training library with pre-trained models for segmentation and detection. Ultralytics \u2013 Provides an easy-to-use API for training and deploying YOLO models.","title":"Computer Vision (CV) Frameworks"},{"location":"ai_frameworks/#ai-cloud-services","text":"","title":"AI Cloud Services \u2601\ufe0f\ud83d\udca1"},{"location":"ai_frameworks/#major-ai-cloud-platforms","text":"","title":"\ud83d\udd39 Major AI Cloud Platforms"},{"location":"ai_frameworks/#1-google-cloud-ai-vertex-ai-tensorflow-cloud","text":"Vertex AI \u2013 Unified ML platform for training, deploying, and managing models. AutoML \u2013 No-code/low-code ML model training for vision, NLP, and structured data. TensorFlow Cloud \u2013 Seamlessly runs TensorFlow models on Google Cloud infrastructure. TPUs (Tensor Processing Units) \u2013 Specialized hardware for deep learning acceleration. \u2705 Best for: Scalable deep learning, AutoML, and enterprise AI.","title":"1\ufe0f\u20e3 Google Cloud AI (Vertex AI &amp; TensorFlow Cloud)"},{"location":"ai_frameworks/#2-aws-ai-ml-services","text":"SageMaker \u2013 Fully managed service for building, training, and deploying ML models. Rekognition \u2013 Image and video analysis (face detection, object recognition, etc.). Lex \u2013 Conversational AI for chatbots (based on Alexa). Comprehend \u2013 NLP for sentiment analysis, entity recognition, etc. Transcribe & Polly \u2013 Speech-to-text and text-to-speech services. Inferentia \u2013 Custom chips for AI inference acceleration. \u2705 Best for: Scalable ML training, NLP, and real-time vision applications.","title":"2\ufe0f\u20e3 AWS AI &amp; ML Services"},{"location":"ai_frameworks/#3-microsoft-azure-ai","text":"Azure Machine Learning \u2013 End-to-end ML lifecycle management. Cognitive Services \u2013 Pre-trained AI models for vision, speech, NLP, and decision-making. Azure OpenAI Service \u2013 Provides access to OpenAI\u2019s GPT models. Computer Vision API \u2013 Object detection, OCR, and facial recognition. Speech Services \u2013 Text-to-speech, speech recognition, and translation. \u2705 Best for: Enterprises using Microsoft ecosystems (Windows, Office, etc.).","title":"3\ufe0f\u20e3 Microsoft Azure AI"},{"location":"ai_frameworks/#specialized-cloud-ai-services","text":"","title":"\ud83d\udd39 Specialized Cloud AI Services"},{"location":"ai_frameworks/#4-openai-api-chatgpt-dalle-whisper","text":"GPT-4 & GPT-3.5 \u2013 Text generation and conversational AI. DALL\u00b7E \u2013 AI-powered image generation. Whisper \u2013 AI-powered speech-to-text. \u2705 Best for: NLP, chatbots, and AI-generated content.","title":"4\ufe0f\u20e3 OpenAI API (ChatGPT, DALL\u00b7E, Whisper)"},{"location":"ai_frameworks/#5-hugging-face-hub","text":"Inference API \u2013 Deploys pre-trained AI models instantly. Spaces \u2013 Hosts AI apps using Gradio and Streamlit. AutoTrain \u2013 Automates model fine-tuning for NLP, vision, and tabular data. \u2705 Best for: Rapid AI prototyping and model sharing.","title":"5\ufe0f\u20e3 Hugging Face Hub"},{"location":"ai_frameworks/#6-nvidia-ai-cloud-nim-dgx-cloud","text":"DGX Cloud \u2013 On-demand GPU clusters for AI model training. NVIDIA AI Enterprise \u2013 Pre-trained models for speech, CV, and recommendation systems. RAPIDS \u2013 GPU-accelerated ML for big data processing. \u2705 Best for: High-performance AI training (e.g., deep learning, reinforcement learning).","title":"6\ufe0f\u20e3 NVIDIA AI Cloud (NIM &amp; DGX Cloud)"},{"location":"ai_frameworks/#7-ibm-watson-ai","text":"Watson Studio \u2013 AI/ML model training with AutoML and cloud deployment. Watson Assistant \u2013 AI-powered chatbots. Watson NLP \u2013 Pre-trained NLP models. \u2705 Best for: Enterprise AI applications and NLP-driven automation. TensorFlow vs PyTorch https://opencv.org/blog/pytorch-vs-tensorflow/","title":"7\ufe0f\u20e3 IBM Watson AI"},{"location":"db/db_basic/","text":"How to choose DB Integration The most important thing to consider while choosing the right database is what system you need to integrate together? Make sure that your database management system can be integrated with other tools and services within your project. Different technologies have different connectors for different other technologies. For example, if you have a big analytics job that\u2019s currently running an Apache spark then probably you want to limit yourself to external databases that can connect easily to apache spark. Scaling Requirement It\u2019s important to know the scaling requirement before installing your production database. How much data are you really talking about? Is it really going to grow unbounded over time? if so then you need some sort of database technology that is not limited to the data that you can store on one PC. You need to look at something like Cassandra or MongoDB or HBase where you can actually distribute the storage of your data across an entire cluster and scale horizontally instead of vertically. While choosing a database you also need to think about the transaction rate or throughput which means how many requests you intend to get per second. Databases with high throughput can support many simultaneous users. If we are talking about thousands then again a single database service is not going to work out. This is especially important when you are working on some big websites where we have a lot of web servers that are serving a lot of people at the same time. You will have to choose a database that is distributed and allows you to spread out a load of those transactions more evenly. In those situations, NoSQL databases are a good choice instead of RDBMS. Support Consideration Think about the supports you might need for your database. Do you have the in-house expertise to spin up this new technology and actually configure it properly? It\u2019s going to be harder than you think especially if you\u2019re using this in the real world or any sort of situation where you have personally identifiable information in the mix from your end-users. In that case, you need to make sure you\u2019re thinking about the security of your system. The truth is most of the NoSQL database we\u2019ve talked about if you configure them with their default settings there will be no security at all. CAP Consideration CAP stands for Consistency, Availability, and Partition tolerance. The theorem states that you cannot achieve all the properties at the best level in a single database, as there are natural trade offs between the items. You can only pick two out of three at a time and that totally depends on your prioritize based on your requirements. For example, if your system needs to be available and partition tolerant, then you must be willing to accept some latency in your consistency requirements. Traditional relational databases are a natural fit for the CA side whereas Non-relational database engines mostly satisfy AP and CP requirements. Consistency means that any read request will return the most recent write. Data consistency is usually \u201cstrong\u201d for SQL databases and for NoSQL database consistency may be anything from \u201ceventual\u201d to \u201cstrong\u201d. Availability means that a non-responding node must respond in a reasonable amount of time. Not every application needs to run 24/7 with 99.999% availability but most likely you will prefer a database with higher availability. Partition tolerance means the system will continue to operate despite network or node failures. Schemas or Data Model Relational databases store data in a fixed and predefined structure. It means when you start development you will have to define your data schema in terms of tables and columns. You have to change the schema every time the requirements change. This will lead to creating new columns, defining new relations, reflecting the changes in your application, discussing with your database administrators, etc. NoSQL database provides much more flexibility when it comes to handling data. There is no requirement to specify the schema to start working with the application. Also, the NoSQL database doesn\u2019t put a restriction on the types of data you can store together. It allows you to add more new types as your needs change. In the application building process, most of the developers prefer high coding velocity and great agility. NoSQL databases have proven to be a much better choice in that regard especially for agile development which requires fast implementation. Types of DB You have a variety of options available in relational (MySQL, PostgreSQL, Oracle DB, etc) and non-relational (MongoDB, Apache HBase, Cassandra, etc) database but you need to understand none of them fits on all kinds of projects requirement. Each one of them has some strengths and weaknesses. Databases through two lenses: access characteristics and the pattern of the data being stored. Relational With large spans of usage, relational databases are still the dominant database type today. A relational database is self-describing because it enables developers to define the database's schema as well as relations and constraints between rows and tables in the database. Developers rely on the functionality of the relational database and not the application code to enforce the schema and preserve the referential integrity of the data within the database. Typical use cases for a relational database include web and mobile applications, enterprise applications, and online gaming. Various flavors or versions of Amazon RDS and Amazon Aurora are used by startups for high-performance and scalable applications on AWS. Both RDS and Aurora are fully managed, scalable systems. MySQL Relational (+Document since 5.7.8) SQL with JOINS JSON type support (since 5.7.8) Open source (with proprietary, closed-sourced modules) When to use MySQL When you already widely use it in your organization When you want both relational tables (when you know the schema upfront) and JSON collections (Schemaless) Relational / normalized \u2014 when you need to optimize on writes instead of reads, to have strong read consistency MySQL Advantages Maturity & Reliability \u2014 MySQL is highly used, battle tested and mature Fast read performance Improved JSON/Document support (MySQL 8) Cross-DC write consistency (when ProxySQL is used) MySQL Disadvantages Scalability \u2014 Does not scale horizontally. Limited by amount of disk space Consistency and Replication Issues (when not using ProxySQL) Other DBs in this category PostgreSQL, MariaDB, SQL Server, Oracle, Db2, SQLite Key-value As your system grows, large amounts of data are often in the form of key-value data, where a single row maps to a primary key. Key-value databases are highly partitionable and allow horizontal scaling at levels that other types of databases cannot achieve. Use cases such as gaming, ad tech, and IoT lend themselves particularly well to the key-value data model where the access patterns require low-latency Gets/Puts for known key values. Amazon DynamoDB is a managed key-value and document database that delivers single-digit millisecond performance at any scale. Key-value DBs store data in pairs, each containing a unique ID and a data value. These DBs provide a flexible storage structure since values can store any amount of unstructured data. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Session management, user preferences, and product recommendations. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Amazon DynamoDB, Azure Cosmos DB. Amazon DynamoDB Other DBs in this category Cassandra, HBase, Redis (Key-value Store) When to use DynamoDB When you need a simple key value store without complex querying patterns When you need to store expirable data Low-medium throughput apps as writes are expensive and consistent reads are twice the cost of eventually consistent reads. DynamoDB Advantages Fast performance in any scale (as long as enough capacity is provisioned) No storage limit Schemaless \u2014 it\u2019s possible to define a schema for each item, rather than for the whole table +Multi-master replication (update data in multiple regions) Supports TTL per item Built-in CDC events (DynamoDB streams) DynamoDB Disadvantages Size limit \u2014 item can only reach 400KB in size Limited querying options (limited number of indices) Throttling on burst throughput (and hot keys in certain situations) Amazon Simple Storage Service (S3) Other DBs in this category Google Cloud Storage, Azure Blob Storage When to use S3 When you need to store large binary objects/files (up to 5TB each) When the amount of data you need to store is large (>10TB), continues to grow daily, and may need to be retrieved (can\u2019t be deleted) S3 Advantages Supports very high throughput Infinite scalability \u2014 No limit on amount of storage S3 Disadvantages No Query support, only key-based retrieval Latency is 100\u2013200 ms for small objects. Caching can alleviate this Document Document databases are intuitive for developers to use, because the data in the application tier is typically represented as a JSON document. Developers can persist data using the same document model format that they use in their application code and use the flexible schema model of Amazon DocumentDB to achieve developer efficiency. \ud835\uddd7\ud835\uddfc\ud835\uddf0\ud835\ude02\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00 Document databases are structured similarly to key-value databases except that keys and values are stored in documents written in a markup language like JSON, XML, or YAML. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: User profiles, product catalogs, and content management. Examples: MongoDB, Amazon DocumentDB MongoDB When to use MongoDB When data schema is predicted to keep changing and evolving When working with dynamic JSON content When keeping data denormalized is not a problem. i.e. to have eventual consistency MongoDB Advantages Flexibility \u2014 with Schemaless documents, the number of fields, content and size of the document can differ from one document to another in the same collection. Easy to scale with sharding MongoDB Disadvantages High Memory Usage \u2014 a lot of denormalized data is kept in memory Document size limit \u2014 16MB Non optimal replication solution (data cannot be re-replicated after recovery from failure). Consistency issues on traffic switch to another data center (No automatic remaster) Graph databases A graph database's purpose is to make it easy to build and run applications that work with highly connected data sets. Typical use cases for a graph database include social networking, recommendation engines, fraud detection, and knowledge graphs. Amazon Neptune is a fully managed graph database service. Neptune supports both the Property Graph model and the Resource Description Framework (RDF), giving you the choice of two graph APIs: TinkerPop and RDF/SPARQL. Startups use Amazon Neptune to build knowledge graphs, make in-game offer recommendations, and detect fraud. \ud835\uddda\ud835\uddff\ud835\uddee\ud835\uddfd\ud835\uddf5 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00 Graph databases map the relationships between data using nodes and edges. Nodes are the individual data values, and edges are the relationships between those values. Use cases : Social graphs, recommendation engines, and fraud detection. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: New4j, Amazon Neptune, Azure Gremlin. In-memory databases Financial services, ecommerce, web, and mobile applications have use cases such as leaderboards, session stores, and real-time analytics that require microsecond response times and can have large spikes in traffic coming at any time. We built Amazon ElastiCache, offering Memcached and Redis, to serve low latency, high throughput workloads that cannot be served with disk-based data stores. Amazon DynamoDB Accelerator (DAX) is another example of a purpose-built data store. DAX was built to make DynamoDB reads an order of magnitude faster, from milliseconds to microseconds, even at millions of requests per second. \ud835\udddc\ud835\uddfb-\ud835\uddfa\ud835\uddf2\ud835\uddfa\ud835\uddfc\ud835\uddff\ud835\ude06 \ud835\uddde\ud835\uddf2\ud835\ude06-\ud835\udde9\ud835\uddee\ud835\uddf9\ud835\ude02\ud835\uddf2 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00 The data is primarily stored in memory, unlike disk-based databases. By eliminating disk access, these databases enable minimal response times. Because all data is stored in main memory, in-memory databases risk losing data upon a process or server failure. In-memory databases can persist data on disks by storing each operation in a log or by taking snapshots. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Redis, Memcached, Amazon ElastiCache Search Databases Many applications output logs to help developers troubleshoot issues. Amazon Elasticsearch Service, or Amazon ES, is purpose-built for providing near real-time visualizations and analytics of machine-generated data by indexing, aggregating, and searching semi-structured logs and metrics. Amazon ES is also a powerful, high-performance search engine for full-text search use cases. Startups store billions of documents for a variety of mission-critical use cases, ranging from operational monitoring and troubleshooting to distributed application stack tracing and pricing optimization. Amazon ElasticSearch Other DBs in this category Apache Solr, Splunk, Amazon CloudSearch When to use Elasticsearch When you need to perform fuzzy search or have results with ranking When you have another data store as source of truth (populate Elasticsearch as a materialized view) Elasticsearch Advantages Easy to horizontally scale with index sharding Rich search API Query for analytical data using aggregations Elasticsearch Disadvantages Indexes are created with a predefined number of shards. More shards requires migration to a new index. Usually done with ReIndex API Performance issues when indices hit very large scale (> 1TB with hundreds of nodes and shards) Wide Column Databases Wide column databases are based on tables but without a strict column format. Rows do not need a value in every column and segments of rows and columns containing different data formats can be combined. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Telemetry, analytics data, messaging, and time-series data. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Apache Cassandra, Azure Table Storage, HBase Time Series Databases These DBs store data in time-ordered streams. Data is not sorted by value or ID but by the time of collection, ingestion, or other timestamps included in the metadata. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Industrial telemetry, DevOps, and Internet of things (IoT) applications. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Graphite, Prometheus, Amazon Timestream Ledger Databases Ledger databases are based on logs that record events related to data values. These DBs store data changes that are used to verify the integrity of data. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Banking systems, registrations, supply chains, and systems of record. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Amazon Quantum Ledger Database (QLDB)","title":"General"},{"location":"db/db_basic/#how-to-choose-db","text":"","title":"How to choose DB"},{"location":"db/db_basic/#integration","text":"The most important thing to consider while choosing the right database is what system you need to integrate together? Make sure that your database management system can be integrated with other tools and services within your project. Different technologies have different connectors for different other technologies. For example, if you have a big analytics job that\u2019s currently running an Apache spark then probably you want to limit yourself to external databases that can connect easily to apache spark.","title":"Integration"},{"location":"db/db_basic/#scaling-requirement","text":"It\u2019s important to know the scaling requirement before installing your production database. How much data are you really talking about? Is it really going to grow unbounded over time? if so then you need some sort of database technology that is not limited to the data that you can store on one PC. You need to look at something like Cassandra or MongoDB or HBase where you can actually distribute the storage of your data across an entire cluster and scale horizontally instead of vertically. While choosing a database you also need to think about the transaction rate or throughput which means how many requests you intend to get per second. Databases with high throughput can support many simultaneous users. If we are talking about thousands then again a single database service is not going to work out. This is especially important when you are working on some big websites where we have a lot of web servers that are serving a lot of people at the same time. You will have to choose a database that is distributed and allows you to spread out a load of those transactions more evenly. In those situations, NoSQL databases are a good choice instead of RDBMS.","title":"Scaling Requirement"},{"location":"db/db_basic/#support-consideration","text":"Think about the supports you might need for your database. Do you have the in-house expertise to spin up this new technology and actually configure it properly? It\u2019s going to be harder than you think especially if you\u2019re using this in the real world or any sort of situation where you have personally identifiable information in the mix from your end-users. In that case, you need to make sure you\u2019re thinking about the security of your system. The truth is most of the NoSQL database we\u2019ve talked about if you configure them with their default settings there will be no security at all.","title":"Support Consideration"},{"location":"db/db_basic/#cap-consideration","text":"CAP stands for Consistency, Availability, and Partition tolerance. The theorem states that you cannot achieve all the properties at the best level in a single database, as there are natural trade offs between the items. You can only pick two out of three at a time and that totally depends on your prioritize based on your requirements. For example, if your system needs to be available and partition tolerant, then you must be willing to accept some latency in your consistency requirements. Traditional relational databases are a natural fit for the CA side whereas Non-relational database engines mostly satisfy AP and CP requirements. Consistency means that any read request will return the most recent write. Data consistency is usually \u201cstrong\u201d for SQL databases and for NoSQL database consistency may be anything from \u201ceventual\u201d to \u201cstrong\u201d. Availability means that a non-responding node must respond in a reasonable amount of time. Not every application needs to run 24/7 with 99.999% availability but most likely you will prefer a database with higher availability. Partition tolerance means the system will continue to operate despite network or node failures.","title":"CAP Consideration"},{"location":"db/db_basic/#schemas-or-data-model","text":"Relational databases store data in a fixed and predefined structure. It means when you start development you will have to define your data schema in terms of tables and columns. You have to change the schema every time the requirements change. This will lead to creating new columns, defining new relations, reflecting the changes in your application, discussing with your database administrators, etc. NoSQL database provides much more flexibility when it comes to handling data. There is no requirement to specify the schema to start working with the application. Also, the NoSQL database doesn\u2019t put a restriction on the types of data you can store together. It allows you to add more new types as your needs change. In the application building process, most of the developers prefer high coding velocity and great agility. NoSQL databases have proven to be a much better choice in that regard especially for agile development which requires fast implementation.","title":"Schemas or Data Model"},{"location":"db/db_basic/#types-of-db","text":"You have a variety of options available in relational (MySQL, PostgreSQL, Oracle DB, etc) and non-relational (MongoDB, Apache HBase, Cassandra, etc) database but you need to understand none of them fits on all kinds of projects requirement. Each one of them has some strengths and weaknesses. Databases through two lenses: access characteristics and the pattern of the data being stored.","title":"Types of DB"},{"location":"db/db_basic/#relational","text":"With large spans of usage, relational databases are still the dominant database type today. A relational database is self-describing because it enables developers to define the database's schema as well as relations and constraints between rows and tables in the database. Developers rely on the functionality of the relational database and not the application code to enforce the schema and preserve the referential integrity of the data within the database. Typical use cases for a relational database include web and mobile applications, enterprise applications, and online gaming. Various flavors or versions of Amazon RDS and Amazon Aurora are used by startups for high-performance and scalable applications on AWS. Both RDS and Aurora are fully managed, scalable systems.","title":"Relational"},{"location":"db/db_basic/#mysql","text":"Relational (+Document since 5.7.8) SQL with JOINS JSON type support (since 5.7.8) Open source (with proprietary, closed-sourced modules) When to use MySQL When you already widely use it in your organization When you want both relational tables (when you know the schema upfront) and JSON collections (Schemaless) Relational / normalized \u2014 when you need to optimize on writes instead of reads, to have strong read consistency MySQL Advantages Maturity & Reliability \u2014 MySQL is highly used, battle tested and mature Fast read performance Improved JSON/Document support (MySQL 8) Cross-DC write consistency (when ProxySQL is used) MySQL Disadvantages Scalability \u2014 Does not scale horizontally. Limited by amount of disk space Consistency and Replication Issues (when not using ProxySQL) Other DBs in this category PostgreSQL, MariaDB, SQL Server, Oracle, Db2, SQLite","title":"MySQL"},{"location":"db/db_basic/#key-value","text":"As your system grows, large amounts of data are often in the form of key-value data, where a single row maps to a primary key. Key-value databases are highly partitionable and allow horizontal scaling at levels that other types of databases cannot achieve. Use cases such as gaming, ad tech, and IoT lend themselves particularly well to the key-value data model where the access patterns require low-latency Gets/Puts for known key values. Amazon DynamoDB is a managed key-value and document database that delivers single-digit millisecond performance at any scale. Key-value DBs store data in pairs, each containing a unique ID and a data value. These DBs provide a flexible storage structure since values can store any amount of unstructured data. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Session management, user preferences, and product recommendations. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Amazon DynamoDB, Azure Cosmos DB.","title":"Key-value"},{"location":"db/db_basic/#amazon-dynamodb","text":"Other DBs in this category Cassandra, HBase, Redis (Key-value Store) When to use DynamoDB When you need a simple key value store without complex querying patterns When you need to store expirable data Low-medium throughput apps as writes are expensive and consistent reads are twice the cost of eventually consistent reads. DynamoDB Advantages Fast performance in any scale (as long as enough capacity is provisioned) No storage limit Schemaless \u2014 it\u2019s possible to define a schema for each item, rather than for the whole table +Multi-master replication (update data in multiple regions) Supports TTL per item Built-in CDC events (DynamoDB streams) DynamoDB Disadvantages Size limit \u2014 item can only reach 400KB in size Limited querying options (limited number of indices) Throttling on burst throughput (and hot keys in certain situations)","title":"Amazon DynamoDB"},{"location":"db/db_basic/#amazon-simple-storage-service-s3","text":"Other DBs in this category Google Cloud Storage, Azure Blob Storage When to use S3 When you need to store large binary objects/files (up to 5TB each) When the amount of data you need to store is large (>10TB), continues to grow daily, and may need to be retrieved (can\u2019t be deleted) S3 Advantages Supports very high throughput Infinite scalability \u2014 No limit on amount of storage S3 Disadvantages No Query support, only key-based retrieval Latency is 100\u2013200 ms for small objects. Caching can alleviate this","title":"Amazon Simple Storage Service (S3)"},{"location":"db/db_basic/#document","text":"Document databases are intuitive for developers to use, because the data in the application tier is typically represented as a JSON document. Developers can persist data using the same document model format that they use in their application code and use the flexible schema model of Amazon DocumentDB to achieve developer efficiency. \ud835\uddd7\ud835\uddfc\ud835\uddf0\ud835\ude02\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00 Document databases are structured similarly to key-value databases except that keys and values are stored in documents written in a markup language like JSON, XML, or YAML. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: User profiles, product catalogs, and content management. Examples: MongoDB, Amazon DocumentDB","title":"Document"},{"location":"db/db_basic/#mongodb","text":"When to use MongoDB When data schema is predicted to keep changing and evolving When working with dynamic JSON content When keeping data denormalized is not a problem. i.e. to have eventual consistency MongoDB Advantages Flexibility \u2014 with Schemaless documents, the number of fields, content and size of the document can differ from one document to another in the same collection. Easy to scale with sharding MongoDB Disadvantages High Memory Usage \u2014 a lot of denormalized data is kept in memory Document size limit \u2014 16MB Non optimal replication solution (data cannot be re-replicated after recovery from failure). Consistency issues on traffic switch to another data center (No automatic remaster)","title":"MongoDB"},{"location":"db/db_basic/#graph-databases","text":"A graph database's purpose is to make it easy to build and run applications that work with highly connected data sets. Typical use cases for a graph database include social networking, recommendation engines, fraud detection, and knowledge graphs. Amazon Neptune is a fully managed graph database service. Neptune supports both the Property Graph model and the Resource Description Framework (RDF), giving you the choice of two graph APIs: TinkerPop and RDF/SPARQL. Startups use Amazon Neptune to build knowledge graphs, make in-game offer recommendations, and detect fraud. \ud835\uddda\ud835\uddff\ud835\uddee\ud835\uddfd\ud835\uddf5 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00 Graph databases map the relationships between data using nodes and edges. Nodes are the individual data values, and edges are the relationships between those values. Use cases : Social graphs, recommendation engines, and fraud detection. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: New4j, Amazon Neptune, Azure Gremlin.","title":"Graph databases"},{"location":"db/db_basic/#in-memory-databases","text":"Financial services, ecommerce, web, and mobile applications have use cases such as leaderboards, session stores, and real-time analytics that require microsecond response times and can have large spikes in traffic coming at any time. We built Amazon ElastiCache, offering Memcached and Redis, to serve low latency, high throughput workloads that cannot be served with disk-based data stores. Amazon DynamoDB Accelerator (DAX) is another example of a purpose-built data store. DAX was built to make DynamoDB reads an order of magnitude faster, from milliseconds to microseconds, even at millions of requests per second. \ud835\udddc\ud835\uddfb-\ud835\uddfa\ud835\uddf2\ud835\uddfa\ud835\uddfc\ud835\uddff\ud835\ude06 \ud835\uddde\ud835\uddf2\ud835\ude06-\ud835\udde9\ud835\uddee\ud835\uddf9\ud835\ude02\ud835\uddf2 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00 The data is primarily stored in memory, unlike disk-based databases. By eliminating disk access, these databases enable minimal response times. Because all data is stored in main memory, in-memory databases risk losing data upon a process or server failure. In-memory databases can persist data on disks by storing each operation in a log or by taking snapshots. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Redis, Memcached, Amazon ElastiCache","title":"In-memory databases"},{"location":"db/db_basic/#search-databases","text":"Many applications output logs to help developers troubleshoot issues. Amazon Elasticsearch Service, or Amazon ES, is purpose-built for providing near real-time visualizations and analytics of machine-generated data by indexing, aggregating, and searching semi-structured logs and metrics. Amazon ES is also a powerful, high-performance search engine for full-text search use cases. Startups store billions of documents for a variety of mission-critical use cases, ranging from operational monitoring and troubleshooting to distributed application stack tracing and pricing optimization.","title":"Search Databases"},{"location":"db/db_basic/#amazon-elasticsearch","text":"Other DBs in this category Apache Solr, Splunk, Amazon CloudSearch When to use Elasticsearch When you need to perform fuzzy search or have results with ranking When you have another data store as source of truth (populate Elasticsearch as a materialized view) Elasticsearch Advantages Easy to horizontally scale with index sharding Rich search API Query for analytical data using aggregations Elasticsearch Disadvantages Indexes are created with a predefined number of shards. More shards requires migration to a new index. Usually done with ReIndex API Performance issues when indices hit very large scale (> 1TB with hundreds of nodes and shards)","title":"Amazon ElasticSearch"},{"location":"db/db_basic/#wide-column-databases","text":"Wide column databases are based on tables but without a strict column format. Rows do not need a value in every column and segments of rows and columns containing different data formats can be combined. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Telemetry, analytics data, messaging, and time-series data. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Apache Cassandra, Azure Table Storage, HBase","title":"Wide Column Databases"},{"location":"db/db_basic/#time-series-databases","text":"These DBs store data in time-ordered streams. Data is not sorted by value or ID but by the time of collection, ingestion, or other timestamps included in the metadata. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Industrial telemetry, DevOps, and Internet of things (IoT) applications. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Graphite, Prometheus, Amazon Timestream","title":"Time Series Databases"},{"location":"db/db_basic/#ledger-databases","text":"Ledger databases are based on logs that record events related to data values. These DBs store data changes that are used to verify the integrity of data. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00: Banking systems, registrations, supply chains, and systems of record. \ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00: Amazon Quantum Ledger Database (QLDB)","title":"Ledger Databases"},{"location":"db/nosql/db_redis/","text":"Redis: High-Performance In-Memory Data Store Redis (Remote Dictionary Server) is an open-source, in-memory key-value data store known for its speed, scalability, and flexibility. It is commonly used for caching, real-time analytics, session storage, and as a message broker. Key Features Blazing Fast Performance: In-memory storage enables sub-millisecond read/write operations. Data Persistence: Supports both snapshotting (RDB) and append-only file persistence (AOF). Data Structures: Provides rich data types like Strings, Lists, Sets, Sorted Sets, Hashes, Bitmaps, and HyperLogLogs. Pub/Sub Messaging: Enables real-time communication between distributed applications. High Availability: Supports replication , clustering , and sentinel mode for fault tolerance. Atomic Operations: All commands are atomic, ensuring data consistency. Lua Scripting: Supports embedded Lua scripting for efficient processing. Particularities of Redis In-Memory vs. Persistent Storage Redis operates primarily in RAM , making it much faster than traditional databases like MySQL or PostgreSQL. Offers optional persistence with RDB snapshots and AOF logs. Data Structures & Use Cases Strings: Storing key-value pairs (e.g., session data, counters, user profiles). Lists: Used for message queues and task management. Sets & Sorted Sets: Ideal for leaderboards, tagging systems, and unique elements. Hashes: Storing structured objects like JSON-like documents. Replication & Scaling Master-Slave Replication: Enables read scaling with replica nodes. Redis Cluster: Distributes data across multiple nodes for horizontal scaling. Sentinel Mode: Provides automatic failover for high availability. Transactions & Atomicity Supports MULTI/EXEC transactions for executing multiple commands atomically. Watch Keys: Allows optimistic locking to prevent race conditions. Cache Expiry & Eviction Policies Supports TTL (Time-To-Live) for automatic expiration of keys. Offers multiple eviction policies like Least Recently Used (LRU) and Least Frequently Used (LFU) . Best Use Cases for Redis Caching: Frequently accessed data (e.g., API responses, session storage, database query results). Real-Time Analytics: Processing high-speed metrics and event tracking. Message Queues: Lightweight Pub/Sub system for real-time messaging. Rate Limiting: Controlling API request rates with atomic counters. Gaming Leaderboards: Maintaining ranked scores with Sorted Sets. Machine Learning & AI: Storing embeddings, recommendation data, and feature stores. Redis vs. Other Databases Feature Redis Speed Extremely fast (in-memory) Persistence Optional (RDB, AOF) Data Structures Rich (Lists, Sets, Hashes, etc.) Replication & Scaling Master-Slave, Clustering Transactions Supports MULTI/EXEC (atomic operations) Full-Text Search Not natively supported Use Case Caching, real-time processing, message queues Example Usage in Python Setting Up Redis import sys IN_COLAB = 'google.colab' in sys . modules if IN_COLAB : ! pip install redis - server ! / usr / local / lib / python */ dist - packages / redis_server / bin / redis - server -- daemonize yes else : ! redis - server -- daemonize yes try : import redis except ImportError : ! pip install redis import redis r = redis . Redis () Basic Operations The set method adds a key-value pair to the database. In the following example, the key and value are both strings. r . set ( 'key' , 'value' ) print ( r . get ( 'key' )) # Output: b'value' The get method looks up a key and returns the corresponding value. The result is not actually a string; it is a bytearray. For many purposes, a bytearray behaves like a string so for now we will treat it like a string and deal with differences as they arise. The values can be integers or floating-point numbers. r . set ( 'x' , 5 ) r . incr ( 'x' ) print ( int ( r . get ( 'x' ))) # Output: 6 And Redis provides some functions that understand numbers, like incr. But if you get a numeric value, the result is a bytearray. If you want to do math with it, you have to convert it back to a number, using the built-in int function. Setting Multiple Values d = dict ( x = 5 , y = 'string' , z = 1.23 ) r . mset ( d ) print ( r . get ( 'y' )) # Output: b'string' print ( r . get ( 'z' )) # Output: b'1.23' If you try to store any other type in a Redis database, you get an error. from redis import DataError t = [ 1 , 2 , 3 ] try : r . set ( 't' , t ) except DataError as e : print ( e ) JSON We could use the repr function to create a string representation of a list, but that representation is Python-specific. It would be better to make a database that can work with any language. To do that, we can use JSON to create a string representation. The json module provides a function dumps, that creates a language-independent representation of most Python objects. import json t = [ 1 , 2 , 3 ] s = json . dumps ( t ) s When we read one of these strings back, we can use loads to convert it back to a Python object. t = json . loads ( s ) t Lists The rpush method adds new elements to the end of a list (the r indicates the right-hand side of the list). r . rpush ( 't' , 1 , 2 , 3 ) print ( r . lrange ( 't' , 0 , - 1 )) # Output: [b'1', b'2', b'3'] You don\u2019t have to do anything special to create a list; if it doesn\u2019t exist, Redis creates it. llen returns the length of the list. r . llen ( 't' ) lrange gets elements from a list. With the indices 0 and -1, it gets all of the elements. The result is a Python list, but the elements are bytestrings. rpop removes elements from the end of the list. r . rpop ( 't' ) Note: Redis lists behave like linked lists, so you can add and remove elements from either end in constant time. r . lpush ( 't' , - 3 , - 2 , - 1 ) r . lpop ( 't' ) Hashes A Redis hash is similar to a Python dictionary, but just to make things confusing the nomenclature is a little different. What we would call a \u201ckey\u201d in a Python dictionary is called a \u201cfield\u201d in a Redis hash. The hset method sets a field-value pair in a hash: r . hset ( 'h' , 'field' , 'value' ) print ( r . hget ( 'h' , 'field' )) # Output: b'value' The hget method looks up a field and returns the corresponding value. hset can also take a Python dictionary as a parameter: d = dict ( a = 1 , b = 2 , c = 3 ) r . hset ( 'h' , mapping = d ) for field , value in r . hscan_iter ( 'h' ): print ( field , value ) To iterate the elements of a hash, we can use hscan_iter. The results are bytestrings for both the fields and values. Deleting Data for key in r . keys (): r . delete ( key ) Anagrams!! We\u2019ll start by solving this problem again using Python data structures; then we\u2019ll translate it into Redis. The following cell downloads a file that contains the list of words. from os.path import basename , exists def download ( url ): filename = basename ( url ) if not exists ( filename ): from urllib.request import urlretrieve local , _ = urlretrieve ( url , filename ) print ( 'Downloaded ' + local ) download ( 'https://github.com/AllenDowney/DSIRP/raw/main/american-english' ) And here\u2019s a generator function that reads the words in the file and yields them one at a time. def iterate_words ( filename ): \"\"\"Read lines from a file and split them into words.\"\"\" for line in open ( filename ): for word in line . split (): yield word . strip () The \u201csignature\u201d of a word is a string that contains the letter of the word in sorted order. So if two words are anagrams, they have the same signature. def signature ( word ): return '' . join ( sorted ( word )) The following loop makes a dictionary of anagram lists. anagram_dict = {} for word in iterate_words ( 'american-english' ): key = signature ( word ) anagram_dict . setdefault ( key , []) . append ( word ) The following loop prints all anagram lists with 6 or more words for v in anagram_dict . values (): if len ( v ) >= 6 : print ( len ( v ), v ) Now, to do the same thing in Redis, we have two options: We can store the anagram lists using Redis lists, using the signatures as keys. We can store the whole data structure in a Redis hash. A problem with the first option is that the keys in a Redis database are like global variables. If we create a large number of keys, we are likely to run into name conflicts. We can mitigate this problem by giving each key a prefix that identifies its purpose. The following loop implements the first option, using \u201cAnagram\u201d as a prefix for the keys. for word in iterate_words ( 'american-english' ): key = f 'Anagram: { signature ( word ) } ' r . rpush ( key , word ) An advantage of this option is that it makes good use of Redis lists. A drawback is that makes many small database transactions, so it is relatively slow. We can use keys to get a list of all keys with a given prefix. keys = r . keys ( 'Anagram*' ) len ( keys ) Before we go on, we can delete the keys from the database like this. r . delete ( * keys ) The second option is to compute the dictionary of anagram lists locally and then store it as a Redis hash. The following function uses dumps to convert lists to strings that can be stored as values in a Redis hash. hash_key = 'AnagramHash' for field , t in anagram_dict . items (): value = json . dumps ( t ) r . hset ( hash_key , field , value ) We can do the same thing faster if we convert all of the lists to JSON locally and store all of the field-value pairs with one hset command. First, I\u2019ll delete the hash we just created. r . delete ( hash_key ) Shut down If you are running a notebook on your own computer, you can use the following command to shut down the Redis server. If you are running on Colab, it\u2019s not really necessary: the Redis server will get shut down when the Colab runtime shuts down (and everything stored in it will disappear). ! killall redis - server Redis is a highly efficient, in-memory data store best suited for caching, real-time analytics, session management, and messaging systems . Its speed, scalability, and advanced data structures make it a crucial component in modern web applications. However, it is not a traditional relational database, and proper use of persistence mechanisms is necessary for data durability.","title":"Redis\ud83d\udd25"},{"location":"db/nosql/db_redis/#redis-high-performance-in-memory-data-store","text":"Redis (Remote Dictionary Server) is an open-source, in-memory key-value data store known for its speed, scalability, and flexibility. It is commonly used for caching, real-time analytics, session storage, and as a message broker.","title":"Redis: High-Performance In-Memory Data Store"},{"location":"db/nosql/db_redis/#key-features","text":"Blazing Fast Performance: In-memory storage enables sub-millisecond read/write operations. Data Persistence: Supports both snapshotting (RDB) and append-only file persistence (AOF). Data Structures: Provides rich data types like Strings, Lists, Sets, Sorted Sets, Hashes, Bitmaps, and HyperLogLogs. Pub/Sub Messaging: Enables real-time communication between distributed applications. High Availability: Supports replication , clustering , and sentinel mode for fault tolerance. Atomic Operations: All commands are atomic, ensuring data consistency. Lua Scripting: Supports embedded Lua scripting for efficient processing.","title":"Key Features"},{"location":"db/nosql/db_redis/#particularities-of-redis","text":"","title":"Particularities of Redis"},{"location":"db/nosql/db_redis/#in-memory-vs-persistent-storage","text":"Redis operates primarily in RAM , making it much faster than traditional databases like MySQL or PostgreSQL. Offers optional persistence with RDB snapshots and AOF logs.","title":"In-Memory vs. Persistent Storage"},{"location":"db/nosql/db_redis/#data-structures-use-cases","text":"Strings: Storing key-value pairs (e.g., session data, counters, user profiles). Lists: Used for message queues and task management. Sets & Sorted Sets: Ideal for leaderboards, tagging systems, and unique elements. Hashes: Storing structured objects like JSON-like documents.","title":"Data Structures &amp; Use Cases"},{"location":"db/nosql/db_redis/#replication-scaling","text":"Master-Slave Replication: Enables read scaling with replica nodes. Redis Cluster: Distributes data across multiple nodes for horizontal scaling. Sentinel Mode: Provides automatic failover for high availability.","title":"Replication &amp; Scaling"},{"location":"db/nosql/db_redis/#transactions-atomicity","text":"Supports MULTI/EXEC transactions for executing multiple commands atomically. Watch Keys: Allows optimistic locking to prevent race conditions.","title":"Transactions &amp; Atomicity"},{"location":"db/nosql/db_redis/#cache-expiry-eviction-policies","text":"Supports TTL (Time-To-Live) for automatic expiration of keys. Offers multiple eviction policies like Least Recently Used (LRU) and Least Frequently Used (LFU) .","title":"Cache Expiry &amp; Eviction Policies"},{"location":"db/nosql/db_redis/#best-use-cases-for-redis","text":"Caching: Frequently accessed data (e.g., API responses, session storage, database query results). Real-Time Analytics: Processing high-speed metrics and event tracking. Message Queues: Lightweight Pub/Sub system for real-time messaging. Rate Limiting: Controlling API request rates with atomic counters. Gaming Leaderboards: Maintaining ranked scores with Sorted Sets. Machine Learning & AI: Storing embeddings, recommendation data, and feature stores.","title":"Best Use Cases for Redis"},{"location":"db/nosql/db_redis/#redis-vs-other-databases","text":"Feature Redis Speed Extremely fast (in-memory) Persistence Optional (RDB, AOF) Data Structures Rich (Lists, Sets, Hashes, etc.) Replication & Scaling Master-Slave, Clustering Transactions Supports MULTI/EXEC (atomic operations) Full-Text Search Not natively supported Use Case Caching, real-time processing, message queues","title":"Redis vs. Other Databases"},{"location":"db/nosql/db_redis/#example-usage-in-python","text":"","title":"Example Usage in Python"},{"location":"db/nosql/db_redis/#setting-up-redis","text":"import sys IN_COLAB = 'google.colab' in sys . modules if IN_COLAB : ! pip install redis - server ! / usr / local / lib / python */ dist - packages / redis_server / bin / redis - server -- daemonize yes else : ! redis - server -- daemonize yes try : import redis except ImportError : ! pip install redis import redis r = redis . Redis ()","title":"Setting Up Redis"},{"location":"db/nosql/db_redis/#basic-operations","text":"The set method adds a key-value pair to the database. In the following example, the key and value are both strings. r . set ( 'key' , 'value' ) print ( r . get ( 'key' )) # Output: b'value' The get method looks up a key and returns the corresponding value. The result is not actually a string; it is a bytearray. For many purposes, a bytearray behaves like a string so for now we will treat it like a string and deal with differences as they arise. The values can be integers or floating-point numbers. r . set ( 'x' , 5 ) r . incr ( 'x' ) print ( int ( r . get ( 'x' ))) # Output: 6 And Redis provides some functions that understand numbers, like incr. But if you get a numeric value, the result is a bytearray. If you want to do math with it, you have to convert it back to a number, using the built-in int function.","title":"Basic Operations"},{"location":"db/nosql/db_redis/#setting-multiple-values","text":"d = dict ( x = 5 , y = 'string' , z = 1.23 ) r . mset ( d ) print ( r . get ( 'y' )) # Output: b'string' print ( r . get ( 'z' )) # Output: b'1.23' If you try to store any other type in a Redis database, you get an error. from redis import DataError t = [ 1 , 2 , 3 ] try : r . set ( 't' , t ) except DataError as e : print ( e )","title":"Setting Multiple Values"},{"location":"db/nosql/db_redis/#json","text":"We could use the repr function to create a string representation of a list, but that representation is Python-specific. It would be better to make a database that can work with any language. To do that, we can use JSON to create a string representation. The json module provides a function dumps, that creates a language-independent representation of most Python objects. import json t = [ 1 , 2 , 3 ] s = json . dumps ( t ) s When we read one of these strings back, we can use loads to convert it back to a Python object. t = json . loads ( s ) t","title":"JSON"},{"location":"db/nosql/db_redis/#lists","text":"The rpush method adds new elements to the end of a list (the r indicates the right-hand side of the list). r . rpush ( 't' , 1 , 2 , 3 ) print ( r . lrange ( 't' , 0 , - 1 )) # Output: [b'1', b'2', b'3'] You don\u2019t have to do anything special to create a list; if it doesn\u2019t exist, Redis creates it. llen returns the length of the list. r . llen ( 't' ) lrange gets elements from a list. With the indices 0 and -1, it gets all of the elements. The result is a Python list, but the elements are bytestrings. rpop removes elements from the end of the list. r . rpop ( 't' ) Note: Redis lists behave like linked lists, so you can add and remove elements from either end in constant time. r . lpush ( 't' , - 3 , - 2 , - 1 ) r . lpop ( 't' )","title":"Lists"},{"location":"db/nosql/db_redis/#hashes","text":"A Redis hash is similar to a Python dictionary, but just to make things confusing the nomenclature is a little different. What we would call a \u201ckey\u201d in a Python dictionary is called a \u201cfield\u201d in a Redis hash. The hset method sets a field-value pair in a hash: r . hset ( 'h' , 'field' , 'value' ) print ( r . hget ( 'h' , 'field' )) # Output: b'value' The hget method looks up a field and returns the corresponding value. hset can also take a Python dictionary as a parameter: d = dict ( a = 1 , b = 2 , c = 3 ) r . hset ( 'h' , mapping = d ) for field , value in r . hscan_iter ( 'h' ): print ( field , value ) To iterate the elements of a hash, we can use hscan_iter. The results are bytestrings for both the fields and values.","title":"Hashes"},{"location":"db/nosql/db_redis/#deleting-data","text":"for key in r . keys (): r . delete ( key )","title":"Deleting Data"},{"location":"db/nosql/db_redis/#anagrams","text":"We\u2019ll start by solving this problem again using Python data structures; then we\u2019ll translate it into Redis. The following cell downloads a file that contains the list of words. from os.path import basename , exists def download ( url ): filename = basename ( url ) if not exists ( filename ): from urllib.request import urlretrieve local , _ = urlretrieve ( url , filename ) print ( 'Downloaded ' + local ) download ( 'https://github.com/AllenDowney/DSIRP/raw/main/american-english' ) And here\u2019s a generator function that reads the words in the file and yields them one at a time. def iterate_words ( filename ): \"\"\"Read lines from a file and split them into words.\"\"\" for line in open ( filename ): for word in line . split (): yield word . strip () The \u201csignature\u201d of a word is a string that contains the letter of the word in sorted order. So if two words are anagrams, they have the same signature. def signature ( word ): return '' . join ( sorted ( word )) The following loop makes a dictionary of anagram lists. anagram_dict = {} for word in iterate_words ( 'american-english' ): key = signature ( word ) anagram_dict . setdefault ( key , []) . append ( word ) The following loop prints all anagram lists with 6 or more words for v in anagram_dict . values (): if len ( v ) >= 6 : print ( len ( v ), v ) Now, to do the same thing in Redis, we have two options: We can store the anagram lists using Redis lists, using the signatures as keys. We can store the whole data structure in a Redis hash. A problem with the first option is that the keys in a Redis database are like global variables. If we create a large number of keys, we are likely to run into name conflicts. We can mitigate this problem by giving each key a prefix that identifies its purpose. The following loop implements the first option, using \u201cAnagram\u201d as a prefix for the keys. for word in iterate_words ( 'american-english' ): key = f 'Anagram: { signature ( word ) } ' r . rpush ( key , word ) An advantage of this option is that it makes good use of Redis lists. A drawback is that makes many small database transactions, so it is relatively slow. We can use keys to get a list of all keys with a given prefix. keys = r . keys ( 'Anagram*' ) len ( keys ) Before we go on, we can delete the keys from the database like this. r . delete ( * keys ) The second option is to compute the dictionary of anagram lists locally and then store it as a Redis hash. The following function uses dumps to convert lists to strings that can be stored as values in a Redis hash. hash_key = 'AnagramHash' for field , t in anagram_dict . items (): value = json . dumps ( t ) r . hset ( hash_key , field , value ) We can do the same thing faster if we convert all of the lists to JSON locally and store all of the field-value pairs with one hset command. First, I\u2019ll delete the hash we just created. r . delete ( hash_key )","title":"Anagrams!!"},{"location":"db/nosql/db_redis/#shut-down","text":"If you are running a notebook on your own computer, you can use the following command to shut down the Redis server. If you are running on Colab, it\u2019s not really necessary: the Redis server will get shut down when the Colab runtime shuts down (and everything stored in it will disappear). ! killall redis - server Redis is a highly efficient, in-memory data store best suited for caching, real-time analytics, session management, and messaging systems . Its speed, scalability, and advanced data structures make it a crucial component in modern web applications. However, it is not a traditional relational database, and proper use of persistence mechanisms is necessary for data durability.","title":"Shut down"},{"location":"db/sql/db_mysql/","text":"MySQL: Performance-Focused Relational Database MySQL is one of the most popular relational database management systems (RDBMS), known for its speed, reliability, and ease of use. It is widely used in web applications, content management systems, and transactional systems. Key Features High-Speed Transactions: MySQL is optimized for fast read and write operations, making it ideal for web applications. Client-Server Architecture: MySQL operates with a dedicated database server and client connections, ensuring scalability. Storage Engine Options: Supports multiple storage engines, including InnoDB (ACID-compliant) and MyISAM (faster reads but no transactions). Replication & Clustering: Supports master-slave and master-master replication, enabling high availability and scalability. Basic JSON Support: Includes JSON functions like JSON_EXTRACT() , though not as advanced as PostgreSQL\u2019s JSONB . Flexible Indexing: MySQL supports B-tree and full-text indexing but lacks some of PostgreSQL\u2019s advanced indexing options. Security & User Management: Provides role-based access control, SSL encryption, and authentication plugins. Particularities of MySQL Optimized for Read-Heavy Workloads MySQL is particularly suited for applications with frequent read operations, such as content-heavy websites and e-commerce platforms. InnoDB ensures data integrity with transactions and foreign keys, while MyISAM offers faster reads at the cost of transactional support. Replication & Scaling MySQL offers asynchronous replication , allowing read scaling with multiple slave nodes. Group Replication and MySQL Cluster provide high availability and fault tolerance. Concurrency & Locking Mechanisms Row-level locking in InnoDB helps prevent performance bottlenecks during concurrent transactions. MySQL\u2019s concurrency handling is efficient but may not scale as well as PostgreSQL under high write loads. Stored Procedures & Triggers MySQL supports stored procedures, triggers, and events, but they are less feature-rich compared to PostgreSQL. Full-Text Search Limitations Full-text search is available in InnoDB and MyISAM but lacks advanced ranking and search capabilities compared to PostgreSQL\u2019s tsvector . Best Use Cases for MySQL Web Applications: Ideal for content-driven platforms like WordPress, Joomla, and Magento. E-Commerce Systems: Efficient for transactional processing and handling large datasets. Data Warehousing: Used in analytics applications where performance is critical. Cloud & SaaS Applications: Supported by major cloud providers like AWS (RDS, Aurora), Google Cloud, and Azure. MySQL vs. Other Databases Feature MySQL ACID Compliance Yes (InnoDB) Performance Optimized for fast reads and high-speed transactions JSON Support Basic ( JSON type, JSON_EXTRACT() ) Full-Text Search Limited (only in InnoDB, MyISAM) Concurrency Handling Good but weaker than PostgreSQL Replication & Scaling Supports master-slave, master-master replication Security & Roles Role-based access control, SSL encryption MySQL is a powerful, high-performance database suited for web applications, e-commerce platforms, and cloud-based solutions. While it excels in speed and scalability, it has limitations in advanced SQL features and concurrency handling compared to PostgreSQL. For applications requiring simple transactions and high-speed queries, MySQL remains a top choice.","title":"MySQL\ud83d\udc2c"},{"location":"db/sql/db_mysql/#mysql-performance-focused-relational-database","text":"MySQL is one of the most popular relational database management systems (RDBMS), known for its speed, reliability, and ease of use. It is widely used in web applications, content management systems, and transactional systems.","title":"MySQL: Performance-Focused Relational Database"},{"location":"db/sql/db_mysql/#key-features","text":"High-Speed Transactions: MySQL is optimized for fast read and write operations, making it ideal for web applications. Client-Server Architecture: MySQL operates with a dedicated database server and client connections, ensuring scalability. Storage Engine Options: Supports multiple storage engines, including InnoDB (ACID-compliant) and MyISAM (faster reads but no transactions). Replication & Clustering: Supports master-slave and master-master replication, enabling high availability and scalability. Basic JSON Support: Includes JSON functions like JSON_EXTRACT() , though not as advanced as PostgreSQL\u2019s JSONB . Flexible Indexing: MySQL supports B-tree and full-text indexing but lacks some of PostgreSQL\u2019s advanced indexing options. Security & User Management: Provides role-based access control, SSL encryption, and authentication plugins.","title":"Key Features"},{"location":"db/sql/db_mysql/#particularities-of-mysql","text":"","title":"Particularities of MySQL"},{"location":"db/sql/db_mysql/#optimized-for-read-heavy-workloads","text":"MySQL is particularly suited for applications with frequent read operations, such as content-heavy websites and e-commerce platforms. InnoDB ensures data integrity with transactions and foreign keys, while MyISAM offers faster reads at the cost of transactional support.","title":"Optimized for Read-Heavy Workloads"},{"location":"db/sql/db_mysql/#replication-scaling","text":"MySQL offers asynchronous replication , allowing read scaling with multiple slave nodes. Group Replication and MySQL Cluster provide high availability and fault tolerance.","title":"Replication &amp; Scaling"},{"location":"db/sql/db_mysql/#concurrency-locking-mechanisms","text":"Row-level locking in InnoDB helps prevent performance bottlenecks during concurrent transactions. MySQL\u2019s concurrency handling is efficient but may not scale as well as PostgreSQL under high write loads.","title":"Concurrency &amp; Locking Mechanisms"},{"location":"db/sql/db_mysql/#stored-procedures-triggers","text":"MySQL supports stored procedures, triggers, and events, but they are less feature-rich compared to PostgreSQL.","title":"Stored Procedures &amp; Triggers"},{"location":"db/sql/db_mysql/#full-text-search-limitations","text":"Full-text search is available in InnoDB and MyISAM but lacks advanced ranking and search capabilities compared to PostgreSQL\u2019s tsvector .","title":"Full-Text Search Limitations"},{"location":"db/sql/db_mysql/#best-use-cases-for-mysql","text":"Web Applications: Ideal for content-driven platforms like WordPress, Joomla, and Magento. E-Commerce Systems: Efficient for transactional processing and handling large datasets. Data Warehousing: Used in analytics applications where performance is critical. Cloud & SaaS Applications: Supported by major cloud providers like AWS (RDS, Aurora), Google Cloud, and Azure.","title":"Best Use Cases for MySQL"},{"location":"db/sql/db_mysql/#mysql-vs-other-databases","text":"Feature MySQL ACID Compliance Yes (InnoDB) Performance Optimized for fast reads and high-speed transactions JSON Support Basic ( JSON type, JSON_EXTRACT() ) Full-Text Search Limited (only in InnoDB, MyISAM) Concurrency Handling Good but weaker than PostgreSQL Replication & Scaling Supports master-slave, master-master replication Security & Roles Role-based access control, SSL encryption MySQL is a powerful, high-performance database suited for web applications, e-commerce platforms, and cloud-based solutions. While it excels in speed and scalability, it has limitations in advanced SQL features and concurrency handling compared to PostgreSQL. For applications requiring simple transactions and high-speed queries, MySQL remains a top choice.","title":"MySQL vs. Other Databases"},{"location":"db/sql/db_odbc/","text":"Connecting SQL to Programming Languages What is ODBC? (Open Database Connectivity) \ud83d\udee0\ufe0f ODBC (Open Database Connectivity) is a standard interface for connecting applications to databases, regardless of the database system. \ud83d\udca1 Use Case: ODBC allows applications written in Python, PHP, JavaScript, C#, and more to connect to MySQL, PostgreSQL, SQL Server, and other databases. Install ODBC Driver for MySQL (Example for Linux): sudo apt install unixODBC unixODBC-dev sudo apt install odbc-mariadb Configure ODBC in odbc.ini (Linux/Mac) or ODBC Data Source Administrator (Windows). SQL Libraries for Different Programming Languages JavaScript (Node.js) SQL Libraries \ud83d\udfe2 Library Database Support Features mysql2 MySQL Lightweight MySQL client pg (node-postgres) PostgreSQL Native PostgreSQL driver sqlite3 SQLite SQLite support in Node.js knex.js MySQL, PostgreSQL, SQLite SQL query builder for JavaScript Sequelize MySQL, PostgreSQL, SQLite ORM for Node.js \ud83d\udca1 Example: Connecting Node.js to MySQL with mysql2 const mysql = require ( 'mysql2' ); const connection = mysql . createConnection ({ host : 'localhost' , user : 'root' , password : 'mypassword' , database : 'mydb' }); connection . query ( 'SELECT * FROM users' , ( err , results ) => { if ( err ) throw err ; console . log ( results ); }); connection . end (); PHP SQL Libraries \ud83d\udc18 Library Database Support Features MySQLi (Improved MySQL Extension) MySQL Native MySQL support in PHP PDO (PHP Data Objects) MySQL, PostgreSQL, SQLite Supports multiple databases pg_connect PostgreSQL Native PostgreSQL driver \ud83d\udca1 Example: Connecting PHP to MySQL using PDO <?php $dsn = 'mysql:host=localhost;dbname=mydb;charset=utf8mb4' ; $username = 'root' ; $password = 'mypassword' ; try { $pdo = new PDO ( $dsn , $username , $password ); $stmt = $pdo -> query ( \"SELECT * FROM users\" ); while ( $row = $stmt -> fetch ( PDO :: FETCH_ASSOC )) { print_r ( $row ); } } catch ( PDOException $e ) { echo \"Connection failed: \" . $e -> getMessage (); } ?> Python SQL Libraries \ud83d\udc0d Library Database Support Features sqlite3 SQLite Built into Python, easy for local databases mysql-connector-python MySQL Official MySQL connector for Python psycopg2 PostgreSQL Most popular PostgreSQL driver SQLAlchemy MySQL, PostgreSQL, SQLite ORM (Object-Relational Mapping) for Python pyodbc All (via ODBC) Universal ODBC connector \ud83d\udca1 Example: Connecting Python to MySQL with mysql-connector-python import mysql.connector conn = mysql . connector . connect ( host = \"localhost\" , user = \"root\" , password = \"mypassword\" , database = \"mydb\" ) cursor = conn . cursor () cursor . execute ( \"SELECT * FROM users;\" ) for row in cursor . fetchall (): print ( row ) conn . close () pyodbc pip install pyodbc pyodbc is a python-ODBC bridge library that also supports the DB-API 2.0 specification. and can connect to a vast number of databases, including MS SQL Server, MySQL, PostgreSQL, Oracle, Google Big Data, SQLite, among others. supporting the DB-API 2.0 means that code that uses pyodbc can look almost identical to code using SQLite Open Database Connectivity (ODBC) is the standard that allows using identical (or at least very similar) SQL statements for querying different Databases (DBMS). The designers of ODBC aimed to make it independent of database systems and operating systems. ODBC accomplishes DBMS independence by using an ODBC driver as a translation layer between the application and the DBMS. The driver often has to be installed on the client operating system Example connections to connect to a DB, your often need to know the server/IP it is running on, the name of the datanase, and username/password to access the databse import pyodbc server = \"your_server\" db = \"your_db\" user = \"your_user\" password = \"your_password\" MS SQL Server connection_str = \\ 'DRIVER={ODBC Driver 17 for SQL Server};' + \\ f 'SERVER= { server } ;' \\ f 'DATABASE= { db } ;' \\ f 'UID= { user } ;' \\ f 'PWD= { password } ' print ( connection_str ) # # Connect to MS SQL Server # conn = pyodbc.connect(connection_str) MySQL connection_str = \\ \"DRIVER={MySQL ODBC 3.51 Driver};\" \\ f \"SERVER= { server } ;\" \\ f \"DATABASE= { db } ;\" \\ f \"UID= { user } ;\" \\ f \"PASSWORD= { password } ;\" print ( connection_str ) # # Connect to MySQL # conn = pyodbc.connect(connection_str) SQLite We don't to connect to SQLite via ODBC, because python can use these databases directly. however, if we want to show this as a demo, we need to install the SQLite ODBC driver For Windows, you can get the SQLite ODBC driver here . Download \"sqliteodbc.exe\" if you are using 32-bit Python, or \"sqliteodbc_w64.exe\" if you are using 64-bit Python. db = \"example.db\" connection_str = \\ \"Driver=SQLite3 ODBC Driver;\" \\ f \"Database= { db } \" print ( connection_str ) conn = pyodbc . connect ( connection_str ) c = conn . cursor () for row in c . execute ( 'SELECT * FROM stocks ORDER BY price' ): print ( row )","title":"ODBC"},{"location":"db/sql/db_odbc/#connecting-sql-to-programming-languages","text":"","title":"Connecting SQL to Programming Languages"},{"location":"db/sql/db_odbc/#what-is-odbc-open-database-connectivity","text":"ODBC (Open Database Connectivity) is a standard interface for connecting applications to databases, regardless of the database system. \ud83d\udca1 Use Case: ODBC allows applications written in Python, PHP, JavaScript, C#, and more to connect to MySQL, PostgreSQL, SQL Server, and other databases. Install ODBC Driver for MySQL (Example for Linux): sudo apt install unixODBC unixODBC-dev sudo apt install odbc-mariadb Configure ODBC in odbc.ini (Linux/Mac) or ODBC Data Source Administrator (Windows).","title":"What is ODBC? (Open Database Connectivity) \ud83d\udee0\ufe0f"},{"location":"db/sql/db_odbc/#sql-libraries-for-different-programming-languages","text":"","title":"SQL Libraries for Different Programming Languages"},{"location":"db/sql/db_odbc/#javascript-nodejs-sql-libraries","text":"Library Database Support Features mysql2 MySQL Lightweight MySQL client pg (node-postgres) PostgreSQL Native PostgreSQL driver sqlite3 SQLite SQLite support in Node.js knex.js MySQL, PostgreSQL, SQLite SQL query builder for JavaScript Sequelize MySQL, PostgreSQL, SQLite ORM for Node.js \ud83d\udca1 Example: Connecting Node.js to MySQL with mysql2 const mysql = require ( 'mysql2' ); const connection = mysql . createConnection ({ host : 'localhost' , user : 'root' , password : 'mypassword' , database : 'mydb' }); connection . query ( 'SELECT * FROM users' , ( err , results ) => { if ( err ) throw err ; console . log ( results ); }); connection . end ();","title":"JavaScript (Node.js) SQL Libraries \ud83d\udfe2"},{"location":"db/sql/db_odbc/#php-sql-libraries","text":"Library Database Support Features MySQLi (Improved MySQL Extension) MySQL Native MySQL support in PHP PDO (PHP Data Objects) MySQL, PostgreSQL, SQLite Supports multiple databases pg_connect PostgreSQL Native PostgreSQL driver \ud83d\udca1 Example: Connecting PHP to MySQL using PDO <?php $dsn = 'mysql:host=localhost;dbname=mydb;charset=utf8mb4' ; $username = 'root' ; $password = 'mypassword' ; try { $pdo = new PDO ( $dsn , $username , $password ); $stmt = $pdo -> query ( \"SELECT * FROM users\" ); while ( $row = $stmt -> fetch ( PDO :: FETCH_ASSOC )) { print_r ( $row ); } } catch ( PDOException $e ) { echo \"Connection failed: \" . $e -> getMessage (); } ?>","title":"PHP SQL Libraries \ud83d\udc18"},{"location":"db/sql/db_odbc/#python-sql-libraries","text":"Library Database Support Features sqlite3 SQLite Built into Python, easy for local databases mysql-connector-python MySQL Official MySQL connector for Python psycopg2 PostgreSQL Most popular PostgreSQL driver SQLAlchemy MySQL, PostgreSQL, SQLite ORM (Object-Relational Mapping) for Python pyodbc All (via ODBC) Universal ODBC connector \ud83d\udca1 Example: Connecting Python to MySQL with mysql-connector-python import mysql.connector conn = mysql . connector . connect ( host = \"localhost\" , user = \"root\" , password = \"mypassword\" , database = \"mydb\" ) cursor = conn . cursor () cursor . execute ( \"SELECT * FROM users;\" ) for row in cursor . fetchall (): print ( row ) conn . close ()","title":"Python SQL Libraries \ud83d\udc0d"},{"location":"db/sql/db_odbc/#pyodbc","text":"pip install pyodbc pyodbc is a python-ODBC bridge library that also supports the DB-API 2.0 specification. and can connect to a vast number of databases, including MS SQL Server, MySQL, PostgreSQL, Oracle, Google Big Data, SQLite, among others. supporting the DB-API 2.0 means that code that uses pyodbc can look almost identical to code using SQLite Open Database Connectivity (ODBC) is the standard that allows using identical (or at least very similar) SQL statements for querying different Databases (DBMS). The designers of ODBC aimed to make it independent of database systems and operating systems. ODBC accomplishes DBMS independence by using an ODBC driver as a translation layer between the application and the DBMS. The driver often has to be installed on the client operating system","title":"pyodbc"},{"location":"db/sql/db_postgresql/","text":"PostgreSQL: Feature-Rich and Extensible Database PostgreSQL is an advanced open-source relational database management system (RDBMS) known for its extensibility, robustness, and full ACID compliance. It is widely used in applications requiring high concurrency, complex queries, and advanced data types. Key Features Full ACID Compliance: Ensures data integrity through atomicity, consistency, isolation, and durability. Advanced Indexing: Supports B-tree, GIN, BRIN, and Hash indexes for optimized query performance. MVCC (Multi-Version Concurrency Control): Enables high concurrency without locking reads. Powerful JSON Handling: Supports JSONB for efficient storage and indexing of JSON data. Rich SQL Support: Includes Common Table Expressions (CTEs), Window Functions, and Recursive Queries. Extensibility: Allows custom functions in languages like PL/pgSQL, Python, and Perl. Full-Text Search: Provides powerful full-text search capabilities with tsvector and tsquery . Role-Based Access Control: Supports advanced user permissions and authentication methods. Particularities of PostgreSQL Better JSON & NoSQL Capabilities PostgreSQL supports JSONB , a binary JSON format that is faster and more indexable than MySQL\u2019s JSON type. Provides NoSQL-like functionality while maintaining strong relational capabilities. Advanced Query & Indexing Support PostgreSQL allows expression indexes , partial indexes , and covering indexes , which are more powerful than MySQL\u2019s indexing options. Supports recursive queries , WITH RECURSIVE , and common table expressions (CTEs) for better query structuring. Concurrency & Performance Uses MVCC (Multi-Version Concurrency Control) to handle multiple concurrent transactions efficiently. Transactional DDL: Schema changes (like ALTER TABLE ) can be rolled back within a transaction. Parallel Query Execution: Supports parallelized execution of queries for performance optimization. Full-Text Search & Geospatial Data PostgreSQL includes native full-text search with ranking, highlighting, and indexing capabilities. Supports PostGIS extension , enabling advanced geospatial queries and GIS applications. Best Use Cases for PostgreSQL Enterprise Applications: Ideal for applications that require complex transactions and strict data consistency . Data Analytics & Reporting: Supports advanced window functions and aggregation features . High-Concurrency Systems: Performs well under heavy concurrent read/write operations. Geospatial Applications: Best suited for GIS applications due to PostGIS support . Hybrid SQL & NoSQL Applications: Suitable for projects that require structured data with NoSQL-like flexibility . PostgreSQL vs. Other Databases Feature PostgreSQL ACID Compliance Yes (Full ACID support) Performance Excellent for high concurrency and complex queries JSON Support Advanced ( JSONB with indexing) Full-Text Search Powerful ( tsvector , tsquery ) Concurrency Handling Strong (MVCC-based) Replication & Scaling Supports synchronous & logical replication, clustering Security & Roles Advanced role-based access control Conclusion PostgreSQL is a powerful, feature-rich database system ideal for applications requiring complex queries, high concurrency, and advanced data types. Its superior JSON support, full-text search, and extensibility make it a great choice for modern web applications, analytics, and enterprise systems.","title":"PostgreSQL\ud83d\udc18"},{"location":"db/sql/db_postgresql/#postgresql-feature-rich-and-extensible-database","text":"PostgreSQL is an advanced open-source relational database management system (RDBMS) known for its extensibility, robustness, and full ACID compliance. It is widely used in applications requiring high concurrency, complex queries, and advanced data types.","title":"PostgreSQL: Feature-Rich and Extensible Database"},{"location":"db/sql/db_postgresql/#key-features","text":"Full ACID Compliance: Ensures data integrity through atomicity, consistency, isolation, and durability. Advanced Indexing: Supports B-tree, GIN, BRIN, and Hash indexes for optimized query performance. MVCC (Multi-Version Concurrency Control): Enables high concurrency without locking reads. Powerful JSON Handling: Supports JSONB for efficient storage and indexing of JSON data. Rich SQL Support: Includes Common Table Expressions (CTEs), Window Functions, and Recursive Queries. Extensibility: Allows custom functions in languages like PL/pgSQL, Python, and Perl. Full-Text Search: Provides powerful full-text search capabilities with tsvector and tsquery . Role-Based Access Control: Supports advanced user permissions and authentication methods.","title":"Key Features"},{"location":"db/sql/db_postgresql/#particularities-of-postgresql","text":"","title":"Particularities of PostgreSQL"},{"location":"db/sql/db_postgresql/#better-json-nosql-capabilities","text":"PostgreSQL supports JSONB , a binary JSON format that is faster and more indexable than MySQL\u2019s JSON type. Provides NoSQL-like functionality while maintaining strong relational capabilities.","title":"Better JSON &amp; NoSQL Capabilities"},{"location":"db/sql/db_postgresql/#advanced-query-indexing-support","text":"PostgreSQL allows expression indexes , partial indexes , and covering indexes , which are more powerful than MySQL\u2019s indexing options. Supports recursive queries , WITH RECURSIVE , and common table expressions (CTEs) for better query structuring.","title":"Advanced Query &amp; Indexing Support"},{"location":"db/sql/db_postgresql/#concurrency-performance","text":"Uses MVCC (Multi-Version Concurrency Control) to handle multiple concurrent transactions efficiently. Transactional DDL: Schema changes (like ALTER TABLE ) can be rolled back within a transaction. Parallel Query Execution: Supports parallelized execution of queries for performance optimization.","title":"Concurrency &amp; Performance"},{"location":"db/sql/db_postgresql/#full-text-search-geospatial-data","text":"PostgreSQL includes native full-text search with ranking, highlighting, and indexing capabilities. Supports PostGIS extension , enabling advanced geospatial queries and GIS applications.","title":"Full-Text Search &amp; Geospatial Data"},{"location":"db/sql/db_postgresql/#best-use-cases-for-postgresql","text":"Enterprise Applications: Ideal for applications that require complex transactions and strict data consistency . Data Analytics & Reporting: Supports advanced window functions and aggregation features . High-Concurrency Systems: Performs well under heavy concurrent read/write operations. Geospatial Applications: Best suited for GIS applications due to PostGIS support . Hybrid SQL & NoSQL Applications: Suitable for projects that require structured data with NoSQL-like flexibility .","title":"Best Use Cases for PostgreSQL"},{"location":"db/sql/db_postgresql/#postgresql-vs-other-databases","text":"Feature PostgreSQL ACID Compliance Yes (Full ACID support) Performance Excellent for high concurrency and complex queries JSON Support Advanced ( JSONB with indexing) Full-Text Search Powerful ( tsvector , tsquery ) Concurrency Handling Strong (MVCC-based) Replication & Scaling Supports synchronous & logical replication, clustering Security & Roles Advanced role-based access control","title":"PostgreSQL vs. Other Databases"},{"location":"db/sql/db_postgresql/#conclusion","text":"PostgreSQL is a powerful, feature-rich database system ideal for applications requiring complex queries, high concurrency, and advanced data types. Its superior JSON support, full-text search, and extensibility make it a great choice for modern web applications, analytics, and enterprise systems.","title":"Conclusion"},{"location":"db/sql/db_regex/","text":"What is Regex? Regular expressions (regex) are patterns used to match character combinations within strings. In SQL, regex can be used to perform advanced search operations based on string patterns. Regex in MySQL MySQL provides the REGEXP (or its synonym RLIKE ) operator to perform regex-based matching in SQL queries. Syntax Example: SELECT * FROM users WHERE name REGEXP '^A' ; This query selects all records from the users table where the name starts with the letter 'A'. Using Regex in PHP with MySQLi Example Table: users id name email 1 Alice alice@email.com 2 Bob bob@example.com 3 Annabelle annabelle@site.com Step 1: Connect to the Database <?php $mysqli = new mysqli ( \"localhost\" , \"username\" , \"password\" , \"database\" ); if ( $mysqli -> connect_error ) { die ( \"Connection failed: \" . $mysqli -> connect_error ); } ?> Step 2: Execute a Regex Query Example query to fetch users whose names start with 'A': <?php $query = \"SELECT * FROM users WHERE name REGEXP '^A'\" ; $result = $mysqli -> query ( $query ); while ( $row = $result -> fetch_assoc ()) { echo $row [ 'name' ] . \"<br>\" ; } ?> Step 3: Use User Input Safely with Regex It is important to sanitize user input to prevent SQL injection or malformed queries, especially when dealing with regex patterns. Since MySQLi prepared statements do not support binding regex patterns directly, input should be safely escaped. <?php $pattern = 'A' ; // Example user input $pattern = $mysqli -> real_escape_string ( $pattern ); $query = \"SELECT * FROM users WHERE name REGEXP '^ $pattern '\" ; $result = $mysqli -> query ( $query ); while ( $row = $result -> fetch_assoc ()) { echo $row [ 'name' ] . \"<br>\" ; } ?> Common Regex Patterns in MySQL Pattern Description ^A Starts with 'A' e$ Ends with 'e' ^[A-C] Starts with 'A', 'B', or 'C' b.t 'b' followed by any character and 't' ^.{5,10}$ String length between 5 and 10 characters Example usage: $query = \"SELECT * FROM users WHERE name REGEXP 'b.t'\"; Best Practices Always sanitize user input using $mysqli->real_escape_string() . Avoid directly injecting untrusted regex patterns into queries. Use regex in SQL queries for search functionality that requires complex pattern matching. Validate the regex pattern on the server side before using it in a query if it is user-provided. Complete Example: User Search with Regex <?php $mysqli = new mysqli ( \"localhost\" , \"username\" , \"password\" , \"database\" ); if ( $mysqli -> connect_error ) { die ( \"Connection failed: \" . $mysqli -> connect_error ); } $startLetter = 'A' ; $pattern = $mysqli -> real_escape_string ( $startLetter ); $query = \"SELECT * FROM users WHERE name REGEXP '^ $pattern '\" ; $result = $mysqli -> query ( $query ); while ( $row = $result -> fetch_assoc ()) { echo $row [ 'name' ] . \" - \" . $row [ 'email' ] . \"<br>\" ; } $mysqli -> close (); ?>","title":"Regex"},{"location":"db/sql/db_regex/#what-is-regex","text":"Regular expressions (regex) are patterns used to match character combinations within strings. In SQL, regex can be used to perform advanced search operations based on string patterns.","title":"What is Regex?"},{"location":"db/sql/db_regex/#regex-in-mysql","text":"MySQL provides the REGEXP (or its synonym RLIKE ) operator to perform regex-based matching in SQL queries. Syntax Example: SELECT * FROM users WHERE name REGEXP '^A' ; This query selects all records from the users table where the name starts with the letter 'A'.","title":"Regex in MySQL"},{"location":"db/sql/db_regex/#using-regex-in-php-with-mysqli","text":"","title":"Using Regex in PHP with MySQLi"},{"location":"db/sql/db_regex/#example-table-users","text":"id name email 1 Alice alice@email.com 2 Bob bob@example.com 3 Annabelle annabelle@site.com","title":"Example Table: users"},{"location":"db/sql/db_regex/#step-1-connect-to-the-database","text":"<?php $mysqli = new mysqli ( \"localhost\" , \"username\" , \"password\" , \"database\" ); if ( $mysqli -> connect_error ) { die ( \"Connection failed: \" . $mysqli -> connect_error ); } ?>","title":"Step 1: Connect to the Database"},{"location":"db/sql/db_regex/#step-2-execute-a-regex-query","text":"Example query to fetch users whose names start with 'A': <?php $query = \"SELECT * FROM users WHERE name REGEXP '^A'\" ; $result = $mysqli -> query ( $query ); while ( $row = $result -> fetch_assoc ()) { echo $row [ 'name' ] . \"<br>\" ; } ?>","title":"Step 2: Execute a Regex Query"},{"location":"db/sql/db_regex/#step-3-use-user-input-safely-with-regex","text":"It is important to sanitize user input to prevent SQL injection or malformed queries, especially when dealing with regex patterns. Since MySQLi prepared statements do not support binding regex patterns directly, input should be safely escaped. <?php $pattern = 'A' ; // Example user input $pattern = $mysqli -> real_escape_string ( $pattern ); $query = \"SELECT * FROM users WHERE name REGEXP '^ $pattern '\" ; $result = $mysqli -> query ( $query ); while ( $row = $result -> fetch_assoc ()) { echo $row [ 'name' ] . \"<br>\" ; } ?>","title":"Step 3: Use User Input Safely with Regex"},{"location":"db/sql/db_regex/#common-regex-patterns-in-mysql","text":"Pattern Description ^A Starts with 'A' e$ Ends with 'e' ^[A-C] Starts with 'A', 'B', or 'C' b.t 'b' followed by any character and 't' ^.{5,10}$ String length between 5 and 10 characters Example usage: $query = \"SELECT * FROM users WHERE name REGEXP 'b.t'\";","title":"Common Regex Patterns in MySQL"},{"location":"db/sql/db_regex/#best-practices","text":"Always sanitize user input using $mysqli->real_escape_string() . Avoid directly injecting untrusted regex patterns into queries. Use regex in SQL queries for search functionality that requires complex pattern matching. Validate the regex pattern on the server side before using it in a query if it is user-provided.","title":"Best Practices"},{"location":"db/sql/db_regex/#complete-example-user-search-with-regex","text":"<?php $mysqli = new mysqli ( \"localhost\" , \"username\" , \"password\" , \"database\" ); if ( $mysqli -> connect_error ) { die ( \"Connection failed: \" . $mysqli -> connect_error ); } $startLetter = 'A' ; $pattern = $mysqli -> real_escape_string ( $startLetter ); $query = \"SELECT * FROM users WHERE name REGEXP '^ $pattern '\" ; $result = $mysqli -> query ( $query ); while ( $row = $result -> fetch_assoc ()) { echo $row [ 'name' ] . \" - \" . $row [ 'email' ] . \"<br>\" ; } $mysqli -> close (); ?>","title":"Complete Example: User Search with Regex"},{"location":"db/sql/db_sql/","text":"Topic Key Concepts Introduction to SQL What is SQL , SQL vs NoSQL , Installation (SQLite, MySQL, PostgreSQL) Basic SQL Commands SELECT , INSERT , UPDATE , DELETE Working with Tables CREATE TABLE , ALTER TABLE , DROP TABLE , Data Types ( INT , VARCHAR , etc.) Filtering Data WHERE , ORDER BY , LIMIT , DISTINCT SQL Joins & Relationships INNER JOIN , LEFT JOIN , RIGHT JOIN , FULL OUTER JOIN , SELF JOIN , CROSS JOIN Grouping & Aggregation GROUP BY , HAVING , Aggregate Functions ( COUNT() , SUM() , AVG() , MIN() , MAX() ) Subqueries & CTEs Subqueries ( SELECT inside SELECT ), CTEs ( WITH clause), Recursive CTEs Indexing & Performance Optimization CREATE INDEX , Execution Plans ( EXPLAIN ), Query Optimization Transactions & ACID Principles BEGIN , COMMIT , ROLLBACK , ACID Properties (Atomicity, Consistency, Isolation, Durability) Advanced SQL Features CASE Statements, Window Functions ( RANK() , DENSE_RANK() , ROW_NUMBER() ), Pivoting & JSON Handling Stored Procedures & Triggers Creating & Executing Stored Procedures, Triggers, Cursors & Loops NoSQL Features in SQL Working with JSON & XML, Full-Text Search ( tsvector , MATCH AGAINST ) Security & Best Practices SQL Injection Prevention, Role-Based Access ( GRANT , REVOKE ), Data Encryption ( MD5() , SHA256() ) Introduction to SQL What is SQL? SQL ( Structured Query Language ) is used to manage relational databases , allowing you to: \u2714 Store, retrieve, and manipulate data \u2714 Define database structures (tables, indexes, etc.) \u2714 Control access and security SQL is used in Relational Database Management Systems (RDBMS) such as: - MySQL \ud83d\udc2c - PostgreSQL \ud83d\udc18 - SQLite \ud83d\udee2\ufe0f - Microsoft SQL Server \ud83c\udfe2 - Oracle Database \ud83d\udd25 Comparing SQL vs. NoSQL Feature SQL (Relational DBs) NoSQL (Non-Relational DBs) Structure Tables (rows & columns) Documents, Key-Value, Graphs Schema Fixed, predefined Flexible, dynamic Scalability Vertical (scale-up) Horizontal (scale-out) Use Case Structured data (banking, e-commerce) Unstructured data (social media, real-time apps) Popular NoSQL databases: MongoDB \ud83c\udf43, Redis \ud83d\udd25, Cassandra \ud83d\udcbe","title":"General"},{"location":"db/sql/db_sql/#introduction-to-sql","text":"","title":"Introduction to SQL"},{"location":"db/sql/db_sql/#what-is-sql","text":"SQL ( Structured Query Language ) is used to manage relational databases , allowing you to: \u2714 Store, retrieve, and manipulate data \u2714 Define database structures (tables, indexes, etc.) \u2714 Control access and security SQL is used in Relational Database Management Systems (RDBMS) such as: - MySQL \ud83d\udc2c - PostgreSQL \ud83d\udc18 - SQLite \ud83d\udee2\ufe0f - Microsoft SQL Server \ud83c\udfe2 - Oracle Database \ud83d\udd25","title":"What is SQL?"},{"location":"db/sql/db_sql/#comparing-sql-vs-nosql","text":"Feature SQL (Relational DBs) NoSQL (Non-Relational DBs) Structure Tables (rows & columns) Documents, Key-Value, Graphs Schema Fixed, predefined Flexible, dynamic Scalability Vertical (scale-up) Horizontal (scale-out) Use Case Structured data (banking, e-commerce) Unstructured data (social media, real-time apps) Popular NoSQL databases: MongoDB \ud83c\udf43, Redis \ud83d\udd25, Cassandra \ud83d\udcbe","title":"Comparing SQL vs. NoSQL"},{"location":"db/sql/db_sqlacid/","text":"Transactions & ACID Principles BEGIN , COMMIT , ROLLBACK , SAVEPOINT \u2013 Managing Transactions Transactions ensure data integrity by grouping multiple operations. Example: Using transactions BEGIN ; UPDATE accounts SET balance = balance - 100 WHERE account_id = 1 ; UPDATE accounts SET balance = balance + 100 WHERE account_id = 2 ; COMMIT ; Example: Rolling back a failed transaction BEGIN ; UPDATE orders SET status = 'Shipped' WHERE order_id = 5 ; ROLLBACK ; BEGIN vs. BEGIN TRANSACTION BEGIN: In some databases, BEGIN alone is sufficient to start a transaction. However, in others (like SQL Server), BEGIN alone is ambiguous because it is also used in control flow structures (e.g., BEGIN...END for blocks of code). BEGIN TRANSACTION (or BEGIN TRAN): This explicitly starts a transaction and is recommended for clarity, especially in databases like SQL Server. Database System Recommended Usage PostgreSQL BEGIN; or START TRANSACTION; MySQL START TRANSACTION; (preferred) or BEGIN; (can be used but conflicts with stored procedure syntax) SQL Server BEGIN TRANSACTION; or BEGIN TRAN; (explicit, avoids ambiguity) Oracle Transactions are implicitly managed, but BEGIN is used for PL/SQL blocks. Use SET TRANSACTION for control. Microsoft SQL Server BEGIN TRANSACTION; or BEGIN TRAN; (explicit, avoids ambiguity) Summary of Transaction Control Commands Command Purpose Example BEGIN TRANSACTION Starts a new transaction. BEGIN TRANSACTION TransferFunds; COMMIT Saves all changes made during the transaction. COMMIT; ROLLBACK Undoes changes made during the transaction. ROLLBACK; SAVEPOINT Creates a point in the transaction to which you can roll back. SAVEPOINT SP1; ROLLBACK TO SAVEPOINT Rolls back the transaction to a specific savepoint. ROLLBACK TO SP1; RELEASE SAVEPOINT Removes a savepoint from the transaction. RELEASE SAVEPOINT SP2; Scenario: Transferring Funds Between Accounts We want to transfer $500 from Account A (ID: 1) to Account B (ID: 2) . However, if Account B does not exist, we should roll back the transfer. SQL Transaction Example -- Start a new transaction BEGIN TRANSACTION TransferFunds ; -- Deduct $500 from Account A UPDATE accounts SET balance = balance - 500 WHERE account_id = 1 ; -- Create a savepoint in case the next step fails SAVEPOINT SP1 ; -- Attempt to add $500 to Account B UPDATE accounts SET balance = balance + 500 WHERE account_id = 2 ; -- Check if the update was successful IF @@ ROWCOUNT = 0 THEN -- If Account B does not exist, roll back to the savepoint ROLLBACK TO SP1 ; PRINT 'Transfer failed: Account B does not exist. Rolling back deduction.' ; ELSE -- If everything is fine, commit the transaction COMMIT ; PRINT 'Transfer successful!' ; END IF ; -- Release the savepoint since it's no longer needed RELEASE SAVEPOINT SP1 ; IF @@ROWCOUNT = 0 THEN ROLLBACK TO SP1; - If no rows were updated (meaning Account B doesn\u2019t exist), rollback to the savepoint. What is @@ in SQL? In SQL, @@ is used as a prefix for system-defined global variables (also called system functions in some databases). These variables store metadata about the current session, transaction, or system state. Usage of @@ in Different Databases SQL Server : @@ is used for built-in system variables like @@ROWCOUNT , @@TRANCOUNT , @@SERVERNAME , etc. MySQL & PostgreSQL : They do not use @@ for system variables. Instead: MySQL uses @@ for session and global variables , like @@autocommit . PostgreSQL uses functions like pg_backend_pid() instead of @@ variables. Common @@ Variables in SQL Server Variable Purpose Example Usage @@ROWCOUNT Returns the number of rows affected by the last statement. SELECT * FROM employees; PRINT @@ROWCOUNT; @@TRANCOUNT Returns the number of active transactions in the current session. PRINT @@TRANCOUNT; @@IDENTITY Returns the last inserted identity value. INSERT INTO users (name) VALUES ('John'); PRINT @@IDENTITY; @@ERROR Returns the error code from the last statement. UPDATE employees SET salary = NULL; IF @@ERROR <> 0 PRINT 'Error occurred'; @@SERVERNAME Returns the name of the SQL Server instance. SELECT @@SERVERNAME; @@VERSION Returns the version details of the SQL Server. SELECT @@VERSION; Examples of @@ in Action 1. Using @@ROWCOUNT to Check Affected Rows UPDATE employees SET salary = salary * 1 . 1 WHERE department = 'Sales' ; PRINT 'Rows affected: ' + CAST ( @@ ROWCOUNT AS VARCHAR ); If 5 employees in Sales received a 10% salary increase, @@ROWCOUNT would return 5 . 2. Using @@TRANCOUNT to Check Active Transactions BEGIN TRANSACTION ; PRINT 'Active Transactions: ' + CAST ( @@ TRANCOUNT AS VARCHAR ); COMMIT ; This prints the current number of active transactions. 3. Using @@ERROR for Error Handling BEGIN TRANSACTION ; UPDATE employees SET salary = NULL WHERE employee_id = 1 ; IF @@ ERROR <> 0 PRINT 'Error occurred, rolling back!' ; ROLLBACK ; ELSE COMMIT ; If an error occurs, ROLLBACK prevents invalid changes. ACID Properties (Atomicity, Consistency, Isolation, Durability) Atomicity : Transactions are all-or-nothing. Consistency : Data remains valid before and after transactions. Isolation : Transactions execute independently. Durability : Committed data is permanently stored.","title":"ACID"},{"location":"db/sql/db_sqlacid/#transactions-acid-principles","text":"","title":"Transactions &amp; ACID Principles"},{"location":"db/sql/db_sqlacid/#begin-commit-rollback-savepoint-managing-transactions","text":"Transactions ensure data integrity by grouping multiple operations. Example: Using transactions BEGIN ; UPDATE accounts SET balance = balance - 100 WHERE account_id = 1 ; UPDATE accounts SET balance = balance + 100 WHERE account_id = 2 ; COMMIT ; Example: Rolling back a failed transaction BEGIN ; UPDATE orders SET status = 'Shipped' WHERE order_id = 5 ; ROLLBACK ;","title":"BEGIN, COMMIT, ROLLBACK, SAVEPOINT \u2013 Managing Transactions"},{"location":"db/sql/db_sqlacid/#begin-vs-begin-transaction","text":"BEGIN: In some databases, BEGIN alone is sufficient to start a transaction. However, in others (like SQL Server), BEGIN alone is ambiguous because it is also used in control flow structures (e.g., BEGIN...END for blocks of code). BEGIN TRANSACTION (or BEGIN TRAN): This explicitly starts a transaction and is recommended for clarity, especially in databases like SQL Server. Database System Recommended Usage PostgreSQL BEGIN; or START TRANSACTION; MySQL START TRANSACTION; (preferred) or BEGIN; (can be used but conflicts with stored procedure syntax) SQL Server BEGIN TRANSACTION; or BEGIN TRAN; (explicit, avoids ambiguity) Oracle Transactions are implicitly managed, but BEGIN is used for PL/SQL blocks. Use SET TRANSACTION for control. Microsoft SQL Server BEGIN TRANSACTION; or BEGIN TRAN; (explicit, avoids ambiguity)","title":"BEGIN vs. BEGIN TRANSACTION"},{"location":"db/sql/db_sqlacid/#summary-of-transaction-control-commands","text":"Command Purpose Example BEGIN TRANSACTION Starts a new transaction. BEGIN TRANSACTION TransferFunds; COMMIT Saves all changes made during the transaction. COMMIT; ROLLBACK Undoes changes made during the transaction. ROLLBACK; SAVEPOINT Creates a point in the transaction to which you can roll back. SAVEPOINT SP1; ROLLBACK TO SAVEPOINT Rolls back the transaction to a specific savepoint. ROLLBACK TO SP1; RELEASE SAVEPOINT Removes a savepoint from the transaction. RELEASE SAVEPOINT SP2;","title":"Summary of Transaction Control Commands"},{"location":"db/sql/db_sqlacid/#scenario-transferring-funds-between-accounts","text":"We want to transfer $500 from Account A (ID: 1) to Account B (ID: 2) . However, if Account B does not exist, we should roll back the transfer.","title":"Scenario: Transferring Funds Between Accounts"},{"location":"db/sql/db_sqlacid/#sql-transaction-example","text":"-- Start a new transaction BEGIN TRANSACTION TransferFunds ; -- Deduct $500 from Account A UPDATE accounts SET balance = balance - 500 WHERE account_id = 1 ; -- Create a savepoint in case the next step fails SAVEPOINT SP1 ; -- Attempt to add $500 to Account B UPDATE accounts SET balance = balance + 500 WHERE account_id = 2 ; -- Check if the update was successful IF @@ ROWCOUNT = 0 THEN -- If Account B does not exist, roll back to the savepoint ROLLBACK TO SP1 ; PRINT 'Transfer failed: Account B does not exist. Rolling back deduction.' ; ELSE -- If everything is fine, commit the transaction COMMIT ; PRINT 'Transfer successful!' ; END IF ; -- Release the savepoint since it's no longer needed RELEASE SAVEPOINT SP1 ; IF @@ROWCOUNT = 0 THEN ROLLBACK TO SP1; - If no rows were updated (meaning Account B doesn\u2019t exist), rollback to the savepoint.","title":"SQL Transaction Example"},{"location":"db/sql/db_sqlacid/#what-is-in-sql","text":"In SQL, @@ is used as a prefix for system-defined global variables (also called system functions in some databases). These variables store metadata about the current session, transaction, or system state.","title":"What is @@ in SQL?"},{"location":"db/sql/db_sqlacid/#acid-properties-atomicity-consistency-isolation-durability","text":"Atomicity : Transactions are all-or-nothing. Consistency : Data remains valid before and after transactions. Isolation : Transactions execute independently. Durability : Committed data is permanently stored.","title":"ACID Properties (Atomicity, Consistency, Isolation, Durability)"},{"location":"db/sql/db_sqladv2/","text":"CASE Statements \u2013 Conditional Logic The CASE statement allows conditional logic within SQL queries. Example: Categorizing salaries SELECT first_name , last_name , CASE WHEN salary > 80000 THEN 'High' WHEN salary BETWEEN 50000 AND 80000 THEN 'Medium' ELSE 'Low' END AS salary_category FROM employees ; Window Functions ( RANK() , DENSE_RANK() , ROW_NUMBER() ) Window functions perform calculations across table partitions. Example: Ranking employees by salary SELECT first_name , last_name , salary , RANK () OVER ( ORDER BY salary DESC ) AS salary_rank FROM employees ; Pivoting & Unpivoting Data Pivoting transforms row data into columns, while unpivoting does the reverse. JSON Handling ( JSONB , -> , ->> , #> in PostgreSQL) PostgreSQL supports querying JSON data stored in columns. Example: Extracting JSON fields SELECT data -> 'name' AS name FROM customers WHERE data ->> 'status' = 'active' ; Sure! Here's a clear and practical tutorial for using COALESCE in SQL, suitable for beginners and intermediates: COALESCE() The COALESCE() function in SQL returns the first non-null value from a list of expressions. Useful when you're dealing with columns that may contain NULL values and you want to display something instead \u2014 like a default value or a fallback from another column. COALESCE ( expression1 , expression2 , ..., expressionN ) It checks each expression from left to right . Returns the first non-null value it finds. If all expressions are NULL , it returns NULL . Let's say you have a users table: id first_name nickname username 1 John NULL johnny123 2 NULL Sam sam2023 3 NULL NULL guest001 You want to display the best name available in this order: nickname \u2192 first_name \u2192 username SELECT id , COALESCE ( nickname , first_name , username ) AS display_name FROM users ; Result: id display_name 1 John 2 Sam 3 guest001 Use Case: Default Values Let\u2019s say a column can be NULL , but you want to replace it with a default string: SELECT COALESCE ( email , 'no-email@example.com' ) AS user_email FROM users ; This ensures you never show a NULL \u2014 instead, you show a safe placeholder. COALESCE() vs ISNULL() or IFNULL() COALESCE() is standard SQL and portable across databases. ISNULL() is specific to SQL Server . IFNULL() is used in MySQL and SQLite . -- SQL Server ISNULL ( column , 'default' ) -- MySQL / SQLite IFNULL ( column , 'default' ) -- Standard SQL (works everywhere) COALESCE ( column , 'default' ) Use in Aggregations Imagine you\u2019re summing order amounts, but some values are NULL : SELECT customer_id , SUM ( COALESCE ( order_total , 0 )) AS total_spent FROM orders GROUP BY customer_id ; This avoids NULL values skewing your totals. COALESCE() returns the first non-null value. Great for fallback logic and cleaner results. Cross-database friendly. When to Use It Displaying fallback values Cleaning up NULLs in output Creating user-friendly reports Providing default values for calculations","title":"Various"},{"location":"db/sql/db_sqladv2/#case-statements-conditional-logic","text":"The CASE statement allows conditional logic within SQL queries. Example: Categorizing salaries SELECT first_name , last_name , CASE WHEN salary > 80000 THEN 'High' WHEN salary BETWEEN 50000 AND 80000 THEN 'Medium' ELSE 'Low' END AS salary_category FROM employees ;","title":"CASE Statements \u2013 Conditional Logic"},{"location":"db/sql/db_sqladv2/#window-functions-rank-dense_rank-row_number","text":"Window functions perform calculations across table partitions. Example: Ranking employees by salary SELECT first_name , last_name , salary , RANK () OVER ( ORDER BY salary DESC ) AS salary_rank FROM employees ;","title":"Window Functions (RANK(), DENSE_RANK(), ROW_NUMBER())"},{"location":"db/sql/db_sqladv2/#pivoting-unpivoting-data","text":"Pivoting transforms row data into columns, while unpivoting does the reverse.","title":"Pivoting &amp; Unpivoting Data"},{"location":"db/sql/db_sqladv2/#json-handling-jsonb-in-postgresql","text":"PostgreSQL supports querying JSON data stored in columns. Example: Extracting JSON fields SELECT data -> 'name' AS name FROM customers WHERE data ->> 'status' = 'active' ; Sure! Here's a clear and practical tutorial for using COALESCE in SQL, suitable for beginners and intermediates:","title":"JSON Handling (JSONB, -&gt;, -&gt;&gt;, #&gt; in PostgreSQL)"},{"location":"db/sql/db_sqladv2/#coalesce","text":"The COALESCE() function in SQL returns the first non-null value from a list of expressions. Useful when you're dealing with columns that may contain NULL values and you want to display something instead \u2014 like a default value or a fallback from another column. COALESCE ( expression1 , expression2 , ..., expressionN ) It checks each expression from left to right . Returns the first non-null value it finds. If all expressions are NULL , it returns NULL . Let's say you have a users table: id first_name nickname username 1 John NULL johnny123 2 NULL Sam sam2023 3 NULL NULL guest001 You want to display the best name available in this order: nickname \u2192 first_name \u2192 username SELECT id , COALESCE ( nickname , first_name , username ) AS display_name FROM users ; Result: id display_name 1 John 2 Sam 3 guest001","title":"COALESCE()"},{"location":"db/sql/db_sqladv2/#use-case-default-values","text":"Let\u2019s say a column can be NULL , but you want to replace it with a default string: SELECT COALESCE ( email , 'no-email@example.com' ) AS user_email FROM users ; This ensures you never show a NULL \u2014 instead, you show a safe placeholder.","title":"Use Case: Default Values"},{"location":"db/sql/db_sqladv2/#coalesce-vs-isnull-or-ifnull","text":"COALESCE() is standard SQL and portable across databases. ISNULL() is specific to SQL Server . IFNULL() is used in MySQL and SQLite . -- SQL Server ISNULL ( column , 'default' ) -- MySQL / SQLite IFNULL ( column , 'default' ) -- Standard SQL (works everywhere) COALESCE ( column , 'default' )","title":"COALESCE() vs ISNULL() or IFNULL()"},{"location":"db/sql/db_sqladv2/#use-in-aggregations","text":"Imagine you\u2019re summing order amounts, but some values are NULL : SELECT customer_id , SUM ( COALESCE ( order_total , 0 )) AS total_spent FROM orders GROUP BY customer_id ; This avoids NULL values skewing your totals. COALESCE() returns the first non-null value. Great for fallback logic and cleaner results. Cross-database friendly. When to Use It Displaying fallback values Cleaning up NULLs in output Creating user-friendly reports Providing default values for calculations","title":"Use in Aggregations"},{"location":"db/sql/db_sqlbasic/","text":"Basic SQL Commands SELECT \u2013 Retrieving Data The SELECT statement is used to retrieve data from a database. You can specify which columns to retrieve or use * to get all columns. Example 1: Select all columns from a table SELECT * FROM employees ; Example 2: Select specific columns SELECT first_name , last_name , salary FROM employees ; Example 3: Using WHERE to filter results SELECT first_name , last_name FROM employees WHERE department = 'HR' ; INSERT \u2013 Adding Records The INSERT statement is used to add new records into a table. Example 1: Inserting a full row INSERT INTO employees ( first_name , last_name , department , salary ) VALUES ( 'John' , 'Doe' , 'IT' , 60000 ); Example 2: Inserting multiple rows INSERT INTO employees ( first_name , last_name , department , salary ) VALUES ( 'Jane' , 'Smith' , 'Finance' , 70000 ), ( 'Alice' , 'Johnson' , 'HR' , 65000 ); UPDATE \u2013 Modifying Data The UPDATE statement is used to modify existing records. Example: Updating a salary UPDATE employees SET salary = 75000 WHERE first_name = 'John' AND last_name = 'Doe' ; Example: Updating multiple fields UPDATE employees SET department = 'Marketing' , salary = 72000 WHERE employee_id = 5 ; DELETE \u2013 Removing Records The DELETE statement is used to remove records from a table. Example: Deleting a specific record DELETE FROM employees WHERE employee_id = 10 ; Example: Deleting all employees from a department DELETE FROM employees WHERE department = 'HR' ; Working with Tables CREATE TABLE \u2013 Defining Structure The CREATE TABLE statement is used to define a new table. Example: Creating an employees table CREATE TABLE employees ( employee_id INT PRIMARY KEY AUTO_INCREMENT , first_name VARCHAR ( 50 ), last_name VARCHAR ( 50 ), department VARCHAR ( 50 ), salary DECIMAL ( 10 , 2 ), hire_date DATE ); ALTER TABLE \u2013 Modifying Schema The ALTER TABLE statement allows modifications to an existing table. Example: Adding a new column ALTER TABLE employees ADD email VARCHAR ( 100 ); Example: Modifying a column data type ALTER TABLE employees MODIFY salary DECIMAL ( 12 , 2 ); Example: Renaming a column ALTER TABLE employees RENAME COLUMN department TO dept_name ; DROP TABLE \u2013 Deleting a Table The DROP TABLE statement removes a table and all its data. Example: DROP TABLE employees ; Data Types ( INT , VARCHAR , DATE , etc.) Common SQL data types include: INT \u2013 Integer values (e.g., employee_id INT ) VARCHAR(n) \u2013 Variable-length string ( name VARCHAR(100) ) DATE \u2013 Stores date values (e.g., hire_date DATE ) DECIMAL(m,d) \u2013 Decimal numbers with m digits and d decimal places (e.g., salary DECIMAL(10,2) ) SQL data types define the kind of data that can be stored in a table column. While exact data types can vary slightly between SQL dialects (e.g., MySQL, PostgreSQL, SQL Server, Oracle), here\u2019s a comprehensive overview of the main categories of SQL data types used across most systems: Numeric Data Types Used for storing numbers: Integer Types: Type Description TINYINT Very small integers (e.g., -128 to 127) SMALLINT Small integers (e.g., -32,768 to 32,767) MEDIUMINT Medium integers (MySQL specific) INT / INTEGER Standard integer BIGINT Very large integers Decimal/Fixed-point Types: Type Description DECIMAL(p,s) / NUMERIC(p,s) Exact numeric values with precision ( p ) and scale ( s ) p = total digits, s = digits after decimal point Floating-point Types: Type Description FLOAT Approximate floating-point number REAL Higher precision floating-point number DOUBLE / DOUBLE PRECISION Even more precision String/Text Data Types Used for storing characters, text, or binary data. Character Types: Type Description CHAR(n) Fixed-length string (padded with spaces if shorter) VARCHAR(n) Variable-length string, up to n characters Text/Large Object Types: Type Description TEXT Large text block (e.g., article content) TINYTEXT / MEDIUMTEXT / LONGTEXT Different sizes in MySQL CLOB Character Large Object (Oracle/DB2) Binary Data Types: Type Description BINARY(n) Fixed-length binary data VARBINARY(n) Variable-length binary data BLOB Binary Large Object (for images/files etc) Date and Time Data Types Used to store date, time, or both. Type Description DATE Stores a date (YYYY-MM-DD) TIME Stores a time (HH\\:MM\\:SS) DATETIME Stores both date and time TIMESTAMP Stores date and time, often auto-updating YEAR Stores a year (MySQL only) INTERVAL (PostgreSQL) Time intervals Boolean Type Type Description BOOLEAN / BOOL Stores TRUE or FALSE (MySQL uses TINYINT(1) under the hood) UUID / Unique Identifiers Used for globally unique values: Type Description UUID Universally Unique Identifier UNIQUEIDENTIFIER Used in SQL Server JSON and XML For structured data: Type Description JSON Stores JSON-formatted strings XML Stores XML documents (SQL Server, Oracle) Spatial / Geographic Types For storing geolocation data (supported in systems like PostgreSQL with PostGIS, MySQL): Type Description GEOMETRY Base type for all geometry data POINT , LINESTRING , POLYGON , etc. Specific shapes Other Special Types ENUM('value1', 'value2', ...) : Set of predefined values (MySQL) SET : A string object that can store zero or more values from a list (MySQL) ARRAY : A collection of values (PostgreSQL) MONEY / SMALLMONEY : Currency data (SQL Server) Filtering Data WHERE \u2013 Conditional Filtering The WHERE clause is used to filter results based on conditions. Example: Filtering by salary SELECT * FROM employees WHERE salary > 50000 ; Example: Filtering by multiple conditions SELECT * FROM employees WHERE department = 'IT' AND salary > 60000 ; ORDER BY \u2013 Sorting Results The ORDER BY clause sorts results in ascending ( ASC ) or descending ( DESC ) order. Example: Sorting by salary (descending) SELECT * FROM employees ORDER BY salary DESC ; Example: Sorting by department and then by salary SELECT * FROM employees ORDER BY department ASC , salary DESC ; LIMIT \u2013 Restricting Output The LIMIT clause restricts the number of rows returned. Example: Getting the top 5 highest salaries SELECT * FROM employees ORDER BY salary DESC LIMIT 5 ; Example: Paginating results (offset 10, limit 5) SELECT * FROM employees ORDER BY employee_id ASC LIMIT 5 OFFSET 10 ; DISTINCT \u2013 Removing Duplicates The DISTINCT keyword removes duplicate values in query results. Example: Getting unique department names SELECT DISTINCT department FROM employees ; Example: Getting unique job titles from a jobs table SELECT DISTINCT job_title FROM jobs ; These basic SQL commands and filtering techniques help in effectively managing and retrieving data from a relational database system.","title":"Basic SQL Commands"},{"location":"db/sql/db_sqlbasic/#basic-sql-commands","text":"","title":"Basic SQL Commands"},{"location":"db/sql/db_sqlbasic/#select-retrieving-data","text":"The SELECT statement is used to retrieve data from a database. You can specify which columns to retrieve or use * to get all columns. Example 1: Select all columns from a table SELECT * FROM employees ; Example 2: Select specific columns SELECT first_name , last_name , salary FROM employees ; Example 3: Using WHERE to filter results SELECT first_name , last_name FROM employees WHERE department = 'HR' ;","title":"SELECT \u2013 Retrieving Data"},{"location":"db/sql/db_sqlbasic/#insert-adding-records","text":"The INSERT statement is used to add new records into a table. Example 1: Inserting a full row INSERT INTO employees ( first_name , last_name , department , salary ) VALUES ( 'John' , 'Doe' , 'IT' , 60000 ); Example 2: Inserting multiple rows INSERT INTO employees ( first_name , last_name , department , salary ) VALUES ( 'Jane' , 'Smith' , 'Finance' , 70000 ), ( 'Alice' , 'Johnson' , 'HR' , 65000 );","title":"INSERT \u2013 Adding Records"},{"location":"db/sql/db_sqlbasic/#update-modifying-data","text":"The UPDATE statement is used to modify existing records. Example: Updating a salary UPDATE employees SET salary = 75000 WHERE first_name = 'John' AND last_name = 'Doe' ; Example: Updating multiple fields UPDATE employees SET department = 'Marketing' , salary = 72000 WHERE employee_id = 5 ;","title":"UPDATE \u2013 Modifying Data"},{"location":"db/sql/db_sqlbasic/#delete-removing-records","text":"The DELETE statement is used to remove records from a table. Example: Deleting a specific record DELETE FROM employees WHERE employee_id = 10 ; Example: Deleting all employees from a department DELETE FROM employees WHERE department = 'HR' ;","title":"DELETE \u2013 Removing Records"},{"location":"db/sql/db_sqlbasic/#working-with-tables","text":"","title":"Working with Tables"},{"location":"db/sql/db_sqlbasic/#create-table-defining-structure","text":"The CREATE TABLE statement is used to define a new table. Example: Creating an employees table CREATE TABLE employees ( employee_id INT PRIMARY KEY AUTO_INCREMENT , first_name VARCHAR ( 50 ), last_name VARCHAR ( 50 ), department VARCHAR ( 50 ), salary DECIMAL ( 10 , 2 ), hire_date DATE );","title":"CREATE TABLE \u2013 Defining Structure"},{"location":"db/sql/db_sqlbasic/#alter-table-modifying-schema","text":"The ALTER TABLE statement allows modifications to an existing table. Example: Adding a new column ALTER TABLE employees ADD email VARCHAR ( 100 ); Example: Modifying a column data type ALTER TABLE employees MODIFY salary DECIMAL ( 12 , 2 ); Example: Renaming a column ALTER TABLE employees RENAME COLUMN department TO dept_name ;","title":"ALTER TABLE \u2013 Modifying Schema"},{"location":"db/sql/db_sqlbasic/#drop-table-deleting-a-table","text":"The DROP TABLE statement removes a table and all its data. Example: DROP TABLE employees ;","title":"DROP TABLE \u2013 Deleting a Table"},{"location":"db/sql/db_sqlbasic/#data-types-int-varchar-date-etc","text":"Common SQL data types include: INT \u2013 Integer values (e.g., employee_id INT ) VARCHAR(n) \u2013 Variable-length string ( name VARCHAR(100) ) DATE \u2013 Stores date values (e.g., hire_date DATE ) DECIMAL(m,d) \u2013 Decimal numbers with m digits and d decimal places (e.g., salary DECIMAL(10,2) ) SQL data types define the kind of data that can be stored in a table column. While exact data types can vary slightly between SQL dialects (e.g., MySQL, PostgreSQL, SQL Server, Oracle), here\u2019s a comprehensive overview of the main categories of SQL data types used across most systems:","title":"Data Types (INT, VARCHAR, DATE, etc.)"},{"location":"db/sql/db_sqlbasic/#numeric-data-types","text":"Used for storing numbers:","title":"Numeric Data Types"},{"location":"db/sql/db_sqlbasic/#integer-types","text":"Type Description TINYINT Very small integers (e.g., -128 to 127) SMALLINT Small integers (e.g., -32,768 to 32,767) MEDIUMINT Medium integers (MySQL specific) INT / INTEGER Standard integer BIGINT Very large integers","title":"Integer Types:"},{"location":"db/sql/db_sqlbasic/#decimalfixed-point-types","text":"Type Description DECIMAL(p,s) / NUMERIC(p,s) Exact numeric values with precision ( p ) and scale ( s ) p = total digits, s = digits after decimal point","title":"Decimal/Fixed-point Types:"},{"location":"db/sql/db_sqlbasic/#floating-point-types","text":"Type Description FLOAT Approximate floating-point number REAL Higher precision floating-point number DOUBLE / DOUBLE PRECISION Even more precision","title":"Floating-point Types:"},{"location":"db/sql/db_sqlbasic/#stringtext-data-types","text":"Used for storing characters, text, or binary data.","title":"String/Text Data Types"},{"location":"db/sql/db_sqlbasic/#character-types","text":"Type Description CHAR(n) Fixed-length string (padded with spaces if shorter) VARCHAR(n) Variable-length string, up to n characters","title":"Character Types:"},{"location":"db/sql/db_sqlbasic/#textlarge-object-types","text":"Type Description TEXT Large text block (e.g., article content) TINYTEXT / MEDIUMTEXT / LONGTEXT Different sizes in MySQL CLOB Character Large Object (Oracle/DB2)","title":"Text/Large Object Types:"},{"location":"db/sql/db_sqlbasic/#binary-data-types","text":"Type Description BINARY(n) Fixed-length binary data VARBINARY(n) Variable-length binary data BLOB Binary Large Object (for images/files etc)","title":"Binary Data Types:"},{"location":"db/sql/db_sqlbasic/#date-and-time-data-types","text":"Used to store date, time, or both. Type Description DATE Stores a date (YYYY-MM-DD) TIME Stores a time (HH\\:MM\\:SS) DATETIME Stores both date and time TIMESTAMP Stores date and time, often auto-updating YEAR Stores a year (MySQL only) INTERVAL (PostgreSQL) Time intervals","title":"Date and Time Data Types"},{"location":"db/sql/db_sqlbasic/#boolean-type","text":"Type Description BOOLEAN / BOOL Stores TRUE or FALSE (MySQL uses TINYINT(1) under the hood)","title":"Boolean Type"},{"location":"db/sql/db_sqlbasic/#uuid-unique-identifiers","text":"Used for globally unique values: Type Description UUID Universally Unique Identifier UNIQUEIDENTIFIER Used in SQL Server","title":"UUID / Unique Identifiers"},{"location":"db/sql/db_sqlbasic/#json-and-xml","text":"For structured data: Type Description JSON Stores JSON-formatted strings XML Stores XML documents (SQL Server, Oracle)","title":"JSON and XML"},{"location":"db/sql/db_sqlbasic/#spatial-geographic-types","text":"For storing geolocation data (supported in systems like PostgreSQL with PostGIS, MySQL): Type Description GEOMETRY Base type for all geometry data POINT , LINESTRING , POLYGON , etc. Specific shapes","title":"Spatial / Geographic Types"},{"location":"db/sql/db_sqlbasic/#other-special-types","text":"ENUM('value1', 'value2', ...) : Set of predefined values (MySQL) SET : A string object that can store zero or more values from a list (MySQL) ARRAY : A collection of values (PostgreSQL) MONEY / SMALLMONEY : Currency data (SQL Server)","title":"Other Special Types"},{"location":"db/sql/db_sqlbasic/#filtering-data","text":"","title":"Filtering Data"},{"location":"db/sql/db_sqlbasic/#where-conditional-filtering","text":"The WHERE clause is used to filter results based on conditions. Example: Filtering by salary SELECT * FROM employees WHERE salary > 50000 ; Example: Filtering by multiple conditions SELECT * FROM employees WHERE department = 'IT' AND salary > 60000 ;","title":"WHERE \u2013 Conditional Filtering"},{"location":"db/sql/db_sqlbasic/#order-by-sorting-results","text":"The ORDER BY clause sorts results in ascending ( ASC ) or descending ( DESC ) order. Example: Sorting by salary (descending) SELECT * FROM employees ORDER BY salary DESC ; Example: Sorting by department and then by salary SELECT * FROM employees ORDER BY department ASC , salary DESC ;","title":"ORDER BY \u2013 Sorting Results"},{"location":"db/sql/db_sqlbasic/#limit-restricting-output","text":"The LIMIT clause restricts the number of rows returned. Example: Getting the top 5 highest salaries SELECT * FROM employees ORDER BY salary DESC LIMIT 5 ; Example: Paginating results (offset 10, limit 5) SELECT * FROM employees ORDER BY employee_id ASC LIMIT 5 OFFSET 10 ;","title":"LIMIT \u2013 Restricting Output"},{"location":"db/sql/db_sqlbasic/#distinct-removing-duplicates","text":"The DISTINCT keyword removes duplicate values in query results. Example: Getting unique department names SELECT DISTINCT department FROM employees ; Example: Getting unique job titles from a jobs table SELECT DISTINCT job_title FROM jobs ; These basic SQL commands and filtering techniques help in effectively managing and retrieving data from a relational database system.","title":"DISTINCT \u2013 Removing Duplicates"},{"location":"db/sql/db_sqlindex/","text":"Indexing & Performance Optimization What is an Index? Indexes improve query performance by allowing faster lookups. Creating & Using Indexes ( CREATE INDEX ) Indexes speed up searches but can slow down insert/update operations. Example: Creating an index on the last_name column CREATE INDEX idx_lastname ON employees ( last_name ); Understanding Execution Plans ( EXPLAIN ) The EXPLAIN statement helps analyze query performance. Example: Analyzing a query execution plan EXPLAIN ANALYZE SELECT * FROM employees WHERE last_name = 'Smith' ;","title":"Index"},{"location":"db/sql/db_sqlindex/#indexing-performance-optimization","text":"","title":"Indexing &amp; Performance Optimization"},{"location":"db/sql/db_sqlindex/#what-is-an-index","text":"Indexes improve query performance by allowing faster lookups.","title":"What is an Index?"},{"location":"db/sql/db_sqlindex/#creating-using-indexes-create-index","text":"Indexes speed up searches but can slow down insert/update operations. Example: Creating an index on the last_name column CREATE INDEX idx_lastname ON employees ( last_name );","title":"Creating &amp; Using Indexes (CREATE INDEX)"},{"location":"db/sql/db_sqlindex/#understanding-execution-plans-explain","text":"The EXPLAIN statement helps analyze query performance. Example: Analyzing a query execution plan EXPLAIN ANALYZE SELECT * FROM employees WHERE last_name = 'Smith' ;","title":"Understanding Execution Plans (EXPLAIN)"},{"location":"db/sql/db_sqlinstall/","text":"Installing & Setting Up SQL Installing SQLite (Lightweight, No Server Needed) \ud83d\udca1 SQLite is great for local development and small applications. - Install SQLite: sudo apt install sqlite3 # Linux brew install sqlite3 # macOS - Open SQLite CLI: sqlite3 my_database.db - Verify installation: SELECT sqlite_version (); Installing MySQL (Popular for Web Apps) Install MySQL Server: sudo apt install mysql-server # Linux brew install mysql # macOS Start MySQL and login: mysql -u root -p Installing PostgreSQL (Advanced & Scalable) Install PostgreSQL: sudo apt install postgresql # Linux brew install postgresql # macOS Start PostgreSQL and login: psql -U postgres","title":"General"},{"location":"db/sql/db_sqlinstall/#installing-setting-up-sql","text":"","title":"Installing &amp; Setting Up SQL"},{"location":"db/sql/db_sqlinstall/#installing-sqlite-lightweight-no-server-needed","text":"\ud83d\udca1 SQLite is great for local development and small applications. - Install SQLite: sudo apt install sqlite3 # Linux brew install sqlite3 # macOS - Open SQLite CLI: sqlite3 my_database.db - Verify installation: SELECT sqlite_version ();","title":"Installing SQLite (Lightweight, No Server Needed)"},{"location":"db/sql/db_sqlinstall/#installing-mysql-popular-for-web-apps","text":"Install MySQL Server: sudo apt install mysql-server # Linux brew install mysql # macOS Start MySQL and login: mysql -u root -p","title":"Installing MySQL (Popular for Web Apps)"},{"location":"db/sql/db_sqlinstall/#installing-postgresql-advanced-scalable","text":"Install PostgreSQL: sudo apt install postgresql # Linux brew install postgresql # macOS Start PostgreSQL and login: psql -U postgres","title":"Installing PostgreSQL (Advanced &amp; Scalable)"},{"location":"db/sql/db_sqlite/","text":"SQLite is an open-source library that provides a lightweight disk-based database. sqlite it doesn\u2019t require a separate server process allows accessing the database using a nonstandard variant of the SQL query language. provides a SQL interface compliant with the python's DB-API 2.0 specification described by PEP 249 . applications can use SQLite for internal data storage. It\u2019s also possible to prototype an application using SQLite and then port the code to a larger database such as PostgreSQL or Oracle. To use the module, you must first create a Connection object that represents the database. A connection object's key methods are: cursor() returns a cursor object that can execute queries and retrieve results commit() submits the current transaction to the DB. If you don\u2019t call this method, anything you did since the last call to commit() is not visible from other database connections. If you wonder why you don\u2019t see the data you\u2019ve written to the database, please check you didn\u2019t forget to call this method. rollback() rolls back any changes to the database since the last call to commit(). close() closes the database connection. Note that this does not automatically call commit(). If you just close your database connection without calling commit() first, your changes will be lost! in our example the data will be stored in a local file called example.db : import sqlite3 conn = sqlite3 . connect ( 'example.db' ) You can also supply the special name :memory: to create a database in RAM. Once you have a Connection, you can create a Cursor object and call its execute() method to perform SQL commands: c = conn . cursor () # Create table called stocks c . execute ( ''' CREATE TABLE stocks ( date text, trans text, symbol text, qty real, price real ) ''' ) # Save (commit) the changes conn . commit () # We can also close the connection if we are done with it. # Just be sure any changes have been committed or they will be lost. conn . close () since we don't want to forget closing the connection, here's a nice utility function that opens a connection and returns a handy cursor object from contextlib import contextmanager @contextmanager def sqlite3_connect ( database , * args , ** kwargs ): conn = sqlite3 . connect ( database , * args , ** kwargs ) try : cursor = conn . cursor () yield ( conn , cursor ) finally : conn . close () and here's how sqlite3_connect() can be used: with sqlite3_connect ( 'example.db' ) as [ conn , c ]: # Insert a row of data c . execute ( \"INSERT INTO stocks VALUES ('2008-01-05','BUY','AAPL',120,37.14)\" ) # Save (commit) the changes conn . commit () # closing is done for us Usually your SQL operations will need to use values from Python variables like people's names, and fields from forms. DO NOT assemble your SQL using Python\u2019s string operations because doing so is insecure; it makes your program vulnerable to an SQL injection attack Instead, use the API\u2019s parameter substitution - Put ? as a placeholder wherever you want to use a value, and then provide a tuple of values as the second argument to the cursor\u2019s execute() method. with sqlite3_connect ( 'example.db' ) as [ conn , c ]: # DO NOT use str.format or f-strings or any other way to embed your variables into strings # INSTEAD, use the sanitized substitution capability of the execute() function t = ( 'RHAT' ,) c . execute ( 'SELECT * FROM stocks WHERE symbol=?' , t ) print ( c . fetchone ()) # Larger example that inserts many records at a time purchases = [( '2006-03-28' , 'BUY' , 'IBM' , 1000 , 45.00 ), ( '2006-04-05' , 'BUY' , 'MSFT' , 1000 , 72.00 ), ( '2006-04-06' , 'SELL' , 'IBM' , 500 , 53.00 ), ] c . executemany ( 'INSERT INTO stocks VALUES (?,?,?,?,?)' , purchases ) conn . commit () To retrieve data after executing a SELECT statement, you can treat the cursor as an iterator: with sqlite3_connect ( 'example.db' ) as [ conn , c ]: for row in c . execute ( 'SELECT * FROM stocks ORDER BY price' ): print ( row ) you can also call the cursor\u2019s fetchone() method to retrieve a single matching row, or call fetchall() to get a list of the matching rows. Table metadata We can access all the tables, their fields and their types using a simple query with sqlite3_connect ( 'example.db' ) as [ conn , c ]: rs = c . execute ( \"\"\" SELECT name, sql FROM sqlite_master WHERE type='table' ORDER BY name; \"\"\" ) for name , sql , * args in rs : print ( name ) print ( sql ) SQLite and Python types SQLite natively supports the following types: NULL , INTEGER , REAL , TEXT , BLOB . The following Python types can thus be sent to SQLite without any problem: Python type SQLite type None NULL int INTEGER float REAL str TEXT bytes BLOB SQLite supports only a limited set of types natively. To use other Python types with SQLite, you must adapt them to one of the sqlite3 module\u2019s supported types for SQLite: one of NoneType , int , float , str , bytes . Custom types there are two ways to read/write objects from a database: 1. encoding the object into a text/blob column using some format (often JSON) 2. converting objects into tables rows (respecting foreign keys as potential links to other objects) with a DB-API based SQL interface like sqlite3, both of these methods can contain a lot of boiler plate code and so we will leave them out of thid tutorial","title":"SQLite \ud83d\udee2\ufe0f"},{"location":"db/sql/db_sqlite/#table-metadata","text":"We can access all the tables, their fields and their types using a simple query with sqlite3_connect ( 'example.db' ) as [ conn , c ]: rs = c . execute ( \"\"\" SELECT name, sql FROM sqlite_master WHERE type='table' ORDER BY name; \"\"\" ) for name , sql , * args in rs : print ( name ) print ( sql )","title":"Table metadata"},{"location":"db/sql/db_sqlite/#sqlite-and-python-types","text":"SQLite natively supports the following types: NULL , INTEGER , REAL , TEXT , BLOB . The following Python types can thus be sent to SQLite without any problem: Python type SQLite type None NULL int INTEGER float REAL str TEXT bytes BLOB SQLite supports only a limited set of types natively. To use other Python types with SQLite, you must adapt them to one of the sqlite3 module\u2019s supported types for SQLite: one of NoneType , int , float , str , bytes .","title":"SQLite and Python types"},{"location":"db/sql/db_sqlite/#custom-types","text":"there are two ways to read/write objects from a database: 1. encoding the object into a text/blob column using some format (often JSON) 2. converting objects into tables rows (respecting foreign keys as potential links to other objects) with a DB-API based SQL interface like sqlite3, both of these methods can contain a lot of boiler plate code and so we will leave them out of thid tutorial","title":"Custom types"},{"location":"db/sql/db_sqljoin/","text":"SQL Joins & Relationships INNER JOIN \u2013 Matching Records The INNER JOIN returns only records that have matching values in both tables. Example: Retrieve employees and their department names SELECT employees . first_name , employees . last_name , departments . dept_name FROM employees INNER JOIN departments ON employees . department_id = departments . department_id ; LEFT JOIN / RIGHT JOIN \u2013 Including Unmatched Records The LEFT JOIN returns all records from the left table and matching records from the right table. If no match exists, NULL is returned. Example: Retrieve all employees, even if they have no department SELECT employees . first_name , employees . last_name , departments . dept_name FROM employees LEFT JOIN departments ON employees . department_id = departments . department_id ; The RIGHT JOIN does the opposite, returning all records from the right table and matching ones from the left. Example: Retrieve all departments, even if they have no employees SELECT employees . first_name , employees . last_name , departments . dept_name FROM employees RIGHT JOIN departments ON employees . department_id = departments . department_id ; FULL OUTER JOIN \u2013 Combining Everything The FULL OUTER JOIN returns all records from both tables, with NULL for non-matching records. Example: Retrieve all employees and departments, showing unmatched ones SELECT employees . first_name , employees . last_name , departments . dept_name FROM employees FULL OUTER JOIN departments ON employees . department_id = departments . department_id ; Left Excluding JOIN (LEFT JOIN with a WHERE clause) This query will return all of the records in the left table (table A) that do not match any records in the right table (table B). SELECT < select_list > FROM Table_A A LEFT JOIN Table_B B ON A . Key = B . Key WHERE B . Key IS NULL Right Excluding JOIN (RIGHT JOIN with a WHERE clause) This query will return all of the records in the right table (table B) that do not match any records in the left table (table A). SELECT < select_list > FROM Table_A A RIGHT JOIN Table_B B ON A . Key = B . Key WHERE A . Key IS NULL Outer Excluding JOIN (FULL JOIN with a WHERE clause) This query will return all of the records in the left table (table A) and all of the records in the right table (table B) that do not match. SELECT < select_list > FROM Table_A A FULL OUTER JOIN Table_B B ON A . Key = B . Key WHERE A . Key IS NULL OR B . Key IS NULL SELF JOIN \u2013 Joining a Table to Itself A SELF JOIN is used to compare rows within the same table. Example: Find employees and their managers (both stored in the employees table) SELECT e1 . first_name AS employee , e2 . first_name AS manager FROM employees e1 INNER JOIN employees e2 ON e1 . manager_id = e2 . employee_id ; CROSS JOIN \u2013 Cartesian Product A CROSS JOIN returns all possible combinations of records between two tables. Example: Create all combinations of employees and projects SELECT employees . first_name , projects . project_name FROM employees CROSS JOIN projects ; Grouping & Aggregation GROUP BY \u2013 Grouping Data The GROUP BY statement groups rows that have the same values in specified columns. Example: Count employees in each department SELECT department_id , COUNT ( * ) AS employee_count FROM employees GROUP BY department_id ; HAVING \u2013 Filtering Grouped Results The HAVING clause filters grouped results (similar to WHERE but used with aggregation). Example: Get departments with more than 10 employees SELECT department_id , COUNT ( * ) AS employee_count FROM employees GROUP BY department_id HAVING COUNT ( * ) > 10 ; Aggregate Functions (COUNT(), SUM(), AVG(), MIN(), MAX()) SQL provides several functions for performing calculations on data. COUNT() \u2013 Counts the number of rows. SUM() \u2013 Calculates the total sum of a column. AVG() \u2013 Computes the average value. MIN() / MAX() \u2013 Finds the smallest/largest value. Example: Get department statistics SELECT department_id , COUNT ( * ) AS employee_count , SUM ( salary ) AS total_salary , AVG ( salary ) AS average_salary , MIN ( salary ) AS lowest_salary , MAX ( salary ) AS highest_salary FROM employees GROUP BY department_id ; These SQL joins and aggregation techniques help in efficiently managing and analyzing relational data.","title":"JOIN"},{"location":"db/sql/db_sqljoin/#sql-joins-relationships","text":"","title":"SQL Joins &amp; Relationships"},{"location":"db/sql/db_sqljoin/#inner-join-matching-records","text":"The INNER JOIN returns only records that have matching values in both tables. Example: Retrieve employees and their department names SELECT employees . first_name , employees . last_name , departments . dept_name FROM employees INNER JOIN departments ON employees . department_id = departments . department_id ;","title":"INNER JOIN \u2013 Matching Records"},{"location":"db/sql/db_sqljoin/#left-join-right-join-including-unmatched-records","text":"The LEFT JOIN returns all records from the left table and matching records from the right table. If no match exists, NULL is returned. Example: Retrieve all employees, even if they have no department SELECT employees . first_name , employees . last_name , departments . dept_name FROM employees LEFT JOIN departments ON employees . department_id = departments . department_id ; The RIGHT JOIN does the opposite, returning all records from the right table and matching ones from the left. Example: Retrieve all departments, even if they have no employees SELECT employees . first_name , employees . last_name , departments . dept_name FROM employees RIGHT JOIN departments ON employees . department_id = departments . department_id ;","title":"LEFT JOIN / RIGHT JOIN \u2013 Including Unmatched Records"},{"location":"db/sql/db_sqljoin/#full-outer-join-combining-everything","text":"The FULL OUTER JOIN returns all records from both tables, with NULL for non-matching records. Example: Retrieve all employees and departments, showing unmatched ones SELECT employees . first_name , employees . last_name , departments . dept_name FROM employees FULL OUTER JOIN departments ON employees . department_id = departments . department_id ;","title":"FULL OUTER JOIN \u2013 Combining Everything"},{"location":"db/sql/db_sqljoin/#left-excluding-join-left-join-with-a-where-clause","text":"This query will return all of the records in the left table (table A) that do not match any records in the right table (table B). SELECT < select_list > FROM Table_A A LEFT JOIN Table_B B ON A . Key = B . Key WHERE B . Key IS NULL","title":"Left Excluding JOIN (LEFT JOIN with a WHERE clause)"},{"location":"db/sql/db_sqljoin/#right-excluding-join-right-join-with-a-where-clause","text":"This query will return all of the records in the right table (table B) that do not match any records in the left table (table A). SELECT < select_list > FROM Table_A A RIGHT JOIN Table_B B ON A . Key = B . Key WHERE A . Key IS NULL","title":"Right Excluding JOIN (RIGHT JOIN with a WHERE clause)"},{"location":"db/sql/db_sqljoin/#outer-excluding-join-full-join-with-a-where-clause","text":"This query will return all of the records in the left table (table A) and all of the records in the right table (table B) that do not match. SELECT < select_list > FROM Table_A A FULL OUTER JOIN Table_B B ON A . Key = B . Key WHERE A . Key IS NULL OR B . Key IS NULL","title":"Outer Excluding JOIN (FULL JOIN with a WHERE clause)"},{"location":"db/sql/db_sqljoin/#self-join-joining-a-table-to-itself","text":"A SELF JOIN is used to compare rows within the same table. Example: Find employees and their managers (both stored in the employees table) SELECT e1 . first_name AS employee , e2 . first_name AS manager FROM employees e1 INNER JOIN employees e2 ON e1 . manager_id = e2 . employee_id ;","title":"SELF JOIN \u2013 Joining a Table to Itself"},{"location":"db/sql/db_sqljoin/#cross-join-cartesian-product","text":"A CROSS JOIN returns all possible combinations of records between two tables. Example: Create all combinations of employees and projects SELECT employees . first_name , projects . project_name FROM employees CROSS JOIN projects ;","title":"CROSS JOIN \u2013 Cartesian Product"},{"location":"db/sql/db_sqljoin/#grouping-aggregation","text":"","title":"Grouping &amp; Aggregation"},{"location":"db/sql/db_sqljoin/#group-by-grouping-data","text":"The GROUP BY statement groups rows that have the same values in specified columns. Example: Count employees in each department SELECT department_id , COUNT ( * ) AS employee_count FROM employees GROUP BY department_id ;","title":"GROUP BY \u2013 Grouping Data"},{"location":"db/sql/db_sqljoin/#having-filtering-grouped-results","text":"The HAVING clause filters grouped results (similar to WHERE but used with aggregation). Example: Get departments with more than 10 employees SELECT department_id , COUNT ( * ) AS employee_count FROM employees GROUP BY department_id HAVING COUNT ( * ) > 10 ;","title":"HAVING \u2013 Filtering Grouped Results"},{"location":"db/sql/db_sqljoin/#aggregate-functions-count-sum-avg-min-max","text":"SQL provides several functions for performing calculations on data. COUNT() \u2013 Counts the number of rows. SUM() \u2013 Calculates the total sum of a column. AVG() \u2013 Computes the average value. MIN() / MAX() \u2013 Finds the smallest/largest value. Example: Get department statistics SELECT department_id , COUNT ( * ) AS employee_count , SUM ( salary ) AS total_salary , AVG ( salary ) AS average_salary , MIN ( salary ) AS lowest_salary , MAX ( salary ) AS highest_salary FROM employees GROUP BY department_id ; These SQL joins and aggregation techniques help in efficiently managing and analyzing relational data.","title":"Aggregate Functions (COUNT(), SUM(), AVG(), MIN(), MAX())"},{"location":"db/sql/db_sqlproc/","text":"Stored Procedures & Triggers Creating & Executing Stored Procedures Stored procedures allow reusable SQL logic execution. Example: Creating a stored procedure CREATE PROCEDURE UpdateSalary ( IN emp_id INT , IN new_salary DECIMAL ( 10 , 2 )) LANGUAGE SQL AS $$ UPDATE employees SET salary = new_salary WHERE employee_id = emp_id ; $$ ; Using Triggers for Automation Triggers execute actions automatically based on events. Example: Creating a trigger to update timestamps CREATE TRIGGER update_timestamp BEFORE UPDATE ON employees FOR EACH ROW EXECUTE FUNCTION update_modified_column (); Cursors & Loops Cursors iterate over query results within stored procedures. NoSQL Features in SQL Databases Working with JSON & XML Many relational databases support NoSQL-like JSON and XML processing. Full-Text Search ( tsvector , MATCH AGAINST ) Full-text search allows efficient searching within text fields. Example: Using full-text search in PostgreSQL SELECT * FROM articles WHERE to_tsvector ( content ) @@ to_tsquery ( 'database' ); Security & Best Practices SQL Injection Prevention Use prepared statements to prevent SQL injection. Example: Using a parameterized query in PostgreSQL PREPARE stmt FROM 'SELECT * FROM users WHERE username = $1' ; EXECUTE stmt ( 'admin' ); Role-Based Access Control ( GRANT , REVOKE ) Restrict access by assigning roles. Example: Granting read-only access GRANT SELECT ON employees TO readonly_user ; Data Encryption & Hashing ( MD5() , SHA256() ) Encryption and hashing protect sensitive data. Example: Storing hashed passwords UPDATE users SET password = SHA256 ( 'mysecurepassword' );","title":"More"},{"location":"db/sql/db_sqlproc/#stored-procedures-triggers","text":"","title":"Stored Procedures &amp; Triggers"},{"location":"db/sql/db_sqlproc/#creating-executing-stored-procedures","text":"Stored procedures allow reusable SQL logic execution. Example: Creating a stored procedure CREATE PROCEDURE UpdateSalary ( IN emp_id INT , IN new_salary DECIMAL ( 10 , 2 )) LANGUAGE SQL AS $$ UPDATE employees SET salary = new_salary WHERE employee_id = emp_id ; $$ ;","title":"Creating &amp; Executing Stored Procedures"},{"location":"db/sql/db_sqlproc/#using-triggers-for-automation","text":"Triggers execute actions automatically based on events. Example: Creating a trigger to update timestamps CREATE TRIGGER update_timestamp BEFORE UPDATE ON employees FOR EACH ROW EXECUTE FUNCTION update_modified_column ();","title":"Using Triggers for Automation"},{"location":"db/sql/db_sqlproc/#cursors-loops","text":"Cursors iterate over query results within stored procedures.","title":"Cursors &amp; Loops"},{"location":"db/sql/db_sqlproc/#nosql-features-in-sql-databases","text":"","title":"NoSQL Features in SQL Databases"},{"location":"db/sql/db_sqlproc/#working-with-json-xml","text":"Many relational databases support NoSQL-like JSON and XML processing.","title":"Working with JSON &amp; XML"},{"location":"db/sql/db_sqlproc/#full-text-search-tsvector-match-against","text":"Full-text search allows efficient searching within text fields. Example: Using full-text search in PostgreSQL SELECT * FROM articles WHERE to_tsvector ( content ) @@ to_tsquery ( 'database' );","title":"Full-Text Search (tsvector, MATCH AGAINST)"},{"location":"db/sql/db_sqlproc/#security-best-practices","text":"","title":"Security &amp; Best Practices"},{"location":"db/sql/db_sqlproc/#sql-injection-prevention","text":"Use prepared statements to prevent SQL injection. Example: Using a parameterized query in PostgreSQL PREPARE stmt FROM 'SELECT * FROM users WHERE username = $1' ; EXECUTE stmt ( 'admin' );","title":"SQL Injection Prevention"},{"location":"db/sql/db_sqlproc/#role-based-access-control-grant-revoke","text":"Restrict access by assigning roles. Example: Granting read-only access GRANT SELECT ON employees TO readonly_user ;","title":"Role-Based Access Control (GRANT, REVOKE)"},{"location":"db/sql/db_sqlproc/#data-encryption-hashing-md5-sha256","text":"Encryption and hashing protect sensitive data. Example: Storing hashed passwords UPDATE users SET password = SHA256 ( 'mysecurepassword' );","title":"Data Encryption &amp; Hashing (MD5(), SHA256())"},{"location":"db/sql/db_sqlsubquery/","text":"Subqueries & CTEs (Common Table Expressions) Subqueries ( SELECT inside SELECT ) Subqueries allow you to use the result of one query within another query. Example: Find employees who earn more than the average salary SELECT first_name , last_name , salary FROM employees WHERE salary > ( SELECT AVG ( salary ) FROM employees ); What is a CTE (Common Table Expression)? A CTE is a temporary, named result set that you can reference within a SELECT , INSERT , UPDATE , or DELETE statement. It exists only during the execution of the query \u2014 think of it like a temporary view that you can use just within that query. Why Use CTEs? Improve readability and organization for complex queries. Avoid repeating the same subquery multiple times. Enable recursive queries . Break down complicated queries into logical, easy-to-read sections. Types of CTE Usage Simple CTE Used to structure a subquery in a readable way. WITH sales_total AS ( SELECT employee_id , SUM ( sales ) AS total_sales FROM sales GROUP BY employee_id ) SELECT * FROM sales_total WHERE total_sales > 5000 ; CTEs ( WITH clause for readability) Common Table Expressions (CTEs) improve query readability by defining temporary result sets. Example: Using a CTE to list high-salary employees WITH HighEarners AS ( SELECT first_name , last_name , salary FROM employees WHERE salary > 80000 ) SELECT * FROM HighEarners ; Chained / Multiple CTEs You can define multiple CTEs at once. WITH cte1 AS ( SELECT * FROM products WHERE price > 100 ), cte2 AS ( SELECT * FROM cte1 WHERE category = 'Electronics' ) SELECT * FROM cte2 ; Recursive CTE When a CTE refers to itself \u2014 useful for hierarchical or tree-like data (like org charts or category trees). Example: Counting down from 5 WITH RECURSIVE countdown AS ( SELECT 5 AS number UNION ALL SELECT number - 1 FROM countdown WHERE number > 1 ) SELECT * FROM countdown ; UNION ALL is required in recursive CTEs. The first SELECT is the anchor member . The second SELECT is the recursive member . WHERE clause limits recursion. Recursive CTEs are used to handle hierarchical data such as organizational structures. Example: Retrieve an employee hierarchy WITH RECURSIVE EmployeeHierarchy AS ( SELECT employee_id , manager_id , first_name , last_name , 1 AS level FROM employees WHERE manager_id IS NULL UNION ALL SELECT e . employee_id , e . manager_id , e . first_name , e . last_name , eh . level + 1 FROM employees e INNER JOIN EmployeeHierarchy eh ON e . manager_id = eh . employee_id ) SELECT * FROM EmployeeHierarchy ; CTE with DML (INSERT/UPDATE/DELETE) You can use CTEs to simplify data modifications. Example: Updating top salespeople WITH top_sales AS ( SELECT employee_id FROM sales GROUP BY employee_id HAVING SUM ( sales ) > 5000 ) UPDATE employees SET bonus = 1000 WHERE employee_id IN ( SELECT employee_id FROM top_sales ); CTE vs Subquery CTE Subquery Improves readability for complex queries Can get nested and hard to read Can be recursive Cannot be recursive Can be referenced multiple times in the same query Must be repeated for multiple uses Exists only during the query execution Exists only within its surrounding query CTEs don\u2019t persist \u2014 they\u2019re temporary. Available in most modern RDBMS systems: PostgreSQL, SQL Server, Oracle, MySQL 8+, etc. You can nest CTEs and even use them in JOIN s. Summary CTEs are: Temporary, named query results Great for breaking down complex queries Essential for recursive operations Often improve query clarity and maintainability","title":"Subqueries & CTE"},{"location":"db/sql/db_sqlsubquery/#subqueries-ctes-common-table-expressions","text":"","title":"Subqueries &amp; CTEs (Common Table Expressions)"},{"location":"db/sql/db_sqlsubquery/#subqueries-select-inside-select","text":"Subqueries allow you to use the result of one query within another query. Example: Find employees who earn more than the average salary SELECT first_name , last_name , salary FROM employees WHERE salary > ( SELECT AVG ( salary ) FROM employees );","title":"Subqueries (SELECT inside SELECT)"},{"location":"db/sql/db_sqlsubquery/#what-is-a-cte-common-table-expression","text":"A CTE is a temporary, named result set that you can reference within a SELECT , INSERT , UPDATE , or DELETE statement. It exists only during the execution of the query \u2014 think of it like a temporary view that you can use just within that query.","title":"What is a CTE (Common Table Expression)?"},{"location":"db/sql/db_sqlsubquery/#why-use-ctes","text":"Improve readability and organization for complex queries. Avoid repeating the same subquery multiple times. Enable recursive queries . Break down complicated queries into logical, easy-to-read sections.","title":"Why Use CTEs?"},{"location":"db/sql/db_sqlsubquery/#types-of-cte-usage","text":"","title":"Types of CTE Usage"},{"location":"db/sql/db_sqlsubquery/#simple-cte","text":"Used to structure a subquery in a readable way. WITH sales_total AS ( SELECT employee_id , SUM ( sales ) AS total_sales FROM sales GROUP BY employee_id ) SELECT * FROM sales_total WHERE total_sales > 5000 ; CTEs ( WITH clause for readability) Common Table Expressions (CTEs) improve query readability by defining temporary result sets. Example: Using a CTE to list high-salary employees WITH HighEarners AS ( SELECT first_name , last_name , salary FROM employees WHERE salary > 80000 ) SELECT * FROM HighEarners ;","title":"Simple CTE"},{"location":"db/sql/db_sqlsubquery/#chained-multiple-ctes","text":"You can define multiple CTEs at once. WITH cte1 AS ( SELECT * FROM products WHERE price > 100 ), cte2 AS ( SELECT * FROM cte1 WHERE category = 'Electronics' ) SELECT * FROM cte2 ;","title":"Chained / Multiple CTEs"},{"location":"db/sql/db_sqlsubquery/#recursive-cte","text":"When a CTE refers to itself \u2014 useful for hierarchical or tree-like data (like org charts or category trees).","title":"Recursive CTE"},{"location":"db/sql/db_sqlsubquery/#cte-with-dml-insertupdatedelete","text":"You can use CTEs to simplify data modifications.","title":"CTE with DML (INSERT/UPDATE/DELETE)"},{"location":"db/sql/db_sqlsubquery/#cte-vs-subquery","text":"CTE Subquery Improves readability for complex queries Can get nested and hard to read Can be recursive Cannot be recursive Can be referenced multiple times in the same query Must be repeated for multiple uses Exists only during the query execution Exists only within its surrounding query CTEs don\u2019t persist \u2014 they\u2019re temporary. Available in most modern RDBMS systems: PostgreSQL, SQL Server, Oracle, MySQL 8+, etc. You can nest CTEs and even use them in JOIN s. Summary CTEs are: Temporary, named query results Great for breaking down complex queries Essential for recursive operations Often improve query clarity and maintainability","title":"CTE vs Subquery"}]}